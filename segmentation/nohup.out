2023-02-21 13:40:54,840 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.10 (default, Jun  4 2021, 15:09:15) [GCC 7.5.0]
CUDA available: True
GPU 0: NVIDIA A100-PCIE-40GB
CUDA_HOME: /usr/local/cuda
NVCC: Build cuda_11.1.TC455_06.29190527_0
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.9.0+cu111
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.10.0+cu111
OpenCV: 4.6.0
MMCV: 1.4.2
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.1
MMSegmentation: 0.20.2+a1b84a3
------------------------------------------------------------

2023-02-21 13:40:54,840 - mmseg - INFO - Distributed training: False
2023-02-21 13:40:55,429 - mmseg - INFO - Config:
num_things_classes = 0
num_stuff_classes = 16
num_classes = 16
norm_cfg = dict(type='SyncBN', requires_grad=True)
model = dict(
    type='EncoderDecoderMask2FormerAug',
    pretrained='pretrained/beit_large_patch16_224_pt22k_ft22k.pth',
    backbone=dict(
        type='BEiTAdapter',
        patch_size=16,
        embed_dim=1024,
        depth=24,
        num_heads=16,
        mlp_ratio=4,
        qkv_bias=True,
        use_abs_pos_emb=False,
        use_rel_pos_bias=True,
        img_size=896,
        init_values=1e-06,
        drop_path_rate=0.3,
        conv_inplane=64,
        n_points=4,
        deform_num_heads=16,
        cffn_ratio=0.25,
        deform_ratio=0.5,
        with_cp=True,
        interaction_indexes=[[0, 5], [6, 11], [12, 17], [18, 23]]),
    decode_head=dict(
        type='Mask2FormerHead',
        in_channels=[1024, 1024, 1024, 1024],
        feat_channels=1024,
        out_channels=1024,
        in_index=[0, 1, 2, 3],
        num_things_classes=0,
        num_stuff_classes=16,
        num_queries=100,
        num_transformer_feat_level=3,
        pixel_decoder=dict(
            type='MSDeformAttnPixelDecoder',
            num_outs=3,
            norm_cfg=dict(type='GN', num_groups=32),
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=1024,
                        num_heads=32,
                        num_levels=3,
                        num_points=4,
                        im2col_step=64,
                        dropout=0.0,
                        batch_first=False,
                        norm_cfg=None,
                        init_cfg=None),
                    ffn_cfgs=dict(
                        type='FFN',
                        embed_dims=1024,
                        feedforward_channels=4096,
                        num_fcs=2,
                        ffn_drop=0.0,
                        act_cfg=dict(type='ReLU', inplace=True),
                        with_cp=True),
                    operation_order=('self_attn', 'norm', 'ffn', 'norm')),
                init_cfg=None),
            positional_encoding=dict(
                type='SinePositionalEncoding', num_feats=512, normalize=True),
            init_cfg=None),
        enforce_decoder_input_project=False,
        positional_encoding=dict(
            type='SinePositionalEncoding', num_feats=512, normalize=True),
        transformer_decoder=dict(
            type='DetrTransformerDecoder',
            return_intermediate=True,
            num_layers=9,
            transformerlayers=dict(
                type='DetrTransformerDecoderLayer',
                attn_cfgs=dict(
                    type='MultiheadAttention',
                    embed_dims=1024,
                    num_heads=32,
                    attn_drop=0.0,
                    proj_drop=0.0,
                    dropout_layer=None,
                    batch_first=False),
                ffn_cfgs=dict(
                    embed_dims=1024,
                    feedforward_channels=4096,
                    num_fcs=2,
                    act_cfg=dict(type='ReLU', inplace=True),
                    ffn_drop=0.0,
                    dropout_layer=None,
                    add_identity=True,
                    with_cp=True),
                feedforward_channels=4096,
                operation_order=('cross_attn', 'norm', 'self_attn', 'norm',
                                 'ffn', 'norm')),
            init_cfg=None),
        loss_cls=dict(
            type='CrossEntropyLoss',
            use_sigmoid=False,
            loss_weight=2.0,
            reduction='mean',
            class_weight=[
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 0.1
            ]),
        loss_mask=dict(
            type='CrossEntropyLoss',
            use_sigmoid=True,
            reduction='mean',
            loss_weight=5.0),
        loss_dice=dict(
            type='DiceLoss',
            use_sigmoid=True,
            activate=True,
            reduction='mean',
            naive_dice=True,
            eps=1.0,
            loss_weight=5.0)),
    train_cfg=dict(
        num_points=12544,
        oversample_ratio=3.0,
        importance_sample_ratio=0.75,
        assigner=dict(
            type='MaskHungarianAssigner',
            cls_cost=dict(type='ClassificationCost', weight=2.0),
            mask_cost=dict(
                type='CrossEntropyLossCost', weight=5.0, use_sigmoid=True),
            dice_cost=dict(
                type='DiceCost', weight=5.0, pred_act=True, eps=1.0)),
        sampler=dict(type='MaskPseudoSampler')),
    test_cfg=dict(
        panoptic_on=True,
        semantic_on=False,
        instance_on=True,
        max_per_image=100,
        iou_thr=0.8,
        filter_low_score=True,
        mode='slide',
        crop_size=(896, 896),
        stride=(512, 512)),
    init_cfg=None)
dataset_type = 'MyDataset'
data_root = '/root/autodl-tmp/data'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (896, 896)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=(896, 896), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(896, 896), pad_val=0, seg_pad_val=255),
    dict(type='ToMask'),
    dict(type='DefaultFormatBundle'),
    dict(
        type='Collect',
        keys=['img', 'gt_semantic_seg', 'gt_masks', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(2048, 1024),
        img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0],
        flip=True,
        transforms=[
            dict(
                type='SETR_Resize',
                keep_ratio=True,
                crop_size=(896, 896),
                setr_multi_scale=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=1,
    train=dict(
        type='MyDataset',
        data_root='/root/autodl-tmp/data',
        img_dir='images/train',
        ann_dir='annotations/train',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(
                type='Resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),
            dict(type='RandomCrop', crop_size=(896, 896), cat_max_ratio=0.75),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(896, 896), pad_val=0, seg_pad_val=255),
            dict(type='ToMask'),
            dict(type='DefaultFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'gt_semantic_seg', 'gt_masks', 'gt_labels'])
        ]),
    val=dict(
        type='MyDataset',
        data_root='/root/autodl-tmp/data',
        img_dir='images/val',
        ann_dir='annotations/val',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(2048, 1024),
                img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0],
                flip=True,
                transforms=[
                    dict(
                        type='SETR_Resize',
                        keep_ratio=True,
                        crop_size=(896, 896),
                        setr_multi_scale=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='MyDataset',
        data_root='/root/autodl-tmp/data',
        img_dir='images/val',
        ann_dir='annotations/val',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(2048, 1024),
                img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0],
                flip=True,
                transforms=[
                    dict(
                        type='SETR_Resize',
                        keep_ratio=True,
                        crop_size=(896, 896),
                        setr_multi_scale=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
log_config = dict(
    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = 'pretrained/mask2former_beit_adapter_large_896_80k_cityscapes.pth.tar'
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
optimizer = dict(
    type='AdamW',
    lr=2e-05,
    betas=(0.9, 0.999),
    weight_decay=0.05,
    constructor='LayerDecayOptimizerConstructor',
    paramwise_cfg=dict(num_layers=24, layer_decay_rate=0.9))
optimizer_config = dict()
lr_config = dict(
    policy='poly',
    warmup='linear',
    warmup_iters=1500,
    warmup_ratio=1e-06,
    power=1.0,
    min_lr=0.0,
    by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=80000)
checkpoint_config = dict(by_epoch=False, interval=1000, max_keep_ckpts=1)
evaluation = dict(
    interval=1000, metric='mIoU', pre_eval=True, save_best='mIoU')
pretrained = 'pretrained/beit_large_patch16_224_pt22k_ft22k.pth'
work_dir = './work_dirs/my_city'
gpu_ids = range(0, 1)
auto_resume = False

2023-02-21 13:40:55,431 - mmseg - INFO - Set random seed to 85636677, deterministic: False
Position interpolate for blocks.0.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.1.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.2.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.3.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.4.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.5.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.6.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.7.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.8.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.9.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.10.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.11.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.12.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.13.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.14.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.15.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.16.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.17.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.18.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.19.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.20.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.21.attn.relative_position_bias_table from 27x27 to 111x111
2023-02-21 13:41:02,799 - mmseg - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc_norm.weight, fc_norm.bias, head.weight, head.bias

missing keys in source state_dict: blocks.0.attn.relative_position_index, blocks.1.attn.relative_position_index, blocks.2.attn.relative_position_index, blocks.3.attn.relative_position_index, blocks.4.attn.relative_position_index, blocks.5.attn.relative_position_index, blocks.6.attn.relative_position_index, blocks.7.attn.relative_position_index, blocks.8.attn.relative_position_index, blocks.9.attn.relative_position_index, blocks.10.attn.relative_position_index, blocks.11.attn.relative_position_index, blocks.12.attn.relative_position_index, blocks.13.attn.relative_position_index, blocks.14.attn.relative_position_index, blocks.15.attn.relative_position_index, blocks.16.attn.relative_position_index, blocks.17.attn.relative_position_index, blocks.18.attn.relative_position_index, blocks.19.attn.relative_position_index, blocks.20.attn.relative_position_index, blocks.21.attn.relative_position_index, blocks.22.attn.relative_position_index, blocks.23.attn.relative_position_index

/root/autodl-tmp/ViT-Adapter/segmentation/mmseg_custom/models/losses/cross_entropy_loss.py:230: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.
  warnings.warn(
train.py:179: UserWarning: SyncBN is only supported with DDP. To be compatible with DP, we convert SyncBN to BN. Please use dist_train.sh which can avoid this error.
  warnings.warn(
2023-02-21 13:41:06,618 - mmseg - INFO - EncoderDecoderMask2FormerAug(
  (backbone): BEiTAdapter(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.013043479062616825)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.02608695812523365)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.03913043811917305)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.0521739162504673)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.06521739810705185)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.0782608762383461)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.09130435436964035)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1043478325009346)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.11739131063222885)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1304347962141037)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.14347827434539795)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (12): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1565217524766922)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (13): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.16956523060798645)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (14): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1826087087392807)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (15): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.19565218687057495)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (16): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.2086956650018692)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (17): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.22173914313316345)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (18): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.2347826212644577)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (19): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.24782609939575195)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (20): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.260869562625885)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (21): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.27391305565834045)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (22): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.2869565188884735)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (23): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.30000001192092896)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (spm): SpatialPriorModule(
      (stem): Sequential(
        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (7): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (8): ReLU(inplace=True)
        (9): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      )
      (conv2): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (conv3): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (conv4): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (fc1): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))
      (fc2): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))
      (fc3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
      (fc4): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
    )
    (interactions): Sequential(
      (0): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=192, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=64, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1024, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
      (1): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=192, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=64, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1024, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
      (2): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=192, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=64, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1024, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
      (3): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=192, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=64, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1024, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (extra_extractors): Sequential(
          (0): Extractor(
            (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=1024, out_features=128, bias=True)
              (attention_weights): Linear(in_features=1024, out_features=64, bias=True)
              (value_proj): Linear(in_features=1024, out_features=512, bias=True)
              (output_proj): Linear(in_features=512, out_features=1024, bias=True)
            )
            (ffn): ConvFFN(
              (fc1): Linear(in_features=1024, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (drop_path): DropPath()
          )
          (1): Extractor(
            (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=1024, out_features=128, bias=True)
              (attention_weights): Linear(in_features=1024, out_features=64, bias=True)
              (value_proj): Linear(in_features=1024, out_features=512, bias=True)
              (output_proj): Linear(in_features=512, out_features=1024, bias=True)
            )
            (ffn): ConvFFN(
              (fc1): Linear(in_features=1024, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (drop_path): DropPath()
          )
        )
      )
    )
    (up): ConvTranspose2d(1024, 1024, kernel_size=(2, 2), stride=(2, 2))
    (norm1): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm2): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm3): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm4): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (decode_head): Mask2FormerHead(
    input_transform=multiple_select, ignore_index=255, align_corners=False
    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)
    (conv_seg): None
    (dropout): Dropout2d(p=0.1, inplace=False)
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
        (1): ConvModule(
          (conv): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
        (2): ConvModule(
          (conv): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
      )
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (postional_encoding): SinePositionalEncoding(num_feats=512, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
      (level_encoding): Embedding(3, 1024)
      (lateral_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
      )
      (output_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
          (activate): ReLU(inplace=True)
        )
      )
      (mask_feature): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
    )
    (transformer_decoder): DetrTransformerDecoder(
      (layers): ModuleList(
        (0): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (6): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (7): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (8): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (decoder_input_projs): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
    )
    (decoder_positional_encoding): SinePositionalEncoding(num_feats=512, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (query_embed): Embedding(100, 1024)
    (query_feat): Embedding(100, 1024)
    (level_embed): Embedding(3, 1024)
    (cls_embed): Linear(in_features=1024, out_features=17, bias=True)
    (mask_embed): Sequential(
      (0): Linear(in_features=1024, out_features=1024, bias=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=1024, out_features=1024, bias=True)
      (3): ReLU(inplace=True)
      (4): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (loss_cls): CrossEntropyLoss(avg_non_ignore=False)
    (loss_mask): CrossEntropyLoss(avg_non_ignore=False)
    (loss_dice): DiceLoss()
  )
)
2023-02-21 13:41:06,641 - mmseg - INFO - Loaded 1429 images
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.22.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.23.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
{'num_layers': 24, 'layer_decay_rate': 0.9}
Build LayerDecayOptimizerConstructor 0.900000 - 26
Param groups = {
  "layer_0_decay": {
    "param_names": [
      "backbone.cls_token",
      "backbone.patch_embed.proj.weight",
      "decode_head.query_embed.weight",
      "decode_head.query_feat.weight",
      "decode_head.level_embed.weight",
      "decode_head.cls_embed.weight",
      "decode_head.mask_embed.0.weight",
      "decode_head.mask_embed.2.weight",
      "decode_head.mask_embed.4.weight"
    ],
    "lr_scale": 0.0717897987691853,
    "lr": 1.4357959753837061e-06,
    "weight_decay": 0.05
  },
  "layer_25_decay": {
    "param_names": [
      "backbone.level_embed",
      "backbone.spm.stem.0.weight",
      "backbone.spm.stem.3.weight",
      "backbone.spm.stem.6.weight",
      "backbone.spm.conv2.0.weight",
      "backbone.spm.conv3.0.weight",
      "backbone.spm.conv4.0.weight",
      "backbone.spm.fc1.weight",
      "backbone.spm.fc2.weight",
      "backbone.spm.fc3.weight",
      "backbone.spm.fc4.weight",
      "backbone.interactions.0.injector.attn.sampling_offsets.weight",
      "backbone.interactions.0.injector.attn.attention_weights.weight",
      "backbone.interactions.0.injector.attn.value_proj.weight",
      "backbone.interactions.0.injector.attn.output_proj.weight",
      "backbone.interactions.0.extractor.attn.sampling_offsets.weight",
      "backbone.interactions.0.extractor.attn.attention_weights.weight",
      "backbone.interactions.0.extractor.attn.value_proj.weight",
      "backbone.interactions.0.extractor.attn.output_proj.weight",
      "backbone.interactions.0.extractor.ffn.fc1.weight",
      "backbone.interactions.0.extractor.ffn.dwconv.dwconv.weight",
      "backbone.interactions.0.extractor.ffn.fc2.weight",
      "backbone.interactions.1.injector.attn.sampling_offsets.weight",
      "backbone.interactions.1.injector.attn.attention_weights.weight",
      "backbone.interactions.1.injector.attn.value_proj.weight",
      "backbone.interactions.1.injector.attn.output_proj.weight",
      "backbone.interactions.1.extractor.attn.sampling_offsets.weight",
      "backbone.interactions.1.extractor.attn.attention_weights.weight",
      "backbone.interactions.1.extractor.attn.value_proj.weight",
      "backbone.interactions.1.extractor.attn.output_proj.weight",
      "backbone.interactions.1.extractor.ffn.fc1.weight",
      "backbone.interactions.1.extractor.ffn.dwconv.dwconv.weight",
      "backbone.interactions.1.extractor.ffn.fc2.weight",
      "backbone.interactions.2.injector.attn.sampling_offsets.weight",
      "backbone.interactions.2.injector.attn.attention_weights.weight",
      "backbone.interactions.2.injector.attn.value_proj.weight",
      "backbone.interactions.2.injector.attn.output_proj.weight",
      "backbone.interactions.2.extractor.attn.sampling_offsets.weight",
      "backbone.interactions.2.extractor.attn.attention_weights.weight",
      "backbone.interactions.2.extractor.attn.value_proj.weight",
      "backbone.interactions.2.extractor.attn.output_proj.weight",
      "backbone.interactions.2.extractor.ffn.fc1.weight",
      "backbone.interactions.2.extractor.ffn.dwconv.dwconv.weight",
      "backbone.interactions.2.extractor.ffn.fc2.weight",
      "backbone.interactions.3.injector.attn.sampling_offsets.weight",
      "backbone.interactions.3.injector.attn.attention_weights.weight",
      "backbone.interactions.3.injector.attn.value_proj.weight",
      "backbone.interactions.3.injector.attn.output_proj.weight",
      "backbone.interactions.3.extractor.attn.sampling_offsets.weight",
      "backbone.interactions.3.extractor.attn.attention_weights.weight",
      "backbone.interactions.3.extractor.attn.value_proj.weight",
      "backbone.interactions.3.extractor.attn.output_proj.weight",
      "backbone.interactions.3.extractor.ffn.fc1.weight",
      "backbone.interactions.3.extractor.ffn.dwconv.dwconv.weight",
      "backbone.interactions.3.extractor.ffn.fc2.weight",
      "backbone.interactions.3.extra_extractors.0.attn.sampling_offsets.weight",
      "backbone.interactions.3.extra_extractors.0.attn.attention_weights.weight",
      "backbone.interactions.3.extra_extractors.0.attn.value_proj.weight",
      "backbone.interactions.3.extra_extractors.0.attn.output_proj.weight",
      "backbone.interactions.3.extra_extractors.0.ffn.fc1.weight",
      "backbone.interactions.3.extra_extractors.0.ffn.dwconv.dwconv.weight",
      "backbone.interactions.3.extra_extractors.0.ffn.fc2.weight",
      "backbone.interactions.3.extra_extractors.1.attn.sampling_offsets.weight",
      "backbone.interactions.3.extra_extractors.1.attn.attention_weights.weight",
      "backbone.interactions.3.extra_extractors.1.attn.value_proj.weight",
      "backbone.interactions.3.extra_extractors.1.attn.output_proj.weight",
      "backbone.interactions.3.extra_extractors.1.ffn.fc1.weight",
      "backbone.interactions.3.extra_extractors.1.ffn.dwconv.dwconv.weight",
      "backbone.interactions.3.extra_extractors.1.ffn.fc2.weight",
      "backbone.up.weight",
      "decode_head.pixel_decoder.input_convs.0.conv.weight",
      "decode_head.pixel_decoder.input_convs.1.conv.weight",
      "decode_head.pixel_decoder.input_convs.2.conv.weight",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.level_encoding.weight",
      "decode_head.pixel_decoder.lateral_convs.0.conv.weight",
      "decode_head.pixel_decoder.output_convs.0.conv.weight",
      "decode_head.pixel_decoder.mask_feature.weight",
      "decode_head.transformer_decoder.layers.0.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.0.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.0.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.0.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.0.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.0.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.1.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.1.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.1.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.1.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.1.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.1.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.2.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.2.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.2.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.2.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.2.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.2.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.3.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.3.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.3.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.3.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.3.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.3.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.4.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.4.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.4.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.4.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.4.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.4.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.5.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.5.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.5.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.5.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.5.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.5.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.6.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.6.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.6.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.6.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.6.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.6.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.7.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.7.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.7.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.7.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.7.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.7.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.8.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.8.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.8.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.8.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.8.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.8.ffns.0.layers.1.weight"
    ],
    "lr_scale": 1.0,
    "lr": 2e-05,
    "weight_decay": 0.05
  },
  "layer_0_no_decay": {
    "param_names": [
      "backbone.patch_embed.proj.bias",
      "decode_head.cls_embed.bias",
      "decode_head.mask_embed.0.bias",
      "decode_head.mask_embed.2.bias",
      "decode_head.mask_embed.4.bias"
    ],
    "lr_scale": 0.0717897987691853,
    "lr": 1.4357959753837061e-06,
    "weight_decay": 0.0
  },
  "layer_1_no_decay": {
    "param_names": [
      "backbone.blocks.0.gamma_1",
      "backbone.blocks.0.gamma_2",
      "backbone.blocks.0.norm1.weight",
      "backbone.blocks.0.norm1.bias",
      "backbone.blocks.0.attn.q_bias",
      "backbone.blocks.0.attn.v_bias",
      "backbone.blocks.0.attn.proj.bias",
      "backbone.blocks.0.norm2.weight",
      "backbone.blocks.0.norm2.bias",
      "backbone.blocks.0.mlp.fc1.bias",
      "backbone.blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.07976644307687256,
    "lr": 1.5953288615374513e-06,
    "weight_decay": 0.0
  },
  "layer_1_decay": {
    "param_names": [
      "backbone.blocks.0.attn.relative_position_bias_table",
      "backbone.blocks.0.attn.qkv.weight",
      "backbone.blocks.0.attn.proj.weight",
      "backbone.blocks.0.mlp.fc1.weight",
      "backbone.blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.07976644307687256,
    "lr": 1.5953288615374513e-06,
    "weight_decay": 0.05
  },
  "layer_2_no_decay": {
    "param_names": [
      "backbone.blocks.1.gamma_1",
      "backbone.blocks.1.gamma_2",
      "backbone.blocks.1.norm1.weight",
      "backbone.blocks.1.norm1.bias",
      "backbone.blocks.1.attn.q_bias",
      "backbone.blocks.1.attn.v_bias",
      "backbone.blocks.1.attn.proj.bias",
      "backbone.blocks.1.norm2.weight",
      "backbone.blocks.1.norm2.bias",
      "backbone.blocks.1.mlp.fc1.bias",
      "backbone.blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.08862938119652507,
    "lr": 1.7725876239305016e-06,
    "weight_decay": 0.0
  },
  "layer_2_decay": {
    "param_names": [
      "backbone.blocks.1.attn.relative_position_bias_table",
      "backbone.blocks.1.attn.qkv.weight",
      "backbone.blocks.1.attn.proj.weight",
      "backbone.blocks.1.mlp.fc1.weight",
      "backbone.blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.08862938119652507,
    "lr": 1.7725876239305016e-06,
    "weight_decay": 0.05
  },
  "layer_3_no_decay": {
    "param_names": [
      "backbone.blocks.2.gamma_1",
      "backbone.blocks.2.gamma_2",
      "backbone.blocks.2.norm1.weight",
      "backbone.blocks.2.norm1.bias",
      "backbone.blocks.2.attn.q_bias",
      "backbone.blocks.2.attn.v_bias",
      "backbone.blocks.2.attn.proj.bias",
      "backbone.blocks.2.norm2.weight",
      "backbone.blocks.2.norm2.bias",
      "backbone.blocks.2.mlp.fc1.bias",
      "backbone.blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.09847709021836118,
    "lr": 1.969541804367224e-06,
    "weight_decay": 0.0
  },
  "layer_3_decay": {
    "param_names": [
      "backbone.blocks.2.attn.relative_position_bias_table",
      "backbone.blocks.2.attn.qkv.weight",
      "backbone.blocks.2.attn.proj.weight",
      "backbone.blocks.2.mlp.fc1.weight",
      "backbone.blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.09847709021836118,
    "lr": 1.969541804367224e-06,
    "weight_decay": 0.05
  },
  "layer_4_no_decay": {
    "param_names": [
      "backbone.blocks.3.gamma_1",
      "backbone.blocks.3.gamma_2",
      "backbone.blocks.3.norm1.weight",
      "backbone.blocks.3.norm1.bias",
      "backbone.blocks.3.attn.q_bias",
      "backbone.blocks.3.attn.v_bias",
      "backbone.blocks.3.attn.proj.bias",
      "backbone.blocks.3.norm2.weight",
      "backbone.blocks.3.norm2.bias",
      "backbone.blocks.3.mlp.fc1.bias",
      "backbone.blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.10941898913151242,
    "lr": 2.1883797826302486e-06,
    "weight_decay": 0.0
  },
  "layer_4_decay": {
    "param_names": [
      "backbone.blocks.3.attn.relative_position_bias_table",
      "backbone.blocks.3.attn.qkv.weight",
      "backbone.blocks.3.attn.proj.weight",
      "backbone.blocks.3.mlp.fc1.weight",
      "backbone.blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.10941898913151242,
    "lr": 2.1883797826302486e-06,
    "weight_decay": 0.05
  },
  "layer_5_no_decay": {
    "param_names": [
      "backbone.blocks.4.gamma_1",
      "backbone.blocks.4.gamma_2",
      "backbone.blocks.4.norm1.weight",
      "backbone.blocks.4.norm1.bias",
      "backbone.blocks.4.attn.q_bias",
      "backbone.blocks.4.attn.v_bias",
      "backbone.blocks.4.attn.proj.bias",
      "backbone.blocks.4.norm2.weight",
      "backbone.blocks.4.norm2.bias",
      "backbone.blocks.4.mlp.fc1.bias",
      "backbone.blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.12157665459056935,
    "lr": 2.431533091811387e-06,
    "weight_decay": 0.0
  },
  "layer_5_decay": {
    "param_names": [
      "backbone.blocks.4.attn.relative_position_bias_table",
      "backbone.blocks.4.attn.qkv.weight",
      "backbone.blocks.4.attn.proj.weight",
      "backbone.blocks.4.mlp.fc1.weight",
      "backbone.blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.12157665459056935,
    "lr": 2.431533091811387e-06,
    "weight_decay": 0.05
  },
  "layer_6_no_decay": {
    "param_names": [
      "backbone.blocks.5.gamma_1",
      "backbone.blocks.5.gamma_2",
      "backbone.blocks.5.norm1.weight",
      "backbone.blocks.5.norm1.bias",
      "backbone.blocks.5.attn.q_bias",
      "backbone.blocks.5.attn.v_bias",
      "backbone.blocks.5.attn.proj.bias",
      "backbone.blocks.5.norm2.weight",
      "backbone.blocks.5.norm2.bias",
      "backbone.blocks.5.mlp.fc1.bias",
      "backbone.blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.13508517176729928,
    "lr": 2.701703435345986e-06,
    "weight_decay": 0.0
  },
  "layer_6_decay": {
    "param_names": [
      "backbone.blocks.5.attn.relative_position_bias_table",
      "backbone.blocks.5.attn.qkv.weight",
      "backbone.blocks.5.attn.proj.weight",
      "backbone.blocks.5.mlp.fc1.weight",
      "backbone.blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.13508517176729928,
    "lr": 2.701703435345986e-06,
    "weight_decay": 0.05
  },
  "layer_7_no_decay": {
    "param_names": [
      "backbone.blocks.6.gamma_1",
      "backbone.blocks.6.gamma_2",
      "backbone.blocks.6.norm1.weight",
      "backbone.blocks.6.norm1.bias",
      "backbone.blocks.6.attn.q_bias",
      "backbone.blocks.6.attn.v_bias",
      "backbone.blocks.6.attn.proj.bias",
      "backbone.blocks.6.norm2.weight",
      "backbone.blocks.6.norm2.bias",
      "backbone.blocks.6.mlp.fc1.bias",
      "backbone.blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.15009463529699918,
    "lr": 3.001892705939984e-06,
    "weight_decay": 0.0
  },
  "layer_7_decay": {
    "param_names": [
      "backbone.blocks.6.attn.relative_position_bias_table",
      "backbone.blocks.6.attn.qkv.weight",
      "backbone.blocks.6.attn.proj.weight",
      "backbone.blocks.6.mlp.fc1.weight",
      "backbone.blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.15009463529699918,
    "lr": 3.001892705939984e-06,
    "weight_decay": 0.05
  },
  "layer_8_no_decay": {
    "param_names": [
      "backbone.blocks.7.gamma_1",
      "backbone.blocks.7.gamma_2",
      "backbone.blocks.7.norm1.weight",
      "backbone.blocks.7.norm1.bias",
      "backbone.blocks.7.attn.q_bias",
      "backbone.blocks.7.attn.v_bias",
      "backbone.blocks.7.attn.proj.bias",
      "backbone.blocks.7.norm2.weight",
      "backbone.blocks.7.norm2.bias",
      "backbone.blocks.7.mlp.fc1.bias",
      "backbone.blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.16677181699666577,
    "lr": 3.3354363399333156e-06,
    "weight_decay": 0.0
  },
  "layer_8_decay": {
    "param_names": [
      "backbone.blocks.7.attn.relative_position_bias_table",
      "backbone.blocks.7.attn.qkv.weight",
      "backbone.blocks.7.attn.proj.weight",
      "backbone.blocks.7.mlp.fc1.weight",
      "backbone.blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.16677181699666577,
    "lr": 3.3354363399333156e-06,
    "weight_decay": 0.05
  },
  "layer_9_no_decay": {
    "param_names": [
      "backbone.blocks.8.gamma_1",
      "backbone.blocks.8.gamma_2",
      "backbone.blocks.8.norm1.weight",
      "backbone.blocks.8.norm1.bias",
      "backbone.blocks.8.attn.q_bias",
      "backbone.blocks.8.attn.v_bias",
      "backbone.blocks.8.attn.proj.bias",
      "backbone.blocks.8.norm2.weight",
      "backbone.blocks.8.norm2.bias",
      "backbone.blocks.8.mlp.fc1.bias",
      "backbone.blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.18530201888518416,
    "lr": 3.7060403777036836e-06,
    "weight_decay": 0.0
  },
  "layer_9_decay": {
    "param_names": [
      "backbone.blocks.8.attn.relative_position_bias_table",
      "backbone.blocks.8.attn.qkv.weight",
      "backbone.blocks.8.attn.proj.weight",
      "backbone.blocks.8.mlp.fc1.weight",
      "backbone.blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.18530201888518416,
    "lr": 3.7060403777036836e-06,
    "weight_decay": 0.05
  },
  "layer_10_no_decay": {
    "param_names": [
      "backbone.blocks.9.gamma_1",
      "backbone.blocks.9.gamma_2",
      "backbone.blocks.9.norm1.weight",
      "backbone.blocks.9.norm1.bias",
      "backbone.blocks.9.attn.q_bias",
      "backbone.blocks.9.attn.v_bias",
      "backbone.blocks.9.attn.proj.bias",
      "backbone.blocks.9.norm2.weight",
      "backbone.blocks.9.norm2.bias",
      "backbone.blocks.9.mlp.fc1.bias",
      "backbone.blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.20589113209464907,
    "lr": 4.117822641892982e-06,
    "weight_decay": 0.0
  },
  "layer_10_decay": {
    "param_names": [
      "backbone.blocks.9.attn.relative_position_bias_table",
      "backbone.blocks.9.attn.qkv.weight",
      "backbone.blocks.9.attn.proj.weight",
      "backbone.blocks.9.mlp.fc1.weight",
      "backbone.blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.20589113209464907,
    "lr": 4.117822641892982e-06,
    "weight_decay": 0.05
  },
  "layer_11_no_decay": {
    "param_names": [
      "backbone.blocks.10.gamma_1",
      "backbone.blocks.10.gamma_2",
      "backbone.blocks.10.norm1.weight",
      "backbone.blocks.10.norm1.bias",
      "backbone.blocks.10.attn.q_bias",
      "backbone.blocks.10.attn.v_bias",
      "backbone.blocks.10.attn.proj.bias",
      "backbone.blocks.10.norm2.weight",
      "backbone.blocks.10.norm2.bias",
      "backbone.blocks.10.mlp.fc1.bias",
      "backbone.blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.2287679245496101,
    "lr": 4.575358490992202e-06,
    "weight_decay": 0.0
  },
  "layer_11_decay": {
    "param_names": [
      "backbone.blocks.10.attn.relative_position_bias_table",
      "backbone.blocks.10.attn.qkv.weight",
      "backbone.blocks.10.attn.proj.weight",
      "backbone.blocks.10.mlp.fc1.weight",
      "backbone.blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.2287679245496101,
    "lr": 4.575358490992202e-06,
    "weight_decay": 0.05
  },
  "layer_12_no_decay": {
    "param_names": [
      "backbone.blocks.11.gamma_1",
      "backbone.blocks.11.gamma_2",
      "backbone.blocks.11.norm1.weight",
      "backbone.blocks.11.norm1.bias",
      "backbone.blocks.11.attn.q_bias",
      "backbone.blocks.11.attn.v_bias",
      "backbone.blocks.11.attn.proj.bias",
      "backbone.blocks.11.norm2.weight",
      "backbone.blocks.11.norm2.bias",
      "backbone.blocks.11.mlp.fc1.bias",
      "backbone.blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.2541865828329001,
    "lr": 5.083731656658002e-06,
    "weight_decay": 0.0
  },
  "layer_12_decay": {
    "param_names": [
      "backbone.blocks.11.attn.relative_position_bias_table",
      "backbone.blocks.11.attn.qkv.weight",
      "backbone.blocks.11.attn.proj.weight",
      "backbone.blocks.11.mlp.fc1.weight",
      "backbone.blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.2541865828329001,
    "lr": 5.083731656658002e-06,
    "weight_decay": 0.05
  },
  "layer_13_no_decay": {
    "param_names": [
      "backbone.blocks.12.gamma_1",
      "backbone.blocks.12.gamma_2",
      "backbone.blocks.12.norm1.weight",
      "backbone.blocks.12.norm1.bias",
      "backbone.blocks.12.attn.q_bias",
      "backbone.blocks.12.attn.v_bias",
      "backbone.blocks.12.attn.proj.bias",
      "backbone.blocks.12.norm2.weight",
      "backbone.blocks.12.norm2.bias",
      "backbone.blocks.12.mlp.fc1.bias",
      "backbone.blocks.12.mlp.fc2.bias"
    ],
    "lr_scale": 0.2824295364810001,
    "lr": 5.648590729620003e-06,
    "weight_decay": 0.0
  },
  "layer_13_decay": {
    "param_names": [
      "backbone.blocks.12.attn.relative_position_bias_table",
      "backbone.blocks.12.attn.qkv.weight",
      "backbone.blocks.12.attn.proj.weight",
      "backbone.blocks.12.mlp.fc1.weight",
      "backbone.blocks.12.mlp.fc2.weight"
    ],
    "lr_scale": 0.2824295364810001,
    "lr": 5.648590729620003e-06,
    "weight_decay": 0.05
  },
  "layer_14_no_decay": {
    "param_names": [
      "backbone.blocks.13.gamma_1",
      "backbone.blocks.13.gamma_2",
      "backbone.blocks.13.norm1.weight",
      "backbone.blocks.13.norm1.bias",
      "backbone.blocks.13.attn.q_bias",
      "backbone.blocks.13.attn.v_bias",
      "backbone.blocks.13.attn.proj.bias",
      "backbone.blocks.13.norm2.weight",
      "backbone.blocks.13.norm2.bias",
      "backbone.blocks.13.mlp.fc1.bias",
      "backbone.blocks.13.mlp.fc2.bias"
    ],
    "lr_scale": 0.31381059609000006,
    "lr": 6.276211921800002e-06,
    "weight_decay": 0.0
  },
  "layer_14_decay": {
    "param_names": [
      "backbone.blocks.13.attn.relative_position_bias_table",
      "backbone.blocks.13.attn.qkv.weight",
      "backbone.blocks.13.attn.proj.weight",
      "backbone.blocks.13.mlp.fc1.weight",
      "backbone.blocks.13.mlp.fc2.weight"
    ],
    "lr_scale": 0.31381059609000006,
    "lr": 6.276211921800002e-06,
    "weight_decay": 0.05
  },
  "layer_15_no_decay": {
    "param_names": [
      "backbone.blocks.14.gamma_1",
      "backbone.blocks.14.gamma_2",
      "backbone.blocks.14.norm1.weight",
      "backbone.blocks.14.norm1.bias",
      "backbone.blocks.14.attn.q_bias",
      "backbone.blocks.14.attn.v_bias",
      "backbone.blocks.14.attn.proj.bias",
      "backbone.blocks.14.norm2.weight",
      "backbone.blocks.14.norm2.bias",
      "backbone.blocks.14.mlp.fc1.bias",
      "backbone.blocks.14.mlp.fc2.bias"
    ],
    "lr_scale": 0.3486784401000001,
    "lr": 6.973568802000002e-06,
    "weight_decay": 0.0
  },
  "layer_15_decay": {
    "param_names": [
      "backbone.blocks.14.attn.relative_position_bias_table",
      "backbone.blocks.14.attn.qkv.weight",
      "backbone.blocks.14.attn.proj.weight",
      "backbone.blocks.14.mlp.fc1.weight",
      "backbone.blocks.14.mlp.fc2.weight"
    ],
    "lr_scale": 0.3486784401000001,
    "lr": 6.973568802000002e-06,
    "weight_decay": 0.05
  },
  "layer_16_no_decay": {
    "param_names": [
      "backbone.blocks.15.gamma_1",
      "backbone.blocks.15.gamma_2",
      "backbone.blocks.15.norm1.weight",
      "backbone.blocks.15.norm1.bias",
      "backbone.blocks.15.attn.q_bias",
      "backbone.blocks.15.attn.v_bias",
      "backbone.blocks.15.attn.proj.bias",
      "backbone.blocks.15.norm2.weight",
      "backbone.blocks.15.norm2.bias",
      "backbone.blocks.15.mlp.fc1.bias",
      "backbone.blocks.15.mlp.fc2.bias"
    ],
    "lr_scale": 0.3874204890000001,
    "lr": 7.748409780000003e-06,
    "weight_decay": 0.0
  },
  "layer_16_decay": {
    "param_names": [
      "backbone.blocks.15.attn.relative_position_bias_table",
      "backbone.blocks.15.attn.qkv.weight",
      "backbone.blocks.15.attn.proj.weight",
      "backbone.blocks.15.mlp.fc1.weight",
      "backbone.blocks.15.mlp.fc2.weight"
    ],
    "lr_scale": 0.3874204890000001,
    "lr": 7.748409780000003e-06,
    "weight_decay": 0.05
  },
  "layer_17_no_decay": {
    "param_names": [
      "backbone.blocks.16.gamma_1",
      "backbone.blocks.16.gamma_2",
      "backbone.blocks.16.norm1.weight",
      "backbone.blocks.16.norm1.bias",
      "backbone.blocks.16.attn.q_bias",
      "backbone.blocks.16.attn.v_bias",
      "backbone.blocks.16.attn.proj.bias",
      "backbone.blocks.16.norm2.weight",
      "backbone.blocks.16.norm2.bias",
      "backbone.blocks.16.mlp.fc1.bias",
      "backbone.blocks.16.mlp.fc2.bias"
    ],
    "lr_scale": 0.4304672100000001,
    "lr": 8.609344200000003e-06,
    "weight_decay": 0.0
  },
  "layer_17_decay": {
    "param_names": [
      "backbone.blocks.16.attn.relative_position_bias_table",
      "backbone.blocks.16.attn.qkv.weight",
      "backbone.blocks.16.attn.proj.weight",
      "backbone.blocks.16.mlp.fc1.weight",
      "backbone.blocks.16.mlp.fc2.weight"
    ],
    "lr_scale": 0.4304672100000001,
    "lr": 8.609344200000003e-06,
    "weight_decay": 0.05
  },
  "layer_18_no_decay": {
    "param_names": [
      "backbone.blocks.17.gamma_1",
      "backbone.blocks.17.gamma_2",
      "backbone.blocks.17.norm1.weight",
      "backbone.blocks.17.norm1.bias",
      "backbone.blocks.17.attn.q_bias",
      "backbone.blocks.17.attn.v_bias",
      "backbone.blocks.17.attn.proj.bias",
      "backbone.blocks.17.norm2.weight",
      "backbone.blocks.17.norm2.bias",
      "backbone.blocks.17.mlp.fc1.bias",
      "backbone.blocks.17.mlp.fc2.bias"
    ],
    "lr_scale": 0.4782969000000001,
    "lr": 9.565938000000002e-06,
    "weight_decay": 0.0
  },
  "layer_18_decay": {
    "param_names": [
      "backbone.blocks.17.attn.relative_position_bias_table",
      "backbone.blocks.17.attn.qkv.weight",
      "backbone.blocks.17.attn.proj.weight",
      "backbone.blocks.17.mlp.fc1.weight",
      "backbone.blocks.17.mlp.fc2.weight"
    ],
    "lr_scale": 0.4782969000000001,
    "lr": 9.565938000000002e-06,
    "weight_decay": 0.05
  },
  "layer_19_no_decay": {
    "param_names": [
      "backbone.blocks.18.gamma_1",
      "backbone.blocks.18.gamma_2",
      "backbone.blocks.18.norm1.weight",
      "backbone.blocks.18.norm1.bias",
      "backbone.blocks.18.attn.q_bias",
      "backbone.blocks.18.attn.v_bias",
      "backbone.blocks.18.attn.proj.bias",
      "backbone.blocks.18.norm2.weight",
      "backbone.blocks.18.norm2.bias",
      "backbone.blocks.18.mlp.fc1.bias",
      "backbone.blocks.18.mlp.fc2.bias"
    ],
    "lr_scale": 0.531441,
    "lr": 1.0628820000000002e-05,
    "weight_decay": 0.0
  },
  "layer_19_decay": {
    "param_names": [
      "backbone.blocks.18.attn.relative_position_bias_table",
      "backbone.blocks.18.attn.qkv.weight",
      "backbone.blocks.18.attn.proj.weight",
      "backbone.blocks.18.mlp.fc1.weight",
      "backbone.blocks.18.mlp.fc2.weight"
    ],
    "lr_scale": 0.531441,
    "lr": 1.0628820000000002e-05,
    "weight_decay": 0.05
  },
  "layer_20_no_decay": {
    "param_names": [
      "backbone.blocks.19.gamma_1",
      "backbone.blocks.19.gamma_2",
      "backbone.blocks.19.norm1.weight",
      "backbone.blocks.19.norm1.bias",
      "backbone.blocks.19.attn.q_bias",
      "backbone.blocks.19.attn.v_bias",
      "backbone.blocks.19.attn.proj.bias",
      "backbone.blocks.19.norm2.weight",
      "backbone.blocks.19.norm2.bias",
      "backbone.blocks.19.mlp.fc1.bias",
      "backbone.blocks.19.mlp.fc2.bias"
    ],
    "lr_scale": 0.5904900000000001,
    "lr": 1.1809800000000002e-05,
    "weight_decay": 0.0
  },
  "layer_20_decay": {
    "param_names": [
      "backbone.blocks.19.attn.relative_position_bias_table",
      "backbone.blocks.19.attn.qkv.weight",
      "backbone.blocks.19.attn.proj.weight",
      "backbone.blocks.19.mlp.fc1.weight",
      "backbone.blocks.19.mlp.fc2.weight"
    ],
    "lr_scale": 0.5904900000000001,
    "lr": 1.1809800000000002e-05,
    "weight_decay": 0.05
  },
  "layer_21_no_decay": {
    "param_names": [
      "backbone.blocks.20.gamma_1",
      "backbone.blocks.20.gamma_2",
      "backbone.blocks.20.norm1.weight",
      "backbone.blocks.20.norm1.bias",
      "backbone.blocks.20.attn.q_bias",
      "backbone.blocks.20.attn.v_bias",
      "backbone.blocks.20.attn.proj.bias",
      "backbone.blocks.20.norm2.weight",
      "backbone.blocks.20.norm2.bias",
      "backbone.blocks.20.mlp.fc1.bias",
      "backbone.blocks.20.mlp.fc2.bias"
    ],
    "lr_scale": 0.6561,
    "lr": 1.3122e-05,
    "weight_decay": 0.0
  },
  "layer_21_decay": {
    "param_names": [
      "backbone.blocks.20.attn.relative_position_bias_table",
      "backbone.blocks.20.attn.qkv.weight",
      "backbone.blocks.20.attn.proj.weight",
      "backbone.blocks.20.mlp.fc1.weight",
      "backbone.blocks.20.mlp.fc2.weight"
    ],
    "lr_scale": 0.6561,
    "lr": 1.3122e-05,
    "weight_decay": 0.05
  },
  "layer_22_no_decay": {
    "param_names": [
      "backbone.blocks.21.gamma_1",
      "backbone.blocks.21.gamma_2",
      "backbone.blocks.21.norm1.weight",
      "backbone.blocks.21.norm1.bias",
      "backbone.blocks.21.attn.q_bias",
      "backbone.blocks.21.attn.v_bias",
      "backbone.blocks.21.attn.proj.bias",
      "backbone.blocks.21.norm2.weight",
      "backbone.blocks.21.norm2.bias",
      "backbone.blocks.21.mlp.fc1.bias",
      "backbone.blocks.21.mlp.fc2.bias"
    ],
    "lr_scale": 0.7290000000000001,
    "lr": 1.4580000000000003e-05,
    "weight_decay": 0.0
  },
  "layer_22_decay": {
    "param_names": [
      "backbone.blocks.21.attn.relative_position_bias_table",
      "backbone.blocks.21.attn.qkv.weight",
      "backbone.blocks.21.attn.proj.weight",
      "backbone.blocks.21.mlp.fc1.weight",
      "backbone.blocks.21.mlp.fc2.weight"
    ],
    "lr_scale": 0.7290000000000001,
    "lr": 1.4580000000000003e-05,
    "weight_decay": 0.05
  },
  "layer_23_no_decay": {
    "param_names": [
      "backbone.blocks.22.gamma_1",
      "backbone.blocks.22.gamma_2",
      "backbone.blocks.22.norm1.weight",
      "backbone.blocks.22.norm1.bias",
      "backbone.blocks.22.attn.q_bias",
      "backbone.blocks.22.attn.v_bias",
      "backbone.blocks.22.attn.proj.bias",
      "backbone.blocks.22.norm2.weight",
      "backbone.blocks.22.norm2.bias",
      "backbone.blocks.22.mlp.fc1.bias",
      "backbone.blocks.22.mlp.fc2.bias"
    ],
    "lr_scale": 0.81,
    "lr": 1.62e-05,
    "weight_decay": 0.0
  },
  "layer_23_decay": {
    "param_names": [
      "backbone.blocks.22.attn.relative_position_bias_table",
      "backbone.blocks.22.attn.qkv.weight",
      "backbone.blocks.22.attn.proj.weight",
      "backbone.blocks.22.mlp.fc1.weight",
      "backbone.blocks.22.mlp.fc2.weight"
    ],
    "lr_scale": 0.81,
    "lr": 1.62e-05,
    "weight_decay": 0.05
  },
  "layer_24_no_decay": {
    "param_names": [
      "backbone.blocks.23.gamma_1",
      "backbone.blocks.23.gamma_2",
      "backbone.blocks.23.norm1.weight",
      "backbone.blocks.23.norm1.bias",
      "backbone.blocks.23.attn.q_bias",
      "backbone.blocks.23.attn.v_bias",
      "backbone.blocks.23.attn.proj.bias",
      "backbone.blocks.23.norm2.weight",
      "backbone.blocks.23.norm2.bias",
      "backbone.blocks.23.mlp.fc1.bias",
      "backbone.blocks.23.mlp.fc2.bias"
    ],
    "lr_scale": 0.9,
    "lr": 1.8e-05,
    "weight_decay": 0.0
  },
  "layer_24_decay": {
    "param_names": [
      "backbone.blocks.23.attn.relative_position_bias_table",
      "backbone.blocks.23.attn.qkv.weight",
      "backbone.blocks.23.attn.proj.weight",
      "backbone.blocks.23.mlp.fc1.weight",
      "backbone.blocks.23.mlp.fc2.weight"
    ],
    "lr_scale": 0.9,
    "lr": 1.8e-05,
    "weight_decay": 0.05
  },
  "layer_25_no_decay": {
    "param_names": [
      "backbone.spm.stem.1.weight",
      "backbone.spm.stem.1.bias",
      "backbone.spm.stem.4.weight",
      "backbone.spm.stem.4.bias",
      "backbone.spm.stem.7.weight",
      "backbone.spm.stem.7.bias",
      "backbone.spm.conv2.1.weight",
      "backbone.spm.conv2.1.bias",
      "backbone.spm.conv3.1.weight",
      "backbone.spm.conv3.1.bias",
      "backbone.spm.conv4.1.weight",
      "backbone.spm.conv4.1.bias",
      "backbone.spm.fc1.bias",
      "backbone.spm.fc2.bias",
      "backbone.spm.fc3.bias",
      "backbone.spm.fc4.bias",
      "backbone.interactions.0.injector.gamma",
      "backbone.interactions.0.injector.query_norm.weight",
      "backbone.interactions.0.injector.query_norm.bias",
      "backbone.interactions.0.injector.feat_norm.weight",
      "backbone.interactions.0.injector.feat_norm.bias",
      "backbone.interactions.0.injector.attn.sampling_offsets.bias",
      "backbone.interactions.0.injector.attn.attention_weights.bias",
      "backbone.interactions.0.injector.attn.value_proj.bias",
      "backbone.interactions.0.injector.attn.output_proj.bias",
      "backbone.interactions.0.extractor.query_norm.weight",
      "backbone.interactions.0.extractor.query_norm.bias",
      "backbone.interactions.0.extractor.feat_norm.weight",
      "backbone.interactions.0.extractor.feat_norm.bias",
      "backbone.interactions.0.extractor.attn.sampling_offsets.bias",
      "backbone.interactions.0.extractor.attn.attention_weights.bias",
      "backbone.interactions.0.extractor.attn.value_proj.bias",
      "backbone.interactions.0.extractor.attn.output_proj.bias",
      "backbone.interactions.0.extractor.ffn.fc1.bias",
      "backbone.interactions.0.extractor.ffn.dwconv.dwconv.bias",
      "backbone.interactions.0.extractor.ffn.fc2.bias",
      "backbone.interactions.0.extractor.ffn_norm.weight",
      "backbone.interactions.0.extractor.ffn_norm.bias",
      "backbone.interactions.1.injector.gamma",
      "backbone.interactions.1.injector.query_norm.weight",
      "backbone.interactions.1.injector.query_norm.bias",
      "backbone.interactions.1.injector.feat_norm.weight",
      "backbone.interactions.1.injector.feat_norm.bias",
      "backbone.interactions.1.injector.attn.sampling_offsets.bias",
      "backbone.interactions.1.injector.attn.attention_weights.bias",
      "backbone.interactions.1.injector.attn.value_proj.bias",
      "backbone.interactions.1.injector.attn.output_proj.bias",
      "backbone.interactions.1.extractor.query_norm.weight",
      "backbone.interactions.1.extractor.query_norm.bias",
      "backbone.interactions.1.extractor.feat_norm.weight",
      "backbone.interactions.1.extractor.feat_norm.bias",
      "backbone.interactions.1.extractor.attn.sampling_offsets.bias",
      "backbone.interactions.1.extractor.attn.attention_weights.bias",
      "backbone.interactions.1.extractor.attn.value_proj.bias",
      "backbone.interactions.1.extractor.attn.output_proj.bias",
      "backbone.interactions.1.extractor.ffn.fc1.bias",
      "backbone.interactions.1.extractor.ffn.dwconv.dwconv.bias",
      "backbone.interactions.1.extractor.ffn.fc2.bias",
      "backbone.interactions.1.extractor.ffn_norm.weight",
      "backbone.interactions.1.extractor.ffn_norm.bias",
      "backbone.interactions.2.injector.gamma",
      "backbone.interactions.2.injector.query_norm.weight",
      "backbone.interactions.2.injector.query_norm.bias",
      "backbone.interactions.2.injector.feat_norm.weight",
      "backbone.interactions.2.injector.feat_norm.bias",
      "backbone.interactions.2.injector.attn.sampling_offsets.bias",
      "backbone.interactions.2.injector.attn.attention_weights.bias",
      "backbone.interactions.2.injector.attn.value_proj.bias",
      "backbone.interactions.2.injector.attn.output_proj.bias",
      "backbone.interactions.2.extractor.query_norm.weight",
      "backbone.interactions.2.extractor.query_norm.bias",
      "backbone.interactions.2.extractor.feat_norm.weight",
      "backbone.interactions.2.extractor.feat_norm.bias",
      "backbone.interactions.2.extractor.attn.sampling_offsets.bias",
      "backbone.interactions.2.extractor.attn.attention_weights.bias",
      "backbone.interactions.2.extractor.attn.value_proj.bias",
      "backbone.interactions.2.extractor.attn.output_proj.bias",
      "backbone.interactions.2.extractor.ffn.fc1.bias",
      "backbone.interactions.2.extractor.ffn.dwconv.dwconv.bias",
      "backbone.interactions.2.extractor.ffn.fc2.bias",
      "backbone.interactions.2.extractor.ffn_norm.weight",
      "backbone.interactions.2.extractor.ffn_norm.bias",
      "backbone.interactions.3.injector.gamma",
      "backbone.interactions.3.injector.query_norm.weight",
      "backbone.interactions.3.injector.query_norm.bias",
      "backbone.interactions.3.injector.feat_norm.weight",
      "backbone.interactions.3.injector.feat_norm.bias",
      "backbone.interactions.3.injector.attn.sampling_offsets.bias",
      "backbone.interactions.3.injector.attn.attention_weights.bias",
      "backbone.interactions.3.injector.attn.value_proj.bias",
      "backbone.interactions.3.injector.attn.output_proj.bias",
      "backbone.interactions.3.extractor.query_norm.weight",
      "backbone.interactions.3.extractor.query_norm.bias",
      "backbone.interactions.3.extractor.feat_norm.weight",
      "backbone.interactions.3.extractor.feat_norm.bias",
      "backbone.interactions.3.extractor.attn.sampling_offsets.bias",
      "backbone.interactions.3.extractor.attn.attention_weights.bias",
      "backbone.interactions.3.extractor.attn.value_proj.bias",
      "backbone.interactions.3.extractor.attn.output_proj.bias",
      "backbone.interactions.3.extractor.ffn.fc1.bias",
      "backbone.interactions.3.extractor.ffn.dwconv.dwconv.bias",
      "backbone.interactions.3.extractor.ffn.fc2.bias",
      "backbone.interactions.3.extractor.ffn_norm.weight",
      "backbone.interactions.3.extractor.ffn_norm.bias",
      "backbone.interactions.3.extra_extractors.0.query_norm.weight",
      "backbone.interactions.3.extra_extractors.0.query_norm.bias",
      "backbone.interactions.3.extra_extractors.0.feat_norm.weight",
      "backbone.interactions.3.extra_extractors.0.feat_norm.bias",
      "backbone.interactions.3.extra_extractors.0.attn.sampling_offsets.bias",
      "backbone.interactions.3.extra_extractors.0.attn.attention_weights.bias",
      "backbone.interactions.3.extra_extractors.0.attn.value_proj.bias",
      "backbone.interactions.3.extra_extractors.0.attn.output_proj.bias",
      "backbone.interactions.3.extra_extractors.0.ffn.fc1.bias",
      "backbone.interactions.3.extra_extractors.0.ffn.dwconv.dwconv.bias",
      "backbone.interactions.3.extra_extractors.0.ffn.fc2.bias",
      "backbone.interactions.3.extra_extractors.0.ffn_norm.weight",
      "backbone.interactions.3.extra_extractors.0.ffn_norm.bias",
      "backbone.interactions.3.extra_extractors.1.query_norm.weight",
      "backbone.interactions.3.extra_extractors.1.query_norm.bias",
      "backbone.interactions.3.extra_extractors.1.feat_norm.weight",
      "backbone.interactions.3.extra_extractors.1.feat_norm.bias",
      "backbone.interactions.3.extra_extractors.1.attn.sampling_offsets.bias",
      "backbone.interactions.3.extra_extractors.1.attn.attention_weights.bias",
      "backbone.interactions.3.extra_extractors.1.attn.value_proj.bias",
      "backbone.interactions.3.extra_extractors.1.attn.output_proj.bias",
      "backbone.interactions.3.extra_extractors.1.ffn.fc1.bias",
      "backbone.interactions.3.extra_extractors.1.ffn.dwconv.dwconv.bias",
      "backbone.interactions.3.extra_extractors.1.ffn.fc2.bias",
      "backbone.interactions.3.extra_extractors.1.ffn_norm.weight",
      "backbone.interactions.3.extra_extractors.1.ffn_norm.bias",
      "backbone.up.bias",
      "backbone.norm1.weight",
      "backbone.norm1.bias",
      "backbone.norm2.weight",
      "backbone.norm2.bias",
      "backbone.norm3.weight",
      "backbone.norm3.bias",
      "backbone.norm4.weight",
      "backbone.norm4.bias",
      "decode_head.pixel_decoder.input_convs.0.conv.bias",
      "decode_head.pixel_decoder.input_convs.0.gn.weight",
      "decode_head.pixel_decoder.input_convs.0.gn.bias",
      "decode_head.pixel_decoder.input_convs.1.conv.bias",
      "decode_head.pixel_decoder.input_convs.1.gn.weight",
      "decode_head.pixel_decoder.input_convs.1.gn.bias",
      "decode_head.pixel_decoder.input_convs.2.conv.bias",
      "decode_head.pixel_decoder.input_convs.2.gn.weight",
      "decode_head.pixel_decoder.input_convs.2.gn.bias",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.0.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.0.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.0.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.0.norms.1.bias",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.1.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.1.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.1.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.1.norms.1.bias",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.2.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.2.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.2.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.2.norms.1.bias",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.3.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.3.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.3.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.3.norms.1.bias",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.4.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.4.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.4.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.4.norms.1.bias",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.5.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.5.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.5.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.5.norms.1.bias",
      "decode_head.pixel_decoder.lateral_convs.0.gn.weight",
      "decode_head.pixel_decoder.lateral_convs.0.gn.bias",
      "decode_head.pixel_decoder.output_convs.0.gn.weight",
      "decode_head.pixel_decoder.output_convs.0.gn.bias",
      "decode_head.pixel_decoder.mask_feature.bias",
      "decode_head.transformer_decoder.layers.0.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.0.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.0.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.0.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.0.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.0.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.0.norms.0.weight",
      "decode_head.transformer_decoder.layers.0.norms.0.bias",
      "decode_head.transformer_decoder.layers.0.norms.1.weight",
      "decode_head.transformer_decoder.layers.0.norms.1.bias",
      "decode_head.transformer_decoder.layers.0.norms.2.weight",
      "decode_head.transformer_decoder.layers.0.norms.2.bias",
      "decode_head.transformer_decoder.layers.1.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.1.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.1.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.1.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.1.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.1.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.1.norms.0.weight",
      "decode_head.transformer_decoder.layers.1.norms.0.bias",
      "decode_head.transformer_decoder.layers.1.norms.1.weight",
      "decode_head.transformer_decoder.layers.1.norms.1.bias",
      "decode_head.transformer_decoder.layers.1.norms.2.weight",
      "decode_head.transformer_decoder.layers.1.norms.2.bias",
      "decode_head.transformer_decoder.layers.2.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.2.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.2.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.2.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.2.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.2.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.2.norms.0.weight",
      "decode_head.transformer_decoder.layers.2.norms.0.bias",
      "decode_head.transformer_decoder.layers.2.norms.1.weight",
      "decode_head.transformer_decoder.layers.2.norms.1.bias",
      "decode_head.transformer_decoder.layers.2.norms.2.weight",
      "decode_head.transformer_decoder.layers.2.norms.2.bias",
      "decode_head.transformer_decoder.layers.3.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.3.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.3.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.3.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.3.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.3.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.3.norms.0.weight",
      "decode_head.transformer_decoder.layers.3.norms.0.bias",
      "decode_head.transformer_decoder.layers.3.norms.1.weight",
      "decode_head.transformer_decoder.layers.3.norms.1.bias",
      "decode_head.transformer_decoder.layers.3.norms.2.weight",
      "decode_head.transformer_decoder.layers.3.norms.2.bias",
      "decode_head.transformer_decoder.layers.4.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.4.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.4.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.4.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.4.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.4.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.4.norms.0.weight",
      "decode_head.transformer_decoder.layers.4.norms.0.bias",
      "decode_head.transformer_decoder.layers.4.norms.1.weight",
      "decode_head.transformer_decoder.layers.4.norms.1.bias",
      "decode_head.transformer_decoder.layers.4.norms.2.weight",
      "decode_head.transformer_decoder.layers.4.norms.2.bias",
      "decode_head.transformer_decoder.layers.5.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.5.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.5.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.5.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.5.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.5.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.5.norms.0.weight",
      "decode_head.transformer_decoder.layers.5.norms.0.bias",
      "decode_head.transformer_decoder.layers.5.norms.1.weight",
      "decode_head.transformer_decoder.layers.5.norms.1.bias",
      "decode_head.transformer_decoder.layers.5.norms.2.weight",
      "decode_head.transformer_decoder.layers.5.norms.2.bias",
      "decode_head.transformer_decoder.layers.6.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.6.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.6.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.6.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.6.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.6.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.6.norms.0.weight",
      "decode_head.transformer_decoder.layers.6.norms.0.bias",
      "decode_head.transformer_decoder.layers.6.norms.1.weight",
      "decode_head.transformer_decoder.layers.6.norms.1.bias",
      "decode_head.transformer_decoder.layers.6.norms.2.weight",
      "decode_head.transformer_decoder.layers.6.norms.2.bias",
      "decode_head.transformer_decoder.layers.7.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.7.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.7.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.7.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.7.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.7.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.7.norms.0.weight",
      "decode_head.transformer_decoder.layers.7.norms.0.bias",
      "decode_head.transformer_decoder.layers.7.norms.1.weight",
      "decode_head.transformer_decoder.layers.7.norms.1.bias",
      "decode_head.transformer_decoder.layers.7.norms.2.weight",
      "decode_head.transformer_decoder.layers.7.norms.2.bias",
      "decode_head.transformer_decoder.layers.8.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.8.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.8.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.8.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.8.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.8.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.8.norms.0.weight",
      "decode_head.transformer_decoder.layers.8.norms.0.bias",
      "decode_head.transformer_decoder.layers.8.norms.1.weight",
      "decode_head.transformer_decoder.layers.8.norms.1.bias",
      "decode_head.transformer_decoder.layers.8.norms.2.weight",
      "decode_head.transformer_decoder.layers.8.norms.2.bias",
      "decode_head.transformer_decoder.post_norm.weight",
      "decode_head.transformer_decoder.post_norm.bias"
    ],
    "lr_scale": 1.0,
    "lr": 2e-05,
    "weight_decay": 0.0
  }
}2023-02-21 13:41:14,127 - mmseg - INFO - Loaded 357 images
2023-02-21 13:41:14,128 - mmseg - INFO - load checkpoint from local path: pretrained/mask2former_beit_adapter_large_896_80k_cityscapes.pth.tar
2023-02-21 13:41:16,474 - mmseg - WARNING - The model and loaded state dict do not match exactly

size mismatch for decode_head.cls_embed.weight: copying a param with shape torch.Size([20, 1024]) from checkpoint, the shape in current model is torch.Size([17, 1024]).
size mismatch for decode_head.cls_embed.bias: copying a param with shape torch.Size([20]) from checkpoint, the shape in current model is torch.Size([17]).
missing keys in source state_dict: backbone.blocks.0.attn.relative_position_index, backbone.blocks.1.attn.relative_position_index, backbone.blocks.2.attn.relative_position_index, backbone.blocks.3.attn.relative_position_index, backbone.blocks.4.attn.relative_position_index, backbone.blocks.5.attn.relative_position_index, backbone.blocks.6.attn.relative_position_index, backbone.blocks.7.attn.relative_position_index, backbone.blocks.8.attn.relative_position_index, backbone.blocks.9.attn.relative_position_index, backbone.blocks.10.attn.relative_position_index, backbone.blocks.11.attn.relative_position_index, backbone.blocks.12.attn.relative_position_index, backbone.blocks.13.attn.relative_position_index, backbone.blocks.14.attn.relative_position_index, backbone.blocks.15.attn.relative_position_index, backbone.blocks.16.attn.relative_position_index, backbone.blocks.17.attn.relative_position_index, backbone.blocks.18.attn.relative_position_index, backbone.blocks.19.attn.relative_position_index, backbone.blocks.20.attn.relative_position_index, backbone.blocks.21.attn.relative_position_index, backbone.blocks.22.attn.relative_position_index, backbone.blocks.23.attn.relative_position_index

2023-02-21 13:41:16,531 - mmseg - INFO - Start running, host: root@autodl-container-cd46119efa-5e1dcbf2, work_dir: /root/autodl-tmp/ViT-Adapter/segmentation/work_dirs/my_city
2023-02-21 13:41:16,531 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-02-21 13:41:16,532 - mmseg - INFO - workflow: [('train', 1)], max: 80000 iters
2023-02-21 13:41:16,532 - mmseg - INFO - Checkpoints will be saved to /root/autodl-tmp/ViT-Adapter/segmentation/work_dirs/my_city by HardDiskBackend.

/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:3657: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/root/miniconda3/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
2023-02-21 13:43:38,229 - mmseg - INFO - Iter [50/80000]	lr: 4.688e-08, eta: 2 days, 14:53:40, time: 2.832, data_time: 0.029, memory: 31487, decode.loss_cls: 5.9428, decode.loss_mask: 2.2211, decode.loss_dice: 2.6313, decode.d0.loss_cls: 5.9408, decode.d0.loss_mask: 2.2655, decode.d0.loss_dice: 3.0681, decode.d1.loss_cls: 6.2277, decode.d1.loss_mask: 2.2852, decode.d1.loss_dice: 2.7388, decode.d2.loss_cls: 5.9049, decode.d2.loss_mask: 2.1973, decode.d2.loss_dice: 2.6884, decode.d3.loss_cls: 5.8600, decode.d3.loss_mask: 2.1773, decode.d3.loss_dice: 2.6180, decode.d4.loss_cls: 6.1592, decode.d4.loss_mask: 2.2057, decode.d4.loss_dice: 2.6204, decode.d5.loss_cls: 6.3486, decode.d5.loss_mask: 2.1350, decode.d5.loss_dice: 2.6314, decode.d6.loss_cls: 6.5019, decode.d6.loss_mask: 2.1740, decode.d6.loss_dice: 2.6389, decode.d7.loss_cls: 6.3923, decode.d7.loss_mask: 2.2111, decode.d7.loss_dice: 2.6334, decode.d8.loss_cls: 6.2136, decode.d8.loss_mask: 2.2027, decode.d8.loss_dice: 2.6257, loss: 110.4609
2023-02-21 13:45:54,370 - mmseg - INFO - Iter [100/80000]	lr: 9.465e-08, eta: 2 days, 13:38:35, time: 2.723, data_time: 0.019, memory: 31487, decode.loss_cls: 4.8459, decode.loss_mask: 1.8832, decode.loss_dice: 2.5657, decode.d0.loss_cls: 5.9830, decode.d0.loss_mask: 2.1011, decode.d0.loss_dice: 3.0216, decode.d1.loss_cls: 5.6609, decode.d1.loss_mask: 1.9521, decode.d1.loss_dice: 2.6836, decode.d2.loss_cls: 5.1307, decode.d2.loss_mask: 1.8666, decode.d2.loss_dice: 2.6187, decode.d3.loss_cls: 4.9033, decode.d3.loss_mask: 1.8299, decode.d3.loss_dice: 2.5968, decode.d4.loss_cls: 5.0499, decode.d4.loss_mask: 1.8852, decode.d4.loss_dice: 2.5813, decode.d5.loss_cls: 5.2327, decode.d5.loss_mask: 1.8622, decode.d5.loss_dice: 2.5613, decode.d6.loss_cls: 5.2111, decode.d6.loss_mask: 1.8346, decode.d6.loss_dice: 2.5670, decode.d7.loss_cls: 5.0979, decode.d7.loss_mask: 1.8357, decode.d7.loss_dice: 2.5684, decode.d8.loss_cls: 5.0041, decode.d8.loss_mask: 1.8627, decode.d8.loss_dice: 2.5714, loss: 97.3686
2023-02-21 13:48:10,643 - mmseg - INFO - Iter [150/80000]	lr: 1.424e-07, eta: 2 days, 13:13:13, time: 2.725, data_time: 0.018, memory: 31487, decode.loss_cls: 3.7554, decode.loss_mask: 1.4697, decode.loss_dice: 2.2397, decode.d0.loss_cls: 5.9891, decode.d0.loss_mask: 1.8532, decode.d0.loss_dice: 2.7282, decode.d1.loss_cls: 4.6888, decode.d1.loss_mask: 1.6125, decode.d1.loss_dice: 2.4182, decode.d2.loss_cls: 4.1329, decode.d2.loss_mask: 1.4850, decode.d2.loss_dice: 2.3326, decode.d3.loss_cls: 3.9417, decode.d3.loss_mask: 1.4324, decode.d3.loss_dice: 2.2697, decode.d4.loss_cls: 3.9733, decode.d4.loss_mask: 1.4596, decode.d4.loss_dice: 2.2519, decode.d5.loss_cls: 3.9587, decode.d5.loss_mask: 1.4511, decode.d5.loss_dice: 2.2426, decode.d6.loss_cls: 3.9094, decode.d6.loss_mask: 1.4318, decode.d6.loss_dice: 2.2356, decode.d7.loss_cls: 3.8624, decode.d7.loss_mask: 1.4455, decode.d7.loss_dice: 2.2457, decode.d8.loss_cls: 3.8421, decode.d8.loss_mask: 1.4338, decode.d8.loss_dice: 2.2200, loss: 80.3123
2023-02-21 13:50:26,965 - mmseg - INFO - Iter [200/80000]	lr: 1.900e-07, eta: 2 days, 12:59:43, time: 2.726, data_time: 0.019, memory: 31487, decode.loss_cls: 3.5392, decode.loss_mask: 1.2680, decode.loss_dice: 2.0216, decode.d0.loss_cls: 6.0111, decode.d0.loss_mask: 1.7898, decode.d0.loss_dice: 2.5426, decode.d1.loss_cls: 4.0637, decode.d1.loss_mask: 1.4631, decode.d1.loss_dice: 2.2549, decode.d2.loss_cls: 3.7769, decode.d2.loss_mask: 1.3407, decode.d2.loss_dice: 2.1692, decode.d3.loss_cls: 3.6863, decode.d3.loss_mask: 1.2919, decode.d3.loss_dice: 2.0950, decode.d4.loss_cls: 3.7111, decode.d4.loss_mask: 1.2765, decode.d4.loss_dice: 2.0746, decode.d5.loss_cls: 3.6612, decode.d5.loss_mask: 1.2767, decode.d5.loss_dice: 2.0686, decode.d6.loss_cls: 3.6042, decode.d6.loss_mask: 1.2770, decode.d6.loss_dice: 2.0334, decode.d7.loss_cls: 3.6182, decode.d7.loss_mask: 1.2675, decode.d7.loss_dice: 2.0296, decode.d8.loss_cls: 3.5897, decode.d8.loss_mask: 1.2675, decode.d8.loss_dice: 2.0080, loss: 74.0779
2023-02-21 13:52:43,481 - mmseg - INFO - Iter [250/80000]	lr: 2.376e-07, eta: 2 days, 12:51:44, time: 2.730, data_time: 0.019, memory: 31487, decode.loss_cls: 3.4137, decode.loss_mask: 1.2039, decode.loss_dice: 2.0485, decode.d0.loss_cls: 5.9896, decode.d0.loss_mask: 1.7113, decode.d0.loss_dice: 2.5599, decode.d1.loss_cls: 3.9286, decode.d1.loss_mask: 1.3736, decode.d1.loss_dice: 2.3428, decode.d2.loss_cls: 3.7018, decode.d2.loss_mask: 1.2473, decode.d2.loss_dice: 2.2000, decode.d3.loss_cls: 3.5586, decode.d3.loss_mask: 1.2210, decode.d3.loss_dice: 2.1157, decode.d4.loss_cls: 3.5545, decode.d4.loss_mask: 1.2026, decode.d4.loss_dice: 2.1140, decode.d5.loss_cls: 3.5548, decode.d5.loss_mask: 1.2117, decode.d5.loss_dice: 2.0932, decode.d6.loss_cls: 3.5054, decode.d6.loss_mask: 1.1875, decode.d6.loss_dice: 2.0765, decode.d7.loss_cls: 3.4864, decode.d7.loss_mask: 1.1867, decode.d7.loss_dice: 2.0788, decode.d8.loss_cls: 3.4636, decode.d8.loss_mask: 1.2079, decode.d8.loss_dice: 2.0568, loss: 72.5967
2023-02-21 13:54:59,961 - mmseg - INFO - Iter [300/80000]	lr: 2.851e-07, eta: 2 days, 12:45:30, time: 2.730, data_time: 0.019, memory: 31487, decode.loss_cls: 3.0074, decode.loss_mask: 1.1815, decode.loss_dice: 1.9926, decode.d0.loss_cls: 5.9974, decode.d0.loss_mask: 1.6438, decode.d0.loss_dice: 2.5284, decode.d1.loss_cls: 3.5909, decode.d1.loss_mask: 1.2947, decode.d1.loss_dice: 2.3094, decode.d2.loss_cls: 3.3500, decode.d2.loss_mask: 1.1975, decode.d2.loss_dice: 2.1749, decode.d3.loss_cls: 3.1386, decode.d3.loss_mask: 1.1613, decode.d3.loss_dice: 2.0577, decode.d4.loss_cls: 3.1332, decode.d4.loss_mask: 1.1713, decode.d4.loss_dice: 2.0480, decode.d5.loss_cls: 3.1202, decode.d5.loss_mask: 1.1956, decode.d5.loss_dice: 2.0069, decode.d6.loss_cls: 3.0647, decode.d6.loss_mask: 1.1754, decode.d6.loss_dice: 2.0093, decode.d7.loss_cls: 3.0589, decode.d7.loss_mask: 1.1899, decode.d7.loss_dice: 2.0310, decode.d8.loss_cls: 3.0232, decode.d8.loss_mask: 1.1764, decode.d8.loss_dice: 2.0097, loss: 68.0398
2023-02-21 13:57:16,360 - mmseg - INFO - Iter [350/80000]	lr: 3.326e-07, eta: 2 days, 12:40:06, time: 2.728, data_time: 0.018, memory: 31487, decode.loss_cls: 2.7734, decode.loss_mask: 1.1175, decode.loss_dice: 1.8650, decode.d0.loss_cls: 5.9938, decode.d0.loss_mask: 1.5288, decode.d0.loss_dice: 2.3604, decode.d1.loss_cls: 3.4996, decode.d1.loss_mask: 1.2085, decode.d1.loss_dice: 2.1209, decode.d2.loss_cls: 3.1449, decode.d2.loss_mask: 1.1358, decode.d2.loss_dice: 1.9822, decode.d3.loss_cls: 2.9001, decode.d3.loss_mask: 1.1217, decode.d3.loss_dice: 1.9140, decode.d4.loss_cls: 2.8691, decode.d4.loss_mask: 1.1120, decode.d4.loss_dice: 1.9033, decode.d5.loss_cls: 2.8799, decode.d5.loss_mask: 1.1281, decode.d5.loss_dice: 1.8774, decode.d6.loss_cls: 2.8090, decode.d6.loss_mask: 1.1020, decode.d6.loss_dice: 1.8650, decode.d7.loss_cls: 2.7793, decode.d7.loss_mask: 1.1244, decode.d7.loss_dice: 1.8554, decode.d8.loss_cls: 2.7730, decode.d8.loss_mask: 1.1180, decode.d8.loss_dice: 1.8654, loss: 63.7278
2023-02-21 13:59:32,938 - mmseg - INFO - Iter [400/80000]	lr: 3.800e-07, eta: 2 days, 12:36:04, time: 2.732, data_time: 0.019, memory: 31487, decode.loss_cls: 2.3898, decode.loss_mask: 1.0606, decode.loss_dice: 1.7567, decode.d0.loss_cls: 6.0261, decode.d0.loss_mask: 1.4735, decode.d0.loss_dice: 2.2598, decode.d1.loss_cls: 3.2679, decode.d1.loss_mask: 1.1534, decode.d1.loss_dice: 2.0032, decode.d2.loss_cls: 2.7988, decode.d2.loss_mask: 1.0872, decode.d2.loss_dice: 1.8566, decode.d3.loss_cls: 2.5159, decode.d3.loss_mask: 1.0449, decode.d3.loss_dice: 1.7725, decode.d4.loss_cls: 2.4666, decode.d4.loss_mask: 1.0643, decode.d4.loss_dice: 1.7878, decode.d5.loss_cls: 2.4558, decode.d5.loss_mask: 1.0743, decode.d5.loss_dice: 1.7606, decode.d6.loss_cls: 2.4059, decode.d6.loss_mask: 1.0501, decode.d6.loss_dice: 1.7521, decode.d7.loss_cls: 2.3855, decode.d7.loss_mask: 1.0667, decode.d7.loss_dice: 1.7458, decode.d8.loss_cls: 2.3613, decode.d8.loss_mask: 1.0693, decode.d8.loss_dice: 1.7479, loss: 58.6609
2023-02-21 14:01:50,068 - mmseg - INFO - Iter [450/80000]	lr: 4.274e-07, eta: 2 days, 12:34:03, time: 2.743, data_time: 0.020, memory: 31487, decode.loss_cls: 2.2412, decode.loss_mask: 1.0514, decode.loss_dice: 1.7096, decode.d0.loss_cls: 5.9977, decode.d0.loss_mask: 1.3574, decode.d0.loss_dice: 2.2078, decode.d1.loss_cls: 3.0260, decode.d1.loss_mask: 1.1061, decode.d1.loss_dice: 1.9403, decode.d2.loss_cls: 2.5235, decode.d2.loss_mask: 1.0865, decode.d2.loss_dice: 1.7966, decode.d3.loss_cls: 2.2923, decode.d3.loss_mask: 1.0655, decode.d3.loss_dice: 1.7478, decode.d4.loss_cls: 2.2361, decode.d4.loss_mask: 1.0554, decode.d4.loss_dice: 1.7205, decode.d5.loss_cls: 2.2515, decode.d5.loss_mask: 1.0685, decode.d5.loss_dice: 1.7214, decode.d6.loss_cls: 2.2229, decode.d6.loss_mask: 1.0455, decode.d6.loss_dice: 1.7271, decode.d7.loss_cls: 2.2266, decode.d7.loss_mask: 1.0498, decode.d7.loss_dice: 1.7005, decode.d8.loss_cls: 2.2121, decode.d8.loss_mask: 1.0506, decode.d8.loss_dice: 1.7124, loss: 56.1505
2023-02-21 14:04:07,258 - mmseg - INFO - Iter [500/80000]	lr: 4.747e-07, eta: 2 days, 12:32:08, time: 2.744, data_time: 0.020, memory: 31487, decode.loss_cls: 1.9287, decode.loss_mask: 1.0652, decode.loss_dice: 1.6879, decode.d0.loss_cls: 5.9788, decode.d0.loss_mask: 1.2941, decode.d0.loss_dice: 2.1559, decode.d1.loss_cls: 2.7730, decode.d1.loss_mask: 1.0681, decode.d1.loss_dice: 1.8867, decode.d2.loss_cls: 2.1989, decode.d2.loss_mask: 1.0813, decode.d2.loss_dice: 1.7676, decode.d3.loss_cls: 1.9701, decode.d3.loss_mask: 1.0666, decode.d3.loss_dice: 1.7244, decode.d4.loss_cls: 1.9336, decode.d4.loss_mask: 1.0673, decode.d4.loss_dice: 1.7150, decode.d5.loss_cls: 1.9277, decode.d5.loss_mask: 1.0680, decode.d5.loss_dice: 1.6927, decode.d6.loss_cls: 1.9063, decode.d6.loss_mask: 1.0666, decode.d6.loss_dice: 1.7058, decode.d7.loss_cls: 1.8981, decode.d7.loss_mask: 1.0696, decode.d7.loss_dice: 1.7089, decode.d8.loss_cls: 1.8950, decode.d8.loss_mask: 1.0655, decode.d8.loss_dice: 1.7058, loss: 53.0730
2023-02-21 14:06:24,137 - mmseg - INFO - Iter [550/80000]	lr: 5.219e-07, eta: 2 days, 12:29:24, time: 2.738, data_time: 0.023, memory: 31487, decode.loss_cls: 1.6362, decode.loss_mask: 0.9482, decode.loss_dice: 1.6551, decode.d0.loss_cls: 6.0090, decode.d0.loss_mask: 1.1333, decode.d0.loss_dice: 2.0520, decode.d1.loss_cls: 2.4480, decode.d1.loss_mask: 0.9921, decode.d1.loss_dice: 1.7883, decode.d2.loss_cls: 1.9077, decode.d2.loss_mask: 0.9792, decode.d2.loss_dice: 1.6836, decode.d3.loss_cls: 1.7009, decode.d3.loss_mask: 0.9531, decode.d3.loss_dice: 1.6714, decode.d4.loss_cls: 1.6571, decode.d4.loss_mask: 0.9505, decode.d4.loss_dice: 1.6517, decode.d5.loss_cls: 1.6638, decode.d5.loss_mask: 0.9551, decode.d5.loss_dice: 1.6335, decode.d6.loss_cls: 1.6518, decode.d6.loss_mask: 0.9578, decode.d6.loss_dice: 1.6461, decode.d7.loss_cls: 1.6251, decode.d7.loss_mask: 0.9580, decode.d7.loss_dice: 1.6328, decode.d8.loss_cls: 1.6168, decode.d8.loss_mask: 0.9583, decode.d8.loss_dice: 1.6410, loss: 48.7575
2023-02-21 14:08:40,811 - mmseg - INFO - Iter [600/80000]	lr: 5.691e-07, eta: 2 days, 12:26:18, time: 2.733, data_time: 0.019, memory: 31487, decode.loss_cls: 1.4925, decode.loss_mask: 0.9609, decode.loss_dice: 1.5580, decode.d0.loss_cls: 6.0161, decode.d0.loss_mask: 1.0982, decode.d0.loss_dice: 1.9467, decode.d1.loss_cls: 2.1810, decode.d1.loss_mask: 0.9727, decode.d1.loss_dice: 1.6872, decode.d2.loss_cls: 1.7207, decode.d2.loss_mask: 0.9657, decode.d2.loss_dice: 1.5950, decode.d3.loss_cls: 1.5668, decode.d3.loss_mask: 0.9384, decode.d3.loss_dice: 1.5728, decode.d4.loss_cls: 1.5326, decode.d4.loss_mask: 0.9567, decode.d4.loss_dice: 1.5423, decode.d5.loss_cls: 1.5283, decode.d5.loss_mask: 0.9590, decode.d5.loss_dice: 1.5542, decode.d6.loss_cls: 1.5362, decode.d6.loss_mask: 0.9504, decode.d6.loss_dice: 1.5324, decode.d7.loss_cls: 1.5128, decode.d7.loss_mask: 0.9543, decode.d7.loss_dice: 1.5320, decode.d8.loss_cls: 1.4944, decode.d8.loss_mask: 0.9521, decode.d8.loss_dice: 1.5541, loss: 46.3647
2023-02-21 14:10:57,558 - mmseg - INFO - Iter [650/80000]	lr: 6.162e-07, eta: 2 days, 12:23:28, time: 2.735, data_time: 0.020, memory: 31487, decode.loss_cls: 1.3606, decode.loss_mask: 0.9506, decode.loss_dice: 1.5848, decode.d0.loss_cls: 6.0495, decode.d0.loss_mask: 1.0782, decode.d0.loss_dice: 1.9258, decode.d1.loss_cls: 1.9521, decode.d1.loss_mask: 1.0094, decode.d1.loss_dice: 1.7276, decode.d2.loss_cls: 1.5459, decode.d2.loss_mask: 0.9900, decode.d2.loss_dice: 1.6408, decode.d3.loss_cls: 1.4240, decode.d3.loss_mask: 0.9856, decode.d3.loss_dice: 1.6286, decode.d4.loss_cls: 1.3901, decode.d4.loss_mask: 0.9763, decode.d4.loss_dice: 1.6208, decode.d5.loss_cls: 1.3945, decode.d5.loss_mask: 0.9822, decode.d5.loss_dice: 1.5996, decode.d6.loss_cls: 1.3942, decode.d6.loss_mask: 0.9707, decode.d6.loss_dice: 1.6007, decode.d7.loss_cls: 1.3575, decode.d7.loss_mask: 0.9670, decode.d7.loss_dice: 1.6105, decode.d8.loss_cls: 1.3338, decode.d8.loss_mask: 0.9537, decode.d8.loss_dice: 1.6002, loss: 45.6053
2023-02-21 14:13:14,349 - mmseg - INFO - Iter [700/80000]	lr: 6.632e-07, eta: 2 days, 12:20:48, time: 2.736, data_time: 0.019, memory: 31487, decode.loss_cls: 1.2381, decode.loss_mask: 0.9582, decode.loss_dice: 1.6115, decode.d0.loss_cls: 6.0433, decode.d0.loss_mask: 1.0657, decode.d0.loss_dice: 1.9262, decode.d1.loss_cls: 1.8297, decode.d1.loss_mask: 0.9715, decode.d1.loss_dice: 1.6899, decode.d2.loss_cls: 1.4767, decode.d2.loss_mask: 0.9867, decode.d2.loss_dice: 1.6430, decode.d3.loss_cls: 1.3827, decode.d3.loss_mask: 0.9684, decode.d3.loss_dice: 1.5894, decode.d4.loss_cls: 1.3402, decode.d4.loss_mask: 0.9742, decode.d4.loss_dice: 1.6129, decode.d5.loss_cls: 1.3034, decode.d5.loss_mask: 0.9728, decode.d5.loss_dice: 1.6063, decode.d6.loss_cls: 1.2620, decode.d6.loss_mask: 0.9828, decode.d6.loss_dice: 1.6025, decode.d7.loss_cls: 1.2341, decode.d7.loss_mask: 0.9788, decode.d7.loss_dice: 1.6117, decode.d8.loss_cls: 1.2211, decode.d8.loss_mask: 0.9665, decode.d8.loss_dice: 1.6361, loss: 44.6866
2023-02-21 14:15:33,803 - mmseg - INFO - Iter [750/80000]	lr: 7.102e-07, eta: 2 days, 12:22:53, time: 2.789, data_time: 0.068, memory: 31487, decode.loss_cls: 1.1436, decode.loss_mask: 0.9425, decode.loss_dice: 1.6222, decode.d0.loss_cls: 5.9929, decode.d0.loss_mask: 1.0244, decode.d0.loss_dice: 1.9290, decode.d1.loss_cls: 1.5791, decode.d1.loss_mask: 0.9366, decode.d1.loss_dice: 1.7257, decode.d2.loss_cls: 1.2892, decode.d2.loss_mask: 0.9295, decode.d2.loss_dice: 1.6702, decode.d3.loss_cls: 1.2524, decode.d3.loss_mask: 0.9041, decode.d3.loss_dice: 1.6261, decode.d4.loss_cls: 1.1920, decode.d4.loss_mask: 0.9272, decode.d4.loss_dice: 1.6284, decode.d5.loss_cls: 1.1683, decode.d5.loss_mask: 0.9171, decode.d5.loss_dice: 1.6254, decode.d6.loss_cls: 1.1659, decode.d6.loss_mask: 0.9206, decode.d6.loss_dice: 1.6092, decode.d7.loss_cls: 1.1287, decode.d7.loss_mask: 0.9296, decode.d7.loss_dice: 1.6152, decode.d8.loss_cls: 1.1254, decode.d8.loss_mask: 0.9340, decode.d8.loss_dice: 1.6329, loss: 43.0875
2023-02-21 14:17:51,034 - mmseg - INFO - Iter [800/80000]	lr: 7.572e-07, eta: 2 days, 12:20:44, time: 2.745, data_time: 0.021, memory: 31487, decode.loss_cls: 1.0916, decode.loss_mask: 0.9319, decode.loss_dice: 1.5854, decode.d0.loss_cls: 6.0039, decode.d0.loss_mask: 1.0550, decode.d0.loss_dice: 1.9225, decode.d1.loss_cls: 1.4479, decode.d1.loss_mask: 0.9935, decode.d1.loss_dice: 1.7152, decode.d2.loss_cls: 1.2153, decode.d2.loss_mask: 0.9558, decode.d2.loss_dice: 1.6274, decode.d3.loss_cls: 1.1466, decode.d3.loss_mask: 0.9471, decode.d3.loss_dice: 1.6104, decode.d4.loss_cls: 1.0948, decode.d4.loss_mask: 0.9470, decode.d4.loss_dice: 1.5939, decode.d5.loss_cls: 1.1204, decode.d5.loss_mask: 0.9365, decode.d5.loss_dice: 1.5840, decode.d6.loss_cls: 1.1121, decode.d6.loss_mask: 0.9371, decode.d6.loss_dice: 1.5592, decode.d7.loss_cls: 1.0913, decode.d7.loss_mask: 0.9339, decode.d7.loss_dice: 1.5959, decode.d8.loss_cls: 1.0863, decode.d8.loss_mask: 0.9296, decode.d8.loss_dice: 1.5750, loss: 42.3462
2023-02-21 14:20:08,281 - mmseg - INFO - Iter [850/80000]	lr: 8.040e-07, eta: 2 days, 12:18:36, time: 2.745, data_time: 0.022, memory: 31487, decode.loss_cls: 0.9512, decode.loss_mask: 0.9327, decode.loss_dice: 1.5559, decode.d0.loss_cls: 5.9951, decode.d0.loss_mask: 1.0009, decode.d0.loss_dice: 1.8365, decode.d1.loss_cls: 1.3088, decode.d1.loss_mask: 0.9682, decode.d1.loss_dice: 1.6532, decode.d2.loss_cls: 1.0813, decode.d2.loss_mask: 0.9505, decode.d2.loss_dice: 1.5708, decode.d3.loss_cls: 1.0361, decode.d3.loss_mask: 0.9136, decode.d3.loss_dice: 1.5462, decode.d4.loss_cls: 0.9932, decode.d4.loss_mask: 0.9348, decode.d4.loss_dice: 1.5561, decode.d5.loss_cls: 0.9669, decode.d5.loss_mask: 0.9338, decode.d5.loss_dice: 1.5594, decode.d6.loss_cls: 0.9717, decode.d6.loss_mask: 0.9369, decode.d6.loss_dice: 1.5396, decode.d7.loss_cls: 0.9715, decode.d7.loss_mask: 0.9343, decode.d7.loss_dice: 1.5610, decode.d8.loss_cls: 0.9356, decode.d8.loss_mask: 0.9349, decode.d8.loss_dice: 1.5686, loss: 40.5992
2023-02-21 14:22:30,245 - mmseg - INFO - Iter [900/80000]	lr: 8.509e-07, eta: 2 days, 12:23:21, time: 2.839, data_time: 0.024, memory: 31487, decode.loss_cls: 1.0539, decode.loss_mask: 0.9506, decode.loss_dice: 1.5874, decode.d0.loss_cls: 6.0107, decode.d0.loss_mask: 1.0480, decode.d0.loss_dice: 1.8714, decode.d1.loss_cls: 1.2696, decode.d1.loss_mask: 0.9745, decode.d1.loss_dice: 1.7148, decode.d2.loss_cls: 1.1330, decode.d2.loss_mask: 0.9405, decode.d2.loss_dice: 1.6545, decode.d3.loss_cls: 1.0841, decode.d3.loss_mask: 0.9356, decode.d3.loss_dice: 1.6095, decode.d4.loss_cls: 1.0462, decode.d4.loss_mask: 0.9510, decode.d4.loss_dice: 1.6080, decode.d5.loss_cls: 1.0562, decode.d5.loss_mask: 0.9561, decode.d5.loss_dice: 1.6199, decode.d6.loss_cls: 1.0877, decode.d6.loss_mask: 0.9537, decode.d6.loss_dice: 1.5958, decode.d7.loss_cls: 1.0409, decode.d7.loss_mask: 0.9552, decode.d7.loss_dice: 1.6174, decode.d8.loss_cls: 1.0459, decode.d8.loss_mask: 0.9601, decode.d8.loss_dice: 1.5993, loss: 41.9314
2023-02-21 14:25:04,184 - mmseg - INFO - Iter [950/80000]	lr: 8.976e-07, eta: 2 days, 12:43:57, time: 3.079, data_time: 0.024, memory: 31487, decode.loss_cls: 0.9372, decode.loss_mask: 0.9124, decode.loss_dice: 1.5735, decode.d0.loss_cls: 5.9845, decode.d0.loss_mask: 0.9945, decode.d0.loss_dice: 1.8541, decode.d1.loss_cls: 1.1046, decode.d1.loss_mask: 0.9488, decode.d1.loss_dice: 1.6915, decode.d2.loss_cls: 0.9935, decode.d2.loss_mask: 0.9186, decode.d2.loss_dice: 1.5912, decode.d3.loss_cls: 0.9766, decode.d3.loss_mask: 0.9272, decode.d3.loss_dice: 1.5863, decode.d4.loss_cls: 0.9283, decode.d4.loss_mask: 0.9293, decode.d4.loss_dice: 1.5844, decode.d5.loss_cls: 0.9190, decode.d5.loss_mask: 0.9239, decode.d5.loss_dice: 1.5719, decode.d6.loss_cls: 0.9188, decode.d6.loss_mask: 0.9158, decode.d6.loss_dice: 1.5718, decode.d7.loss_cls: 0.8996, decode.d7.loss_mask: 0.9089, decode.d7.loss_dice: 1.5641, decode.d8.loss_cls: 0.8847, decode.d8.loss_mask: 0.9153, decode.d8.loss_dice: 1.5838, loss: 40.0140
2023-02-21 14:27:22,225 - mmseg - INFO - Saving checkpoint at 1000 iterations
2023-02-21 14:27:45,617 - mmseg - INFO - Exp name: my_city.py
2023-02-21 14:27:45,617 - mmseg - INFO - Iter [1000/80000]	lr: 9.443e-07, eta: 2 days, 13:12:07, time: 3.229, data_time: 0.022, memory: 31487, decode.loss_cls: 0.9415, decode.loss_mask: 0.9140, decode.loss_dice: 1.5921, decode.d0.loss_cls: 5.9566, decode.d0.loss_mask: 1.0145, decode.d0.loss_dice: 1.8355, decode.d1.loss_cls: 1.0398, decode.d1.loss_mask: 0.9405, decode.d1.loss_dice: 1.7151, decode.d2.loss_cls: 0.9529, decode.d2.loss_mask: 0.9413, decode.d2.loss_dice: 1.6358, decode.d3.loss_cls: 0.9734, decode.d3.loss_mask: 0.9058, decode.d3.loss_dice: 1.5943, decode.d4.loss_cls: 0.9302, decode.d4.loss_mask: 0.9002, decode.d4.loss_dice: 1.5880, decode.d5.loss_cls: 0.9519, decode.d5.loss_mask: 0.9109, decode.d5.loss_dice: 1.5994, decode.d6.loss_cls: 0.9337, decode.d6.loss_mask: 0.9095, decode.d6.loss_dice: 1.5840, decode.d7.loss_cls: 0.9020, decode.d7.loss_mask: 0.9133, decode.d7.loss_dice: 1.5881, decode.d8.loss_cls: 0.9019, decode.d8.loss_mask: 0.9184, decode.d8.loss_dice: 1.5967, loss: 40.0813
[                                                  ] 0/357, elapsed: 0s, ETA:[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[                                ] 1/357, 0.0 task/s, elapsed: 66s, ETA: 23379s[                               ] 2/357, 0.0 task/s, elapsed: 128s, ETA: 22679s[                               ] 3/357, 0.0 task/s, elapsed: 190s, ETA: 22387s[                               ] 4/357, 0.0 task/s, elapsed: 252s, ETA: 22237s[                               ] 5/357, 0.0 task/s, elapsed: 314s, ETA: 22124s[                               ] 6/357, 0.0 task/s, elapsed: 380s, ETA: 22257s[                               ] 7/357, 0.0 task/s, elapsed: 447s, ETA: 22331s[                               ] 8/357, 0.0 task/s, elapsed: 513s, ETA: 22375s[                               ] 9/357, 0.0 task/s, elapsed: 575s, ETA: 22242s[                              ] 10/357, 0.0 task/s, elapsed: 637s, ETA: 22114s[                              ] 11/357, 0.0 task/s, elapsed: 705s, ETA: 22184s[>                             ] 12/357, 0.0 task/s, elapsed: 767s, ETA: 22065s[>                             ] 13/357, 0.0 task/s, elapsed: 830s, ETA: 21952s[>                             ] 14/357, 0.0 task/s, elapsed: 896s, ETA: 21947s[>                             ] 15/357, 0.0 task/s, elapsed: 962s, ETA: 21933s[>                            ] 16/357, 0.0 task/s, elapsed: 1030s, ETA: 21951s[>                            ] 17/357, 0.0 task/s, elapsed: 1098s, ETA: 21958s[>                            ] 18/357, 0.0 task/s, elapsed: 1160s, ETA: 21854s[>                            ] 19/357, 0.0 task/s, elapsed: 1223s, ETA: 21750s[>                            ] 20/357, 0.0 task/s, elapsed: 1289s, ETA: 21715s[>                            ] 21/357, 0.0 task/s, elapsed: 1355s, ETA: 21678s[>                            ] 22/357, 0.0 task/s, elapsed: 1417s, ETA: 21579s[>                            ] 23/357, 0.0 task/s, elapsed: 1479s, ETA: 21484sTraceback (most recent call last):
  File "train.py", line 215, in <module>
    main()
  File "train.py", line 204, in main
    train_segmentor(
  File "/root/miniconda3/lib/python3.8/site-packages/mmseg/apis/train.py", line 167, in train_segmentor
    runner.run(data_loaders, cfg.workflow)
  File "/root/miniconda3/lib/python3.8/site-packages/mmcv/runner/iter_based_runner.py", line 134, in run
    iter_runner(iter_loaders[i], **kwargs)
  File "/root/miniconda3/lib/python3.8/site-packages/mmcv/runner/iter_based_runner.py", line 67, in train
    self.call_hook('after_train_iter')
  File "/root/miniconda3/lib/python3.8/site-packages/mmcv/runner/base_runner.py", line 309, in call_hook
    getattr(hook, fn_name)(self)
  File "/root/miniconda3/lib/python3.8/site-packages/mmcv/runner/hooks/evaluation.py", line 262, in after_train_iter
    self._do_evaluate(runner)
  File "/root/miniconda3/lib/python3.8/site-packages/mmseg/core/evaluation/eval_hooks.py", line 49, in _do_evaluate
    results = single_gpu_test(
  File "/root/miniconda3/lib/python3.8/site-packages/mmseg/apis/test.py", line 91, in single_gpu_test
    result = model(return_loss=False, **data)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/miniconda3/lib/python3.8/site-packages/mmcv/parallel/data_parallel.py", line 50, in forward
    return super().forward(*inputs, **kwargs)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 166, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/miniconda3/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py", line 98, in new_func
    return old_func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.8/site-packages/mmseg/models/segmentors/base.py", line 110, in forward
    return self.forward_test(img, img_metas, **kwargs)
  File "/root/miniconda3/lib/python3.8/site-packages/mmseg/models/segmentors/base.py", line 94, in forward_test
    return self.aug_test(imgs, img_metas, **kwargs)
  File "/root/autodl-tmp/ViT-Adapter/segmentation/mmseg_custom/models/segmentors/encoder_decoder_mask2former_aug.py", line 282, in aug_test
    cur_seg_logit = self.inference(imgs[i], img_metas[i], rescale)
  File "/root/autodl-tmp/ViT-Adapter/segmentation/mmseg_custom/models/segmentors/encoder_decoder_mask2former_aug.py", line 244, in inference
    seg_logit = self.slide_inference(img, img_meta, rescale)
  File "/root/autodl-tmp/ViT-Adapter/segmentation/mmseg_custom/models/segmentors/encoder_decoder_mask2former_aug.py", line 180, in slide_inference
    crop_seg_logit = self.encode_decode(crop_img, img_meta)
  File "/root/autodl-tmp/ViT-Adapter/segmentation/mmseg_custom/models/segmentors/encoder_decoder_mask2former_aug.py", line 73, in encode_decode
    x = self.extract_feat(img)
  File "/root/autodl-tmp/ViT-Adapter/segmentation/mmseg_custom/models/segmentors/encoder_decoder_mask2former_aug.py", line 65, in extract_feat
    x = self.backbone(img)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/autodl-tmp/ViT-Adapter/segmentation/mmseg_custom/models/backbones/beit_adapter.py", line 115, in forward
    x, c, cls = layer(x, c, cls, self.blocks[indexes[0]:indexes[-1] + 1],
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/autodl-tmp/ViT-Adapter/segmentation/mmseg_custom/models/backbones/adapter_modules.py", line 224, in forward
    c = self.extractor(query=c, reference_points=deform_inputs2[0],
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/autodl-tmp/ViT-Adapter/segmentation/mmseg_custom/models/backbones/adapter_modules.py", line 122, in forward
    query = _inner_forward(query, feat)
  File "/root/autodl-tmp/ViT-Adapter/segmentation/mmseg_custom/models/backbones/adapter_modules.py", line 110, in _inner_forward
    attn = self.attn(self.query_norm(query), reference_points,
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/autodl-tmp/ViT-Adapter/segmentation/ops/modules/ms_deform_attn.py", line 99, in forward
    assert (input_spatial_shapes[:, 0] *
KeyboardInterrupt
2023-02-21 22:35:22,855 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.10 (default, Jun  4 2021, 15:09:15) [GCC 7.5.0]
CUDA available: True
GPU 0: NVIDIA A100-PCIE-40GB
CUDA_HOME: /usr/local/cuda
NVCC: Build cuda_11.1.TC455_06.29190527_0
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.9.0+cu111
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.10.0+cu111
OpenCV: 4.6.0
MMCV: 1.4.2
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.1
MMSegmentation: 0.20.2+a1b84a3
------------------------------------------------------------

2023-02-21 22:35:22,855 - mmseg - INFO - Distributed training: False
2023-02-21 22:35:23,450 - mmseg - INFO - Config:
num_things_classes = 0
num_stuff_classes = 16
num_classes = 16
norm_cfg = dict(type='SyncBN', requires_grad=True)
model = dict(
    type='EncoderDecoderMask2FormerAug',
    pretrained='pretrained/beit_large_patch16_224_pt22k_ft22k.pth',
    backbone=dict(
        type='BEiTAdapter',
        patch_size=16,
        embed_dim=1024,
        depth=24,
        num_heads=16,
        mlp_ratio=4,
        qkv_bias=True,
        use_abs_pos_emb=False,
        use_rel_pos_bias=True,
        img_size=896,
        init_values=1e-06,
        drop_path_rate=0.3,
        conv_inplane=64,
        n_points=4,
        deform_num_heads=16,
        cffn_ratio=0.25,
        deform_ratio=0.5,
        with_cp=True,
        interaction_indexes=[[0, 5], [6, 11], [12, 17], [18, 23]]),
    decode_head=dict(
        type='Mask2FormerHead',
        in_channels=[1024, 1024, 1024, 1024],
        feat_channels=1024,
        out_channels=1024,
        in_index=[0, 1, 2, 3],
        num_things_classes=0,
        num_stuff_classes=16,
        num_queries=100,
        num_transformer_feat_level=3,
        pixel_decoder=dict(
            type='MSDeformAttnPixelDecoder',
            num_outs=3,
            norm_cfg=dict(type='GN', num_groups=32),
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=1024,
                        num_heads=32,
                        num_levels=3,
                        num_points=4,
                        im2col_step=64,
                        dropout=0.0,
                        batch_first=False,
                        norm_cfg=None,
                        init_cfg=None),
                    ffn_cfgs=dict(
                        type='FFN',
                        embed_dims=1024,
                        feedforward_channels=4096,
                        num_fcs=2,
                        ffn_drop=0.0,
                        act_cfg=dict(type='ReLU', inplace=True),
                        with_cp=True),
                    operation_order=('self_attn', 'norm', 'ffn', 'norm')),
                init_cfg=None),
            positional_encoding=dict(
                type='SinePositionalEncoding', num_feats=512, normalize=True),
            init_cfg=None),
        enforce_decoder_input_project=False,
        positional_encoding=dict(
            type='SinePositionalEncoding', num_feats=512, normalize=True),
        transformer_decoder=dict(
            type='DetrTransformerDecoder',
            return_intermediate=True,
            num_layers=9,
            transformerlayers=dict(
                type='DetrTransformerDecoderLayer',
                attn_cfgs=dict(
                    type='MultiheadAttention',
                    embed_dims=1024,
                    num_heads=32,
                    attn_drop=0.0,
                    proj_drop=0.0,
                    dropout_layer=None,
                    batch_first=False),
                ffn_cfgs=dict(
                    embed_dims=1024,
                    feedforward_channels=4096,
                    num_fcs=2,
                    act_cfg=dict(type='ReLU', inplace=True),
                    ffn_drop=0.0,
                    dropout_layer=None,
                    add_identity=True,
                    with_cp=True),
                feedforward_channels=4096,
                operation_order=('cross_attn', 'norm', 'self_attn', 'norm',
                                 'ffn', 'norm')),
            init_cfg=None),
        loss_cls=dict(
            type='CrossEntropyLoss',
            use_sigmoid=False,
            loss_weight=2.0,
            reduction='mean',
            class_weight=[
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 0.1
            ]),
        loss_mask=dict(
            type='CrossEntropyLoss',
            use_sigmoid=True,
            reduction='mean',
            loss_weight=5.0),
        loss_dice=dict(
            type='DiceLoss',
            use_sigmoid=True,
            activate=True,
            reduction='mean',
            naive_dice=True,
            eps=1.0,
            loss_weight=5.0)),
    train_cfg=dict(
        num_points=12544,
        oversample_ratio=3.0,
        importance_sample_ratio=0.75,
        assigner=dict(
            type='MaskHungarianAssigner',
            cls_cost=dict(type='ClassificationCost', weight=2.0),
            mask_cost=dict(
                type='CrossEntropyLossCost', weight=5.0, use_sigmoid=True),
            dice_cost=dict(
                type='DiceCost', weight=5.0, pred_act=True, eps=1.0)),
        sampler=dict(type='MaskPseudoSampler')),
    test_cfg=dict(
        panoptic_on=True,
        semantic_on=False,
        instance_on=True,
        max_per_image=100,
        iou_thr=0.8,
        filter_low_score=True,
        mode='slide',
        crop_size=(896, 896),
        stride=(512, 512)),
    init_cfg=None)
dataset_type = 'MyDataset'
data_root = '/root/autodl-tmp/data'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (896, 896)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=(896, 896), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(896, 896), pad_val=0, seg_pad_val=255),
    dict(type='ToMask'),
    dict(type='DefaultFormatBundle'),
    dict(
        type='Collect',
        keys=['img', 'gt_semantic_seg', 'gt_masks', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(2048, 1024),
        img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0],
        flip=True,
        transforms=[
            dict(
                type='SETR_Resize',
                keep_ratio=True,
                crop_size=(896, 896),
                setr_multi_scale=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=1,
    train=dict(
        type='MyDataset',
        data_root='/root/autodl-tmp/data',
        img_dir='images/train',
        ann_dir='annotations/train',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(
                type='Resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),
            dict(type='RandomCrop', crop_size=(896, 896), cat_max_ratio=0.75),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(896, 896), pad_val=0, seg_pad_val=255),
            dict(type='ToMask'),
            dict(type='DefaultFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'gt_semantic_seg', 'gt_masks', 'gt_labels'])
        ]),
    val=dict(
        type='MyDataset',
        data_root='/root/autodl-tmp/data',
        img_dir='images/val',
        ann_dir='annotations/val',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(2048, 1024),
                img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0],
                flip=True,
                transforms=[
                    dict(
                        type='SETR_Resize',
                        keep_ratio=True,
                        crop_size=(896, 896),
                        setr_multi_scale=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='MyDataset',
        data_root='/root/autodl-tmp/data',
        img_dir='images/val',
        ann_dir='annotations/val',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(2048, 1024),
                img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0],
                flip=True,
                transforms=[
                    dict(
                        type='SETR_Resize',
                        keep_ratio=True,
                        crop_size=(896, 896),
                        setr_multi_scale=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
log_config = dict(
    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = 'work_dirs/my_city/latest.pth'
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
optimizer = dict(
    type='AdamW',
    lr=2e-05,
    betas=(0.9, 0.999),
    weight_decay=0.05,
    constructor='LayerDecayOptimizerConstructor',
    paramwise_cfg=dict(num_layers=24, layer_decay_rate=0.9))
optimizer_config = dict()
lr_config = dict(
    policy='poly',
    warmup='linear',
    warmup_iters=1500,
    warmup_ratio=1e-06,
    power=1.0,
    min_lr=0.0,
    by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=80000)
checkpoint_config = dict(by_epoch=False, interval=1000, max_keep_ckpts=1)
evaluation = dict(
    interval=1000, metric='mIoU', pre_eval=True, save_best='mIoU')
pretrained = 'pretrained/beit_large_patch16_224_pt22k_ft22k.pth'
work_dir = './work_dirs/my_city'
gpu_ids = range(0, 1)
auto_resume = False

2023-02-21 22:35:23,453 - mmseg - INFO - Set random seed to 613560434, deterministic: False
Position interpolate for blocks.0.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.1.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.2.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.3.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.4.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.5.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.6.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.7.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.8.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.9.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.10.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.11.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.12.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.13.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.14.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.15.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.16.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.17.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.18.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.19.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.20.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.21.attn.relative_position_bias_table from 27x27 to 111x111
2023-02-21 22:35:33,695 - mmseg - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc_norm.weight, fc_norm.bias, head.weight, head.bias

missing keys in source state_dict: blocks.0.attn.relative_position_index, blocks.1.attn.relative_position_index, blocks.2.attn.relative_position_index, blocks.3.attn.relative_position_index, blocks.4.attn.relative_position_index, blocks.5.attn.relative_position_index, blocks.6.attn.relative_position_index, blocks.7.attn.relative_position_index, blocks.8.attn.relative_position_index, blocks.9.attn.relative_position_index, blocks.10.attn.relative_position_index, blocks.11.attn.relative_position_index, blocks.12.attn.relative_position_index, blocks.13.attn.relative_position_index, blocks.14.attn.relative_position_index, blocks.15.attn.relative_position_index, blocks.16.attn.relative_position_index, blocks.17.attn.relative_position_index, blocks.18.attn.relative_position_index, blocks.19.attn.relative_position_index, blocks.20.attn.relative_position_index, blocks.21.attn.relative_position_index, blocks.22.attn.relative_position_index, blocks.23.attn.relative_position_index

/root/autodl-tmp/ViT-Adapter/segmentation/mmseg_custom/models/losses/cross_entropy_loss.py:230: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.
  warnings.warn(
train.py:179: UserWarning: SyncBN is only supported with DDP. To be compatible with DP, we convert SyncBN to BN. Please use dist_train.sh which can avoid this error.
  warnings.warn(
2023-02-21 22:35:37,754 - mmseg - INFO - EncoderDecoderMask2FormerAug(
  (backbone): BEiTAdapter(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.013043479062616825)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.02608695812523365)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.03913043811917305)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.0521739162504673)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.06521739810705185)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.0782608762383461)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.09130435436964035)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1043478325009346)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.11739131063222885)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1304347962141037)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.14347827434539795)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (12): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1565217524766922)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (13): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.16956523060798645)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (14): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1826087087392807)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (15): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.19565218687057495)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (16): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.2086956650018692)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (17): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.22173914313316345)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (18): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.2347826212644577)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (19): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.24782609939575195)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (20): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.260869562625885)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (21): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.27391305565834045)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (22): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.2869565188884735)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (23): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.30000001192092896)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (spm): SpatialPriorModule(
      (stem): Sequential(
        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (7): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (8): ReLU(inplace=True)
        (9): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      )
      (conv2): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (conv3): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (conv4): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (fc1): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))
      (fc2): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))
      (fc3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
      (fc4): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
    )
    (interactions): Sequential(
      (0): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=192, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=64, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1024, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
      (1): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=192, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=64, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1024, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
      (2): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=192, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=64, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1024, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
      (3): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=192, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=64, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1024, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (extra_extractors): Sequential(
          (0): Extractor(
            (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=1024, out_features=128, bias=True)
              (attention_weights): Linear(in_features=1024, out_features=64, bias=True)
              (value_proj): Linear(in_features=1024, out_features=512, bias=True)
              (output_proj): Linear(in_features=512, out_features=1024, bias=True)
            )
            (ffn): ConvFFN(
              (fc1): Linear(in_features=1024, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (drop_path): DropPath()
          )
          (1): Extractor(
            (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=1024, out_features=128, bias=True)
              (attention_weights): Linear(in_features=1024, out_features=64, bias=True)
              (value_proj): Linear(in_features=1024, out_features=512, bias=True)
              (output_proj): Linear(in_features=512, out_features=1024, bias=True)
            )
            (ffn): ConvFFN(
              (fc1): Linear(in_features=1024, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (drop_path): DropPath()
          )
        )
      )
    )
    (up): ConvTranspose2d(1024, 1024, kernel_size=(2, 2), stride=(2, 2))
    (norm1): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm2): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm3): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm4): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (decode_head): Mask2FormerHead(
    input_transform=multiple_select, ignore_index=255, align_corners=False
    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)
    (conv_seg): None
    (dropout): Dropout2d(p=0.1, inplace=False)
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
        (1): ConvModule(
          (conv): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
        (2): ConvModule(
          (conv): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
      )
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (postional_encoding): SinePositionalEncoding(num_feats=512, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
      (level_encoding): Embedding(3, 1024)
      (lateral_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
      )
      (output_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
          (activate): ReLU(inplace=True)
        )
      )
      (mask_feature): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
    )
    (transformer_decoder): DetrTransformerDecoder(
      (layers): ModuleList(
        (0): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (6): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (7): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (8): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (decoder_input_projs): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
    )
    (decoder_positional_encoding): SinePositionalEncoding(num_feats=512, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (query_embed): Embedding(100, 1024)
    (query_feat): Embedding(100, 1024)
    (level_embed): Embedding(3, 1024)
    (cls_embed): Linear(in_features=1024, out_features=17, bias=True)
    (mask_embed): Sequential(
      (0): Linear(in_features=1024, out_features=1024, bias=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=1024, out_features=1024, bias=True)
      (3): ReLU(inplace=True)
      (4): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (loss_cls): CrossEntropyLoss(avg_non_ignore=False)
    (loss_mask): CrossEntropyLoss(avg_non_ignore=False)
    (loss_dice): DiceLoss()
  )
)
2023-02-21 22:35:37,778 - mmseg - INFO - Loaded 1429 images
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.22.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.23.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
{'num_layers': 24, 'layer_decay_rate': 0.9}
Build LayerDecayOptimizerConstructor 0.900000 - 26
Param groups = {
  "layer_0_decay": {
    "param_names": [
      "backbone.cls_token",
      "backbone.patch_embed.proj.weight",
      "decode_head.query_embed.weight",
      "decode_head.query_feat.weight",
      "decode_head.level_embed.weight",
      "decode_head.cls_embed.weight",
      "decode_head.mask_embed.0.weight",
      "decode_head.mask_embed.2.weight",
      "decode_head.mask_embed.4.weight"
    ],
    "lr_scale": 0.0717897987691853,
    "lr": 1.4357959753837061e-06,
    "weight_decay": 0.05
  },
  "layer_25_decay": {
    "param_names": [
      "backbone.level_embed",
      "backbone.spm.stem.0.weight",
      "backbone.spm.stem.3.weight",
      "backbone.spm.stem.6.weight",
      "backbone.spm.conv2.0.weight",
      "backbone.spm.conv3.0.weight",
      "backbone.spm.conv4.0.weight",
      "backbone.spm.fc1.weight",
      "backbone.spm.fc2.weight",
      "backbone.spm.fc3.weight",
      "backbone.spm.fc4.weight",
      "backbone.interactions.0.injector.attn.sampling_offsets.weight",
      "backbone.interactions.0.injector.attn.attention_weights.weight",
      "backbone.interactions.0.injector.attn.value_proj.weight",
      "backbone.interactions.0.injector.attn.output_proj.weight",
      "backbone.interactions.0.extractor.attn.sampling_offsets.weight",
      "backbone.interactions.0.extractor.attn.attention_weights.weight",
      "backbone.interactions.0.extractor.attn.value_proj.weight",
      "backbone.interactions.0.extractor.attn.output_proj.weight",
      "backbone.interactions.0.extractor.ffn.fc1.weight",
      "backbone.interactions.0.extractor.ffn.dwconv.dwconv.weight",
      "backbone.interactions.0.extractor.ffn.fc2.weight",
      "backbone.interactions.1.injector.attn.sampling_offsets.weight",
      "backbone.interactions.1.injector.attn.attention_weights.weight",
      "backbone.interactions.1.injector.attn.value_proj.weight",
      "backbone.interactions.1.injector.attn.output_proj.weight",
      "backbone.interactions.1.extractor.attn.sampling_offsets.weight",
      "backbone.interactions.1.extractor.attn.attention_weights.weight",
      "backbone.interactions.1.extractor.attn.value_proj.weight",
      "backbone.interactions.1.extractor.attn.output_proj.weight",
      "backbone.interactions.1.extractor.ffn.fc1.weight",
      "backbone.interactions.1.extractor.ffn.dwconv.dwconv.weight",
      "backbone.interactions.1.extractor.ffn.fc2.weight",
      "backbone.interactions.2.injector.attn.sampling_offsets.weight",
      "backbone.interactions.2.injector.attn.attention_weights.weight",
      "backbone.interactions.2.injector.attn.value_proj.weight",
      "backbone.interactions.2.injector.attn.output_proj.weight",
      "backbone.interactions.2.extractor.attn.sampling_offsets.weight",
      "backbone.interactions.2.extractor.attn.attention_weights.weight",
      "backbone.interactions.2.extractor.attn.value_proj.weight",
      "backbone.interactions.2.extractor.attn.output_proj.weight",
      "backbone.interactions.2.extractor.ffn.fc1.weight",
      "backbone.interactions.2.extractor.ffn.dwconv.dwconv.weight",
      "backbone.interactions.2.extractor.ffn.fc2.weight",
      "backbone.interactions.3.injector.attn.sampling_offsets.weight",
      "backbone.interactions.3.injector.attn.attention_weights.weight",
      "backbone.interactions.3.injector.attn.value_proj.weight",
      "backbone.interactions.3.injector.attn.output_proj.weight",
      "backbone.interactions.3.extractor.attn.sampling_offsets.weight",
      "backbone.interactions.3.extractor.attn.attention_weights.weight",
      "backbone.interactions.3.extractor.attn.value_proj.weight",
      "backbone.interactions.3.extractor.attn.output_proj.weight",
      "backbone.interactions.3.extractor.ffn.fc1.weight",
      "backbone.interactions.3.extractor.ffn.dwconv.dwconv.weight",
      "backbone.interactions.3.extractor.ffn.fc2.weight",
      "backbone.interactions.3.extra_extractors.0.attn.sampling_offsets.weight",
      "backbone.interactions.3.extra_extractors.0.attn.attention_weights.weight",
      "backbone.interactions.3.extra_extractors.0.attn.value_proj.weight",
      "backbone.interactions.3.extra_extractors.0.attn.output_proj.weight",
      "backbone.interactions.3.extra_extractors.0.ffn.fc1.weight",
      "backbone.interactions.3.extra_extractors.0.ffn.dwconv.dwconv.weight",
      "backbone.interactions.3.extra_extractors.0.ffn.fc2.weight",
      "backbone.interactions.3.extra_extractors.1.attn.sampling_offsets.weight",
      "backbone.interactions.3.extra_extractors.1.attn.attention_weights.weight",
      "backbone.interactions.3.extra_extractors.1.attn.value_proj.weight",
      "backbone.interactions.3.extra_extractors.1.attn.output_proj.weight",
      "backbone.interactions.3.extra_extractors.1.ffn.fc1.weight",
      "backbone.interactions.3.extra_extractors.1.ffn.dwconv.dwconv.weight",
      "backbone.interactions.3.extra_extractors.1.ffn.fc2.weight",
      "backbone.up.weight",
      "decode_head.pixel_decoder.input_convs.0.conv.weight",
      "decode_head.pixel_decoder.input_convs.1.conv.weight",
      "decode_head.pixel_decoder.input_convs.2.conv.weight",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.level_encoding.weight",
      "decode_head.pixel_decoder.lateral_convs.0.conv.weight",
      "decode_head.pixel_decoder.output_convs.0.conv.weight",
      "decode_head.pixel_decoder.mask_feature.weight",
      "decode_head.transformer_decoder.layers.0.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.0.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.0.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.0.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.0.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.0.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.1.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.1.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.1.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.1.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.1.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.1.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.2.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.2.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.2.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.2.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.2.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.2.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.3.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.3.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.3.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.3.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.3.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.3.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.4.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.4.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.4.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.4.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.4.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.4.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.5.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.5.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.5.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.5.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.5.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.5.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.6.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.6.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.6.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.6.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.6.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.6.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.7.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.7.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.7.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.7.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.7.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.7.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.8.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.8.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.8.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.8.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.8.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.8.ffns.0.layers.1.weight"
    ],
    "lr_scale": 1.0,
    "lr": 2e-05,
    "weight_decay": 0.05
  },
  "layer_0_no_decay": {
    "param_names": [
      "backbone.patch_embed.proj.bias",
      "decode_head.cls_embed.bias",
      "decode_head.mask_embed.0.bias",
      "decode_head.mask_embed.2.bias",
      "decode_head.mask_embed.4.bias"
    ],
    "lr_scale": 0.0717897987691853,
    "lr": 1.4357959753837061e-06,
    "weight_decay": 0.0
  },
  "layer_1_no_decay": {
    "param_names": [
      "backbone.blocks.0.gamma_1",
      "backbone.blocks.0.gamma_2",
      "backbone.blocks.0.norm1.weight",
      "backbone.blocks.0.norm1.bias",
      "backbone.blocks.0.attn.q_bias",
      "backbone.blocks.0.attn.v_bias",
      "backbone.blocks.0.attn.proj.bias",
      "backbone.blocks.0.norm2.weight",
      "backbone.blocks.0.norm2.bias",
      "backbone.blocks.0.mlp.fc1.bias",
      "backbone.blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.07976644307687256,
    "lr": 1.5953288615374513e-06,
    "weight_decay": 0.0
  },
  "layer_1_decay": {
    "param_names": [
      "backbone.blocks.0.attn.relative_position_bias_table",
      "backbone.blocks.0.attn.qkv.weight",
      "backbone.blocks.0.attn.proj.weight",
      "backbone.blocks.0.mlp.fc1.weight",
      "backbone.blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.07976644307687256,
    "lr": 1.5953288615374513e-06,
    "weight_decay": 0.05
  },
  "layer_2_no_decay": {
    "param_names": [
      "backbone.blocks.1.gamma_1",
      "backbone.blocks.1.gamma_2",
      "backbone.blocks.1.norm1.weight",
      "backbone.blocks.1.norm1.bias",
      "backbone.blocks.1.attn.q_bias",
      "backbone.blocks.1.attn.v_bias",
      "backbone.blocks.1.attn.proj.bias",
      "backbone.blocks.1.norm2.weight",
      "backbone.blocks.1.norm2.bias",
      "backbone.blocks.1.mlp.fc1.bias",
      "backbone.blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.08862938119652507,
    "lr": 1.7725876239305016e-06,
    "weight_decay": 0.0
  },
  "layer_2_decay": {
    "param_names": [
      "backbone.blocks.1.attn.relative_position_bias_table",
      "backbone.blocks.1.attn.qkv.weight",
      "backbone.blocks.1.attn.proj.weight",
      "backbone.blocks.1.mlp.fc1.weight",
      "backbone.blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.08862938119652507,
    "lr": 1.7725876239305016e-06,
    "weight_decay": 0.05
  },
  "layer_3_no_decay": {
    "param_names": [
      "backbone.blocks.2.gamma_1",
      "backbone.blocks.2.gamma_2",
      "backbone.blocks.2.norm1.weight",
      "backbone.blocks.2.norm1.bias",
      "backbone.blocks.2.attn.q_bias",
      "backbone.blocks.2.attn.v_bias",
      "backbone.blocks.2.attn.proj.bias",
      "backbone.blocks.2.norm2.weight",
      "backbone.blocks.2.norm2.bias",
      "backbone.blocks.2.mlp.fc1.bias",
      "backbone.blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.09847709021836118,
    "lr": 1.969541804367224e-06,
    "weight_decay": 0.0
  },
  "layer_3_decay": {
    "param_names": [
      "backbone.blocks.2.attn.relative_position_bias_table",
      "backbone.blocks.2.attn.qkv.weight",
      "backbone.blocks.2.attn.proj.weight",
      "backbone.blocks.2.mlp.fc1.weight",
      "backbone.blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.09847709021836118,
    "lr": 1.969541804367224e-06,
    "weight_decay": 0.05
  },
  "layer_4_no_decay": {
    "param_names": [
      "backbone.blocks.3.gamma_1",
      "backbone.blocks.3.gamma_2",
      "backbone.blocks.3.norm1.weight",
      "backbone.blocks.3.norm1.bias",
      "backbone.blocks.3.attn.q_bias",
      "backbone.blocks.3.attn.v_bias",
      "backbone.blocks.3.attn.proj.bias",
      "backbone.blocks.3.norm2.weight",
      "backbone.blocks.3.norm2.bias",
      "backbone.blocks.3.mlp.fc1.bias",
      "backbone.blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.10941898913151242,
    "lr": 2.1883797826302486e-06,
    "weight_decay": 0.0
  },
  "layer_4_decay": {
    "param_names": [
      "backbone.blocks.3.attn.relative_position_bias_table",
      "backbone.blocks.3.attn.qkv.weight",
      "backbone.blocks.3.attn.proj.weight",
      "backbone.blocks.3.mlp.fc1.weight",
      "backbone.blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.10941898913151242,
    "lr": 2.1883797826302486e-06,
    "weight_decay": 0.05
  },
  "layer_5_no_decay": {
    "param_names": [
      "backbone.blocks.4.gamma_1",
      "backbone.blocks.4.gamma_2",
      "backbone.blocks.4.norm1.weight",
      "backbone.blocks.4.norm1.bias",
      "backbone.blocks.4.attn.q_bias",
      "backbone.blocks.4.attn.v_bias",
      "backbone.blocks.4.attn.proj.bias",
      "backbone.blocks.4.norm2.weight",
      "backbone.blocks.4.norm2.bias",
      "backbone.blocks.4.mlp.fc1.bias",
      "backbone.blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.12157665459056935,
    "lr": 2.431533091811387e-06,
    "weight_decay": 0.0
  },
  "layer_5_decay": {
    "param_names": [
      "backbone.blocks.4.attn.relative_position_bias_table",
      "backbone.blocks.4.attn.qkv.weight",
      "backbone.blocks.4.attn.proj.weight",
      "backbone.blocks.4.mlp.fc1.weight",
      "backbone.blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.12157665459056935,
    "lr": 2.431533091811387e-06,
    "weight_decay": 0.05
  },
  "layer_6_no_decay": {
    "param_names": [
      "backbone.blocks.5.gamma_1",
      "backbone.blocks.5.gamma_2",
      "backbone.blocks.5.norm1.weight",
      "backbone.blocks.5.norm1.bias",
      "backbone.blocks.5.attn.q_bias",
      "backbone.blocks.5.attn.v_bias",
      "backbone.blocks.5.attn.proj.bias",
      "backbone.blocks.5.norm2.weight",
      "backbone.blocks.5.norm2.bias",
      "backbone.blocks.5.mlp.fc1.bias",
      "backbone.blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.13508517176729928,
    "lr": 2.701703435345986e-06,
    "weight_decay": 0.0
  },
  "layer_6_decay": {
    "param_names": [
      "backbone.blocks.5.attn.relative_position_bias_table",
      "backbone.blocks.5.attn.qkv.weight",
      "backbone.blocks.5.attn.proj.weight",
      "backbone.blocks.5.mlp.fc1.weight",
      "backbone.blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.13508517176729928,
    "lr": 2.701703435345986e-06,
    "weight_decay": 0.05
  },
  "layer_7_no_decay": {
    "param_names": [
      "backbone.blocks.6.gamma_1",
      "backbone.blocks.6.gamma_2",
      "backbone.blocks.6.norm1.weight",
      "backbone.blocks.6.norm1.bias",
      "backbone.blocks.6.attn.q_bias",
      "backbone.blocks.6.attn.v_bias",
      "backbone.blocks.6.attn.proj.bias",
      "backbone.blocks.6.norm2.weight",
      "backbone.blocks.6.norm2.bias",
      "backbone.blocks.6.mlp.fc1.bias",
      "backbone.blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.15009463529699918,
    "lr": 3.001892705939984e-06,
    "weight_decay": 0.0
  },
  "layer_7_decay": {
    "param_names": [
      "backbone.blocks.6.attn.relative_position_bias_table",
      "backbone.blocks.6.attn.qkv.weight",
      "backbone.blocks.6.attn.proj.weight",
      "backbone.blocks.6.mlp.fc1.weight",
      "backbone.blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.15009463529699918,
    "lr": 3.001892705939984e-06,
    "weight_decay": 0.05
  },
  "layer_8_no_decay": {
    "param_names": [
      "backbone.blocks.7.gamma_1",
      "backbone.blocks.7.gamma_2",
      "backbone.blocks.7.norm1.weight",
      "backbone.blocks.7.norm1.bias",
      "backbone.blocks.7.attn.q_bias",
      "backbone.blocks.7.attn.v_bias",
      "backbone.blocks.7.attn.proj.bias",
      "backbone.blocks.7.norm2.weight",
      "backbone.blocks.7.norm2.bias",
      "backbone.blocks.7.mlp.fc1.bias",
      "backbone.blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.16677181699666577,
    "lr": 3.3354363399333156e-06,
    "weight_decay": 0.0
  },
  "layer_8_decay": {
    "param_names": [
      "backbone.blocks.7.attn.relative_position_bias_table",
      "backbone.blocks.7.attn.qkv.weight",
      "backbone.blocks.7.attn.proj.weight",
      "backbone.blocks.7.mlp.fc1.weight",
      "backbone.blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.16677181699666577,
    "lr": 3.3354363399333156e-06,
    "weight_decay": 0.05
  },
  "layer_9_no_decay": {
    "param_names": [
      "backbone.blocks.8.gamma_1",
      "backbone.blocks.8.gamma_2",
      "backbone.blocks.8.norm1.weight",
      "backbone.blocks.8.norm1.bias",
      "backbone.blocks.8.attn.q_bias",
      "backbone.blocks.8.attn.v_bias",
      "backbone.blocks.8.attn.proj.bias",
      "backbone.blocks.8.norm2.weight",
      "backbone.blocks.8.norm2.bias",
      "backbone.blocks.8.mlp.fc1.bias",
      "backbone.blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.18530201888518416,
    "lr": 3.7060403777036836e-06,
    "weight_decay": 0.0
  },
  "layer_9_decay": {
    "param_names": [
      "backbone.blocks.8.attn.relative_position_bias_table",
      "backbone.blocks.8.attn.qkv.weight",
      "backbone.blocks.8.attn.proj.weight",
      "backbone.blocks.8.mlp.fc1.weight",
      "backbone.blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.18530201888518416,
    "lr": 3.7060403777036836e-06,
    "weight_decay": 0.05
  },
  "layer_10_no_decay": {
    "param_names": [
      "backbone.blocks.9.gamma_1",
      "backbone.blocks.9.gamma_2",
      "backbone.blocks.9.norm1.weight",
      "backbone.blocks.9.norm1.bias",
      "backbone.blocks.9.attn.q_bias",
      "backbone.blocks.9.attn.v_bias",
      "backbone.blocks.9.attn.proj.bias",
      "backbone.blocks.9.norm2.weight",
      "backbone.blocks.9.norm2.bias",
      "backbone.blocks.9.mlp.fc1.bias",
      "backbone.blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.20589113209464907,
    "lr": 4.117822641892982e-06,
    "weight_decay": 0.0
  },
  "layer_10_decay": {
    "param_names": [
      "backbone.blocks.9.attn.relative_position_bias_table",
      "backbone.blocks.9.attn.qkv.weight",
      "backbone.blocks.9.attn.proj.weight",
      "backbone.blocks.9.mlp.fc1.weight",
      "backbone.blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.20589113209464907,
    "lr": 4.117822641892982e-06,
    "weight_decay": 0.05
  },
  "layer_11_no_decay": {
    "param_names": [
      "backbone.blocks.10.gamma_1",
      "backbone.blocks.10.gamma_2",
      "backbone.blocks.10.norm1.weight",
      "backbone.blocks.10.norm1.bias",
      "backbone.blocks.10.attn.q_bias",
      "backbone.blocks.10.attn.v_bias",
      "backbone.blocks.10.attn.proj.bias",
      "backbone.blocks.10.norm2.weight",
      "backbone.blocks.10.norm2.bias",
      "backbone.blocks.10.mlp.fc1.bias",
      "backbone.blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.2287679245496101,
    "lr": 4.575358490992202e-06,
    "weight_decay": 0.0
  },
  "layer_11_decay": {
    "param_names": [
      "backbone.blocks.10.attn.relative_position_bias_table",
      "backbone.blocks.10.attn.qkv.weight",
      "backbone.blocks.10.attn.proj.weight",
      "backbone.blocks.10.mlp.fc1.weight",
      "backbone.blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.2287679245496101,
    "lr": 4.575358490992202e-06,
    "weight_decay": 0.05
  },
  "layer_12_no_decay": {
    "param_names": [
      "backbone.blocks.11.gamma_1",
      "backbone.blocks.11.gamma_2",
      "backbone.blocks.11.norm1.weight",
      "backbone.blocks.11.norm1.bias",
      "backbone.blocks.11.attn.q_bias",
      "backbone.blocks.11.attn.v_bias",
      "backbone.blocks.11.attn.proj.bias",
      "backbone.blocks.11.norm2.weight",
      "backbone.blocks.11.norm2.bias",
      "backbone.blocks.11.mlp.fc1.bias",
      "backbone.blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.2541865828329001,
    "lr": 5.083731656658002e-06,
    "weight_decay": 0.0
  },
  "layer_12_decay": {
    "param_names": [
      "backbone.blocks.11.attn.relative_position_bias_table",
      "backbone.blocks.11.attn.qkv.weight",
      "backbone.blocks.11.attn.proj.weight",
      "backbone.blocks.11.mlp.fc1.weight",
      "backbone.blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.2541865828329001,
    "lr": 5.083731656658002e-06,
    "weight_decay": 0.05
  },
  "layer_13_no_decay": {
    "param_names": [
      "backbone.blocks.12.gamma_1",
      "backbone.blocks.12.gamma_2",
      "backbone.blocks.12.norm1.weight",
      "backbone.blocks.12.norm1.bias",
      "backbone.blocks.12.attn.q_bias",
      "backbone.blocks.12.attn.v_bias",
      "backbone.blocks.12.attn.proj.bias",
      "backbone.blocks.12.norm2.weight",
      "backbone.blocks.12.norm2.bias",
      "backbone.blocks.12.mlp.fc1.bias",
      "backbone.blocks.12.mlp.fc2.bias"
    ],
    "lr_scale": 0.2824295364810001,
    "lr": 5.648590729620003e-06,
    "weight_decay": 0.0
  },
  "layer_13_decay": {
    "param_names": [
      "backbone.blocks.12.attn.relative_position_bias_table",
      "backbone.blocks.12.attn.qkv.weight",
      "backbone.blocks.12.attn.proj.weight",
      "backbone.blocks.12.mlp.fc1.weight",
      "backbone.blocks.12.mlp.fc2.weight"
    ],
    "lr_scale": 0.2824295364810001,
    "lr": 5.648590729620003e-06,
    "weight_decay": 0.05
  },
  "layer_14_no_decay": {
    "param_names": [
      "backbone.blocks.13.gamma_1",
      "backbone.blocks.13.gamma_2",
      "backbone.blocks.13.norm1.weight",
      "backbone.blocks.13.norm1.bias",
      "backbone.blocks.13.attn.q_bias",
      "backbone.blocks.13.attn.v_bias",
      "backbone.blocks.13.attn.proj.bias",
      "backbone.blocks.13.norm2.weight",
      "backbone.blocks.13.norm2.bias",
      "backbone.blocks.13.mlp.fc1.bias",
      "backbone.blocks.13.mlp.fc2.bias"
    ],
    "lr_scale": 0.31381059609000006,
    "lr": 6.276211921800002e-06,
    "weight_decay": 0.0
  },
  "layer_14_decay": {
    "param_names": [
      "backbone.blocks.13.attn.relative_position_bias_table",
      "backbone.blocks.13.attn.qkv.weight",
      "backbone.blocks.13.attn.proj.weight",
      "backbone.blocks.13.mlp.fc1.weight",
      "backbone.blocks.13.mlp.fc2.weight"
    ],
    "lr_scale": 0.31381059609000006,
    "lr": 6.276211921800002e-06,
    "weight_decay": 0.05
  },
  "layer_15_no_decay": {
    "param_names": [
      "backbone.blocks.14.gamma_1",
      "backbone.blocks.14.gamma_2",
      "backbone.blocks.14.norm1.weight",
      "backbone.blocks.14.norm1.bias",
      "backbone.blocks.14.attn.q_bias",
      "backbone.blocks.14.attn.v_bias",
      "backbone.blocks.14.attn.proj.bias",
      "backbone.blocks.14.norm2.weight",
      "backbone.blocks.14.norm2.bias",
      "backbone.blocks.14.mlp.fc1.bias",
      "backbone.blocks.14.mlp.fc2.bias"
    ],
    "lr_scale": 0.3486784401000001,
    "lr": 6.973568802000002e-06,
    "weight_decay": 0.0
  },
  "layer_15_decay": {
    "param_names": [
      "backbone.blocks.14.attn.relative_position_bias_table",
      "backbone.blocks.14.attn.qkv.weight",
      "backbone.blocks.14.attn.proj.weight",
      "backbone.blocks.14.mlp.fc1.weight",
      "backbone.blocks.14.mlp.fc2.weight"
    ],
    "lr_scale": 0.3486784401000001,
    "lr": 6.973568802000002e-06,
    "weight_decay": 0.05
  },
  "layer_16_no_decay": {
    "param_names": [
      "backbone.blocks.15.gamma_1",
      "backbone.blocks.15.gamma_2",
      "backbone.blocks.15.norm1.weight",
      "backbone.blocks.15.norm1.bias",
      "backbone.blocks.15.attn.q_bias",
      "backbone.blocks.15.attn.v_bias",
      "backbone.blocks.15.attn.proj.bias",
      "backbone.blocks.15.norm2.weight",
      "backbone.blocks.15.norm2.bias",
      "backbone.blocks.15.mlp.fc1.bias",
      "backbone.blocks.15.mlp.fc2.bias"
    ],
    "lr_scale": 0.3874204890000001,
    "lr": 7.748409780000003e-06,
    "weight_decay": 0.0
  },
  "layer_16_decay": {
    "param_names": [
      "backbone.blocks.15.attn.relative_position_bias_table",
      "backbone.blocks.15.attn.qkv.weight",
      "backbone.blocks.15.attn.proj.weight",
      "backbone.blocks.15.mlp.fc1.weight",
      "backbone.blocks.15.mlp.fc2.weight"
    ],
    "lr_scale": 0.3874204890000001,
    "lr": 7.748409780000003e-06,
    "weight_decay": 0.05
  },
  "layer_17_no_decay": {
    "param_names": [
      "backbone.blocks.16.gamma_1",
      "backbone.blocks.16.gamma_2",
      "backbone.blocks.16.norm1.weight",
      "backbone.blocks.16.norm1.bias",
      "backbone.blocks.16.attn.q_bias",
      "backbone.blocks.16.attn.v_bias",
      "backbone.blocks.16.attn.proj.bias",
      "backbone.blocks.16.norm2.weight",
      "backbone.blocks.16.norm2.bias",
      "backbone.blocks.16.mlp.fc1.bias",
      "backbone.blocks.16.mlp.fc2.bias"
    ],
    "lr_scale": 0.4304672100000001,
    "lr": 8.609344200000003e-06,
    "weight_decay": 0.0
  },
  "layer_17_decay": {
    "param_names": [
      "backbone.blocks.16.attn.relative_position_bias_table",
      "backbone.blocks.16.attn.qkv.weight",
      "backbone.blocks.16.attn.proj.weight",
      "backbone.blocks.16.mlp.fc1.weight",
      "backbone.blocks.16.mlp.fc2.weight"
    ],
    "lr_scale": 0.4304672100000001,
    "lr": 8.609344200000003e-06,
    "weight_decay": 0.05
  },
  "layer_18_no_decay": {
    "param_names": [
      "backbone.blocks.17.gamma_1",
      "backbone.blocks.17.gamma_2",
      "backbone.blocks.17.norm1.weight",
      "backbone.blocks.17.norm1.bias",
      "backbone.blocks.17.attn.q_bias",
      "backbone.blocks.17.attn.v_bias",
      "backbone.blocks.17.attn.proj.bias",
      "backbone.blocks.17.norm2.weight",
      "backbone.blocks.17.norm2.bias",
      "backbone.blocks.17.mlp.fc1.bias",
      "backbone.blocks.17.mlp.fc2.bias"
    ],
    "lr_scale": 0.4782969000000001,
    "lr": 9.565938000000002e-06,
    "weight_decay": 0.0
  },
  "layer_18_decay": {
    "param_names": [
      "backbone.blocks.17.attn.relative_position_bias_table",
      "backbone.blocks.17.attn.qkv.weight",
      "backbone.blocks.17.attn.proj.weight",
      "backbone.blocks.17.mlp.fc1.weight",
      "backbone.blocks.17.mlp.fc2.weight"
    ],
    "lr_scale": 0.4782969000000001,
    "lr": 9.565938000000002e-06,
    "weight_decay": 0.05
  },
  "layer_19_no_decay": {
    "param_names": [
      "backbone.blocks.18.gamma_1",
      "backbone.blocks.18.gamma_2",
      "backbone.blocks.18.norm1.weight",
      "backbone.blocks.18.norm1.bias",
      "backbone.blocks.18.attn.q_bias",
      "backbone.blocks.18.attn.v_bias",
      "backbone.blocks.18.attn.proj.bias",
      "backbone.blocks.18.norm2.weight",
      "backbone.blocks.18.norm2.bias",
      "backbone.blocks.18.mlp.fc1.bias",
      "backbone.blocks.18.mlp.fc2.bias"
    ],
    "lr_scale": 0.531441,
    "lr": 1.0628820000000002e-05,
    "weight_decay": 0.0
  },
  "layer_19_decay": {
    "param_names": [
      "backbone.blocks.18.attn.relative_position_bias_table",
      "backbone.blocks.18.attn.qkv.weight",
      "backbone.blocks.18.attn.proj.weight",
      "backbone.blocks.18.mlp.fc1.weight",
      "backbone.blocks.18.mlp.fc2.weight"
    ],
    "lr_scale": 0.531441,
    "lr": 1.0628820000000002e-05,
    "weight_decay": 0.05
  },
  "layer_20_no_decay": {
    "param_names": [
      "backbone.blocks.19.gamma_1",
      "backbone.blocks.19.gamma_2",
      "backbone.blocks.19.norm1.weight",
      "backbone.blocks.19.norm1.bias",
      "backbone.blocks.19.attn.q_bias",
      "backbone.blocks.19.attn.v_bias",
      "backbone.blocks.19.attn.proj.bias",
      "backbone.blocks.19.norm2.weight",
      "backbone.blocks.19.norm2.bias",
      "backbone.blocks.19.mlp.fc1.bias",
      "backbone.blocks.19.mlp.fc2.bias"
    ],
    "lr_scale": 0.5904900000000001,
    "lr": 1.1809800000000002e-05,
    "weight_decay": 0.0
  },
  "layer_20_decay": {
    "param_names": [
      "backbone.blocks.19.attn.relative_position_bias_table",
      "backbone.blocks.19.attn.qkv.weight",
      "backbone.blocks.19.attn.proj.weight",
      "backbone.blocks.19.mlp.fc1.weight",
      "backbone.blocks.19.mlp.fc2.weight"
    ],
    "lr_scale": 0.5904900000000001,
    "lr": 1.1809800000000002e-05,
    "weight_decay": 0.05
  },
  "layer_21_no_decay": {
    "param_names": [
      "backbone.blocks.20.gamma_1",
      "backbone.blocks.20.gamma_2",
      "backbone.blocks.20.norm1.weight",
      "backbone.blocks.20.norm1.bias",
      "backbone.blocks.20.attn.q_bias",
      "backbone.blocks.20.attn.v_bias",
      "backbone.blocks.20.attn.proj.bias",
      "backbone.blocks.20.norm2.weight",
      "backbone.blocks.20.norm2.bias",
      "backbone.blocks.20.mlp.fc1.bias",
      "backbone.blocks.20.mlp.fc2.bias"
    ],
    "lr_scale": 0.6561,
    "lr": 1.3122e-05,
    "weight_decay": 0.0
  },
  "layer_21_decay": {
    "param_names": [
      "backbone.blocks.20.attn.relative_position_bias_table",
      "backbone.blocks.20.attn.qkv.weight",
      "backbone.blocks.20.attn.proj.weight",
      "backbone.blocks.20.mlp.fc1.weight",
      "backbone.blocks.20.mlp.fc2.weight"
    ],
    "lr_scale": 0.6561,
    "lr": 1.3122e-05,
    "weight_decay": 0.05
  },
  "layer_22_no_decay": {
    "param_names": [
      "backbone.blocks.21.gamma_1",
      "backbone.blocks.21.gamma_2",
      "backbone.blocks.21.norm1.weight",
      "backbone.blocks.21.norm1.bias",
      "backbone.blocks.21.attn.q_bias",
      "backbone.blocks.21.attn.v_bias",
      "backbone.blocks.21.attn.proj.bias",
      "backbone.blocks.21.norm2.weight",
      "backbone.blocks.21.norm2.bias",
      "backbone.blocks.21.mlp.fc1.bias",
      "backbone.blocks.21.mlp.fc2.bias"
    ],
    "lr_scale": 0.7290000000000001,
    "lr": 1.4580000000000003e-05,
    "weight_decay": 0.0
  },
  "layer_22_decay": {
    "param_names": [
      "backbone.blocks.21.attn.relative_position_bias_table",
      "backbone.blocks.21.attn.qkv.weight",
      "backbone.blocks.21.attn.proj.weight",
      "backbone.blocks.21.mlp.fc1.weight",
      "backbone.blocks.21.mlp.fc2.weight"
    ],
    "lr_scale": 0.7290000000000001,
    "lr": 1.4580000000000003e-05,
    "weight_decay": 0.05
  },
  "layer_23_no_decay": {
    "param_names": [
      "backbone.blocks.22.gamma_1",
      "backbone.blocks.22.gamma_2",
      "backbone.blocks.22.norm1.weight",
      "backbone.blocks.22.norm1.bias",
      "backbone.blocks.22.attn.q_bias",
      "backbone.blocks.22.attn.v_bias",
      "backbone.blocks.22.attn.proj.bias",
      "backbone.blocks.22.norm2.weight",
      "backbone.blocks.22.norm2.bias",
      "backbone.blocks.22.mlp.fc1.bias",
      "backbone.blocks.22.mlp.fc2.bias"
    ],
    "lr_scale": 0.81,
    "lr": 1.62e-05,
    "weight_decay": 0.0
  },
  "layer_23_decay": {
    "param_names": [
      "backbone.blocks.22.attn.relative_position_bias_table",
      "backbone.blocks.22.attn.qkv.weight",
      "backbone.blocks.22.attn.proj.weight",
      "backbone.blocks.22.mlp.fc1.weight",
      "backbone.blocks.22.mlp.fc2.weight"
    ],
    "lr_scale": 0.81,
    "lr": 1.62e-05,
    "weight_decay": 0.05
  },
  "layer_24_no_decay": {
    "param_names": [
      "backbone.blocks.23.gamma_1",
      "backbone.blocks.23.gamma_2",
      "backbone.blocks.23.norm1.weight",
      "backbone.blocks.23.norm1.bias",
      "backbone.blocks.23.attn.q_bias",
      "backbone.blocks.23.attn.v_bias",
      "backbone.blocks.23.attn.proj.bias",
      "backbone.blocks.23.norm2.weight",
      "backbone.blocks.23.norm2.bias",
      "backbone.blocks.23.mlp.fc1.bias",
      "backbone.blocks.23.mlp.fc2.bias"
    ],
    "lr_scale": 0.9,
    "lr": 1.8e-05,
    "weight_decay": 0.0
  },
  "layer_24_decay": {
    "param_names": [
      "backbone.blocks.23.attn.relative_position_bias_table",
      "backbone.blocks.23.attn.qkv.weight",
      "backbone.blocks.23.attn.proj.weight",
      "backbone.blocks.23.mlp.fc1.weight",
      "backbone.blocks.23.mlp.fc2.weight"
    ],
    "lr_scale": 0.9,
    "lr": 1.8e-05,
    "weight_decay": 0.05
  },
  "layer_25_no_decay": {
    "param_names": [
      "backbone.spm.stem.1.weight",
      "backbone.spm.stem.1.bias",
      "backbone.spm.stem.4.weight",
      "backbone.spm.stem.4.bias",
      "backbone.spm.stem.7.weight",
      "backbone.spm.stem.7.bias",
      "backbone.spm.conv2.1.weight",
      "backbone.spm.conv2.1.bias",
      "backbone.spm.conv3.1.weight",
      "backbone.spm.conv3.1.bias",
      "backbone.spm.conv4.1.weight",
      "backbone.spm.conv4.1.bias",
      "backbone.spm.fc1.bias",
      "backbone.spm.fc2.bias",
      "backbone.spm.fc3.bias",
      "backbone.spm.fc4.bias",
      "backbone.interactions.0.injector.gamma",
      "backbone.interactions.0.injector.query_norm.weight",
      "backbone.interactions.0.injector.query_norm.bias",
      "backbone.interactions.0.injector.feat_norm.weight",
      "backbone.interactions.0.injector.feat_norm.bias",
      "backbone.interactions.0.injector.attn.sampling_offsets.bias",
      "backbone.interactions.0.injector.attn.attention_weights.bias",
      "backbone.interactions.0.injector.attn.value_proj.bias",
      "backbone.interactions.0.injector.attn.output_proj.bias",
      "backbone.interactions.0.extractor.query_norm.weight",
      "backbone.interactions.0.extractor.query_norm.bias",
      "backbone.interactions.0.extractor.feat_norm.weight",
      "backbone.interactions.0.extractor.feat_norm.bias",
      "backbone.interactions.0.extractor.attn.sampling_offsets.bias",
      "backbone.interactions.0.extractor.attn.attention_weights.bias",
      "backbone.interactions.0.extractor.attn.value_proj.bias",
      "backbone.interactions.0.extractor.attn.output_proj.bias",
      "backbone.interactions.0.extractor.ffn.fc1.bias",
      "backbone.interactions.0.extractor.ffn.dwconv.dwconv.bias",
      "backbone.interactions.0.extractor.ffn.fc2.bias",
      "backbone.interactions.0.extractor.ffn_norm.weight",
      "backbone.interactions.0.extractor.ffn_norm.bias",
      "backbone.interactions.1.injector.gamma",
      "backbone.interactions.1.injector.query_norm.weight",
      "backbone.interactions.1.injector.query_norm.bias",
      "backbone.interactions.1.injector.feat_norm.weight",
      "backbone.interactions.1.injector.feat_norm.bias",
      "backbone.interactions.1.injector.attn.sampling_offsets.bias",
      "backbone.interactions.1.injector.attn.attention_weights.bias",
      "backbone.interactions.1.injector.attn.value_proj.bias",
      "backbone.interactions.1.injector.attn.output_proj.bias",
      "backbone.interactions.1.extractor.query_norm.weight",
      "backbone.interactions.1.extractor.query_norm.bias",
      "backbone.interactions.1.extractor.feat_norm.weight",
      "backbone.interactions.1.extractor.feat_norm.bias",
      "backbone.interactions.1.extractor.attn.sampling_offsets.bias",
      "backbone.interactions.1.extractor.attn.attention_weights.bias",
      "backbone.interactions.1.extractor.attn.value_proj.bias",
      "backbone.interactions.1.extractor.attn.output_proj.bias",
      "backbone.interactions.1.extractor.ffn.fc1.bias",
      "backbone.interactions.1.extractor.ffn.dwconv.dwconv.bias",
      "backbone.interactions.1.extractor.ffn.fc2.bias",
      "backbone.interactions.1.extractor.ffn_norm.weight",
      "backbone.interactions.1.extractor.ffn_norm.bias",
      "backbone.interactions.2.injector.gamma",
      "backbone.interactions.2.injector.query_norm.weight",
      "backbone.interactions.2.injector.query_norm.bias",
      "backbone.interactions.2.injector.feat_norm.weight",
      "backbone.interactions.2.injector.feat_norm.bias",
      "backbone.interactions.2.injector.attn.sampling_offsets.bias",
      "backbone.interactions.2.injector.attn.attention_weights.bias",
      "backbone.interactions.2.injector.attn.value_proj.bias",
      "backbone.interactions.2.injector.attn.output_proj.bias",
      "backbone.interactions.2.extractor.query_norm.weight",
      "backbone.interactions.2.extractor.query_norm.bias",
      "backbone.interactions.2.extractor.feat_norm.weight",
      "backbone.interactions.2.extractor.feat_norm.bias",
      "backbone.interactions.2.extractor.attn.sampling_offsets.bias",
      "backbone.interactions.2.extractor.attn.attention_weights.bias",
      "backbone.interactions.2.extractor.attn.value_proj.bias",
      "backbone.interactions.2.extractor.attn.output_proj.bias",
      "backbone.interactions.2.extractor.ffn.fc1.bias",
      "backbone.interactions.2.extractor.ffn.dwconv.dwconv.bias",
      "backbone.interactions.2.extractor.ffn.fc2.bias",
      "backbone.interactions.2.extractor.ffn_norm.weight",
      "backbone.interactions.2.extractor.ffn_norm.bias",
      "backbone.interactions.3.injector.gamma",
      "backbone.interactions.3.injector.query_norm.weight",
      "backbone.interactions.3.injector.query_norm.bias",
      "backbone.interactions.3.injector.feat_norm.weight",
      "backbone.interactions.3.injector.feat_norm.bias",
      "backbone.interactions.3.injector.attn.sampling_offsets.bias",
      "backbone.interactions.3.injector.attn.attention_weights.bias",
      "backbone.interactions.3.injector.attn.value_proj.bias",
      "backbone.interactions.3.injector.attn.output_proj.bias",
      "backbone.interactions.3.extractor.query_norm.weight",
      "backbone.interactions.3.extractor.query_norm.bias",
      "backbone.interactions.3.extractor.feat_norm.weight",
      "backbone.interactions.3.extractor.feat_norm.bias",
      "backbone.interactions.3.extractor.attn.sampling_offsets.bias",
      "backbone.interactions.3.extractor.attn.attention_weights.bias",
      "backbone.interactions.3.extractor.attn.value_proj.bias",
      "backbone.interactions.3.extractor.attn.output_proj.bias",
      "backbone.interactions.3.extractor.ffn.fc1.bias",
      "backbone.interactions.3.extractor.ffn.dwconv.dwconv.bias",
      "backbone.interactions.3.extractor.ffn.fc2.bias",
      "backbone.interactions.3.extractor.ffn_norm.weight",
      "backbone.interactions.3.extractor.ffn_norm.bias",
      "backbone.interactions.3.extra_extractors.0.query_norm.weight",
      "backbone.interactions.3.extra_extractors.0.query_norm.bias",
      "backbone.interactions.3.extra_extractors.0.feat_norm.weight",
      "backbone.interactions.3.extra_extractors.0.feat_norm.bias",
      "backbone.interactions.3.extra_extractors.0.attn.sampling_offsets.bias",
      "backbone.interactions.3.extra_extractors.0.attn.attention_weights.bias",
      "backbone.interactions.3.extra_extractors.0.attn.value_proj.bias",
      "backbone.interactions.3.extra_extractors.0.attn.output_proj.bias",
      "backbone.interactions.3.extra_extractors.0.ffn.fc1.bias",
      "backbone.interactions.3.extra_extractors.0.ffn.dwconv.dwconv.bias",
      "backbone.interactions.3.extra_extractors.0.ffn.fc2.bias",
      "backbone.interactions.3.extra_extractors.0.ffn_norm.weight",
      "backbone.interactions.3.extra_extractors.0.ffn_norm.bias",
      "backbone.interactions.3.extra_extractors.1.query_norm.weight",
      "backbone.interactions.3.extra_extractors.1.query_norm.bias",
      "backbone.interactions.3.extra_extractors.1.feat_norm.weight",
      "backbone.interactions.3.extra_extractors.1.feat_norm.bias",
      "backbone.interactions.3.extra_extractors.1.attn.sampling_offsets.bias",
      "backbone.interactions.3.extra_extractors.1.attn.attention_weights.bias",
      "backbone.interactions.3.extra_extractors.1.attn.value_proj.bias",
      "backbone.interactions.3.extra_extractors.1.attn.output_proj.bias",
      "backbone.interactions.3.extra_extractors.1.ffn.fc1.bias",
      "backbone.interactions.3.extra_extractors.1.ffn.dwconv.dwconv.bias",
      "backbone.interactions.3.extra_extractors.1.ffn.fc2.bias",
      "backbone.interactions.3.extra_extractors.1.ffn_norm.weight",
      "backbone.interactions.3.extra_extractors.1.ffn_norm.bias",
      "backbone.up.bias",
      "backbone.norm1.weight",
      "backbone.norm1.bias",
      "backbone.norm2.weight",
      "backbone.norm2.bias",
      "backbone.norm3.weight",
      "backbone.norm3.bias",
      "backbone.norm4.weight",
      "backbone.norm4.bias",
      "decode_head.pixel_decoder.input_convs.0.conv.bias",
      "decode_head.pixel_decoder.input_convs.0.gn.weight",
      "decode_head.pixel_decoder.input_convs.0.gn.bias",
      "decode_head.pixel_decoder.input_convs.1.conv.bias",
      "decode_head.pixel_decoder.input_convs.1.gn.weight",
      "decode_head.pixel_decoder.input_convs.1.gn.bias",
      "decode_head.pixel_decoder.input_convs.2.conv.bias",
      "decode_head.pixel_decoder.input_convs.2.gn.weight",
      "decode_head.pixel_decoder.input_convs.2.gn.bias",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.0.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.0.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.0.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.0.norms.1.bias",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.1.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.1.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.1.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.1.norms.1.bias",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.2.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.2.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.2.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.2.norms.1.bias",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.3.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.3.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.3.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.3.norms.1.bias",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.4.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.4.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.4.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.4.norms.1.bias",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.5.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.5.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.5.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.5.norms.1.bias",
      "decode_head.pixel_decoder.lateral_convs.0.gn.weight",
      "decode_head.pixel_decoder.lateral_convs.0.gn.bias",
      "decode_head.pixel_decoder.output_convs.0.gn.weight",
      "decode_head.pixel_decoder.output_convs.0.gn.bias",
      "decode_head.pixel_decoder.mask_feature.bias",
      "decode_head.transformer_decoder.layers.0.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.0.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.0.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.0.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.0.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.0.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.0.norms.0.weight",
      "decode_head.transformer_decoder.layers.0.norms.0.bias",
      "decode_head.transformer_decoder.layers.0.norms.1.weight",
      "decode_head.transformer_decoder.layers.0.norms.1.bias",
      "decode_head.transformer_decoder.layers.0.norms.2.weight",
      "decode_head.transformer_decoder.layers.0.norms.2.bias",
      "decode_head.transformer_decoder.layers.1.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.1.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.1.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.1.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.1.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.1.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.1.norms.0.weight",
      "decode_head.transformer_decoder.layers.1.norms.0.bias",
      "decode_head.transformer_decoder.layers.1.norms.1.weight",
      "decode_head.transformer_decoder.layers.1.norms.1.bias",
      "decode_head.transformer_decoder.layers.1.norms.2.weight",
      "decode_head.transformer_decoder.layers.1.norms.2.bias",
      "decode_head.transformer_decoder.layers.2.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.2.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.2.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.2.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.2.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.2.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.2.norms.0.weight",
      "decode_head.transformer_decoder.layers.2.norms.0.bias",
      "decode_head.transformer_decoder.layers.2.norms.1.weight",
      "decode_head.transformer_decoder.layers.2.norms.1.bias",
      "decode_head.transformer_decoder.layers.2.norms.2.weight",
      "decode_head.transformer_decoder.layers.2.norms.2.bias",
      "decode_head.transformer_decoder.layers.3.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.3.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.3.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.3.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.3.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.3.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.3.norms.0.weight",
      "decode_head.transformer_decoder.layers.3.norms.0.bias",
      "decode_head.transformer_decoder.layers.3.norms.1.weight",
      "decode_head.transformer_decoder.layers.3.norms.1.bias",
      "decode_head.transformer_decoder.layers.3.norms.2.weight",
      "decode_head.transformer_decoder.layers.3.norms.2.bias",
      "decode_head.transformer_decoder.layers.4.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.4.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.4.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.4.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.4.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.4.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.4.norms.0.weight",
      "decode_head.transformer_decoder.layers.4.norms.0.bias",
      "decode_head.transformer_decoder.layers.4.norms.1.weight",
      "decode_head.transformer_decoder.layers.4.norms.1.bias",
      "decode_head.transformer_decoder.layers.4.norms.2.weight",
      "decode_head.transformer_decoder.layers.4.norms.2.bias",
      "decode_head.transformer_decoder.layers.5.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.5.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.5.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.5.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.5.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.5.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.5.norms.0.weight",
      "decode_head.transformer_decoder.layers.5.norms.0.bias",
      "decode_head.transformer_decoder.layers.5.norms.1.weight",
      "decode_head.transformer_decoder.layers.5.norms.1.bias",
      "decode_head.transformer_decoder.layers.5.norms.2.weight",
      "decode_head.transformer_decoder.layers.5.norms.2.bias",
      "decode_head.transformer_decoder.layers.6.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.6.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.6.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.6.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.6.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.6.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.6.norms.0.weight",
      "decode_head.transformer_decoder.layers.6.norms.0.bias",
      "decode_head.transformer_decoder.layers.6.norms.1.weight",
      "decode_head.transformer_decoder.layers.6.norms.1.bias",
      "decode_head.transformer_decoder.layers.6.norms.2.weight",
      "decode_head.transformer_decoder.layers.6.norms.2.bias",
      "decode_head.transformer_decoder.layers.7.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.7.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.7.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.7.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.7.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.7.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.7.norms.0.weight",
      "decode_head.transformer_decoder.layers.7.norms.0.bias",
      "decode_head.transformer_decoder.layers.7.norms.1.weight",
      "decode_head.transformer_decoder.layers.7.norms.1.bias",
      "decode_head.transformer_decoder.layers.7.norms.2.weight",
      "decode_head.transformer_decoder.layers.7.norms.2.bias",
      "decode_head.transformer_decoder.layers.8.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.8.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.8.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.8.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.8.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.8.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.8.norms.0.weight",
      "decode_head.transformer_decoder.layers.8.norms.0.bias",
      "decode_head.transformer_decoder.layers.8.norms.1.weight",
      "decode_head.transformer_decoder.layers.8.norms.1.bias",
      "decode_head.transformer_decoder.layers.8.norms.2.weight",
      "decode_head.transformer_decoder.layers.8.norms.2.bias",
      "decode_head.transformer_decoder.post_norm.weight",
      "decode_head.transformer_decoder.post_norm.bias"
    ],
    "lr_scale": 1.0,
    "lr": 2e-05,
    "weight_decay": 0.0
  }
}2023-02-21 22:35:46,913 - mmseg - INFO - Loaded 357 images
2023-02-21 22:35:46,914 - mmseg - INFO - load checkpoint from local path: work_dirs/my_city/latest.pth
2023-02-21 22:36:11,899 - mmseg - INFO - Start running, host: root@autodl-container-cd46119efa-5e1dcbf2, work_dir: /root/autodl-tmp/ViT-Adapter/segmentation/work_dirs/my_city
2023-02-21 22:36:11,899 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-02-21 22:36:11,899 - mmseg - INFO - workflow: [('train', 1)], max: 80000 iters
2023-02-21 22:36:11,900 - mmseg - INFO - Checkpoints will be saved to /root/autodl-tmp/ViT-Adapter/segmentation/work_dirs/my_city by HardDiskBackend.

/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:3657: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/root/miniconda3/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
2023-02-21 22:38:35,300 - mmseg - INFO - Iter [50/80000]	lr: 4.688e-08, eta: 2 days, 15:38:31, time: 2.866, data_time: 0.037, memory: 31494, decode.loss_cls: 0.6342, decode.loss_mask: 0.8522, decode.loss_dice: 1.4489, decode.d0.loss_cls: 5.6842, decode.d0.loss_mask: 0.8590, decode.d0.loss_dice: 1.6023, decode.d1.loss_cls: 0.5979, decode.d1.loss_mask: 0.8522, decode.d1.loss_dice: 1.5335, decode.d2.loss_cls: 0.5938, decode.d2.loss_mask: 0.8472, decode.d2.loss_dice: 1.4878, decode.d3.loss_cls: 0.6242, decode.d3.loss_mask: 0.8492, decode.d3.loss_dice: 1.4328, decode.d4.loss_cls: 0.6595, decode.d4.loss_mask: 0.8428, decode.d4.loss_dice: 1.4229, decode.d5.loss_cls: 0.6208, decode.d5.loss_mask: 0.8603, decode.d5.loss_dice: 1.4495, decode.d6.loss_cls: 0.6279, decode.d6.loss_mask: 0.8395, decode.d6.loss_dice: 1.4424, decode.d7.loss_cls: 0.6270, decode.d7.loss_mask: 0.8472, decode.d7.loss_dice: 1.4496, decode.d8.loss_cls: 0.6110, decode.d8.loss_mask: 0.8682, decode.d8.loss_dice: 1.4590, loss: 34.5270
2023-02-21 22:40:52,638 - mmseg - INFO - Iter [100/80000]	lr: 9.465e-08, eta: 2 days, 14:16:56, time: 2.747, data_time: 0.021, memory: 31494, decode.loss_cls: 0.5260, decode.loss_mask: 0.8088, decode.loss_dice: 1.3170, decode.d0.loss_cls: 5.7236, decode.d0.loss_mask: 0.8671, decode.d0.loss_dice: 1.4620, decode.d1.loss_cls: 0.5796, decode.d1.loss_mask: 0.8396, decode.d1.loss_dice: 1.4046, decode.d2.loss_cls: 0.5626, decode.d2.loss_mask: 0.8306, decode.d2.loss_dice: 1.3503, decode.d3.loss_cls: 0.5861, decode.d3.loss_mask: 0.8367, decode.d3.loss_dice: 1.3061, decode.d4.loss_cls: 0.5663, decode.d4.loss_mask: 0.8268, decode.d4.loss_dice: 1.3087, decode.d5.loss_cls: 0.5300, decode.d5.loss_mask: 0.8345, decode.d5.loss_dice: 1.3068, decode.d6.loss_cls: 0.5386, decode.d6.loss_mask: 0.8127, decode.d6.loss_dice: 1.3164, decode.d7.loss_cls: 0.5042, decode.d7.loss_mask: 0.8160, decode.d7.loss_dice: 1.3312, decode.d8.loss_cls: 0.5394, decode.d8.loss_mask: 0.8076, decode.d8.loss_dice: 1.3186, loss: 32.3587
2023-02-21 22:43:10,429 - mmseg - INFO - Iter [150/80000]	lr: 1.424e-07, eta: 2 days, 13:52:14, time: 2.756, data_time: 0.022, memory: 31494, decode.loss_cls: 0.6113, decode.loss_mask: 0.7327, decode.loss_dice: 1.2735, decode.d0.loss_cls: 5.7278, decode.d0.loss_mask: 0.7763, decode.d0.loss_dice: 1.4581, decode.d1.loss_cls: 0.6730, decode.d1.loss_mask: 0.7481, decode.d1.loss_dice: 1.3731, decode.d2.loss_cls: 0.6146, decode.d2.loss_mask: 0.7320, decode.d2.loss_dice: 1.3314, decode.d3.loss_cls: 0.6368, decode.d3.loss_mask: 0.7404, decode.d3.loss_dice: 1.2934, decode.d4.loss_cls: 0.6685, decode.d4.loss_mask: 0.7261, decode.d4.loss_dice: 1.2793, decode.d5.loss_cls: 0.6125, decode.d5.loss_mask: 0.7356, decode.d5.loss_dice: 1.2930, decode.d6.loss_cls: 0.6320, decode.d6.loss_mask: 0.7249, decode.d6.loss_dice: 1.2924, decode.d7.loss_cls: 0.6093, decode.d7.loss_mask: 0.7258, decode.d7.loss_dice: 1.2999, decode.d8.loss_cls: 0.5918, decode.d8.loss_mask: 0.7344, decode.d8.loss_dice: 1.2988, loss: 31.9468
2023-02-21 22:45:28,015 - mmseg - INFO - Iter [200/80000]	lr: 1.900e-07, eta: 2 days, 13:37:22, time: 2.752, data_time: 0.022, memory: 31494, decode.loss_cls: 0.5423, decode.loss_mask: 0.8025, decode.loss_dice: 1.3701, decode.d0.loss_cls: 5.6748, decode.d0.loss_mask: 0.8489, decode.d0.loss_dice: 1.5204, decode.d1.loss_cls: 0.6162, decode.d1.loss_mask: 0.8288, decode.d1.loss_dice: 1.4622, decode.d2.loss_cls: 0.5692, decode.d2.loss_mask: 0.7959, decode.d2.loss_dice: 1.4041, decode.d3.loss_cls: 0.5790, decode.d3.loss_mask: 0.8024, decode.d3.loss_dice: 1.3757, decode.d4.loss_cls: 0.5443, decode.d4.loss_mask: 0.7994, decode.d4.loss_dice: 1.3732, decode.d5.loss_cls: 0.5553, decode.d5.loss_mask: 0.7995, decode.d5.loss_dice: 1.3902, decode.d6.loss_cls: 0.5389, decode.d6.loss_mask: 0.8011, decode.d6.loss_dice: 1.3705, decode.d7.loss_cls: 0.5551, decode.d7.loss_mask: 0.8065, decode.d7.loss_dice: 1.3807, decode.d8.loss_cls: 0.5302, decode.d8.loss_mask: 0.8000, decode.d8.loss_dice: 1.3912, loss: 32.8287
2023-02-21 22:47:45,534 - mmseg - INFO - Iter [250/80000]	lr: 2.376e-07, eta: 2 days, 13:27:10, time: 2.750, data_time: 0.023, memory: 31494, decode.loss_cls: 0.6242, decode.loss_mask: 0.7972, decode.loss_dice: 1.4043, decode.d0.loss_cls: 5.6768, decode.d0.loss_mask: 0.8871, decode.d0.loss_dice: 1.6053, decode.d1.loss_cls: 0.5890, decode.d1.loss_mask: 0.8239, decode.d1.loss_dice: 1.5035, decode.d2.loss_cls: 0.6353, decode.d2.loss_mask: 0.7958, decode.d2.loss_dice: 1.4365, decode.d3.loss_cls: 0.6194, decode.d3.loss_mask: 0.7974, decode.d3.loss_dice: 1.3991, decode.d4.loss_cls: 0.6141, decode.d4.loss_mask: 0.7990, decode.d4.loss_dice: 1.4065, decode.d5.loss_cls: 0.6224, decode.d5.loss_mask: 0.8013, decode.d5.loss_dice: 1.4071, decode.d6.loss_cls: 0.6086, decode.d6.loss_mask: 0.8035, decode.d6.loss_dice: 1.4106, decode.d7.loss_cls: 0.5880, decode.d7.loss_mask: 0.8028, decode.d7.loss_dice: 1.4094, decode.d8.loss_cls: 0.5785, decode.d8.loss_mask: 0.8034, decode.d8.loss_dice: 1.4042, loss: 33.6543
2023-02-21 22:50:02,965 - mmseg - INFO - Iter [300/80000]	lr: 2.851e-07, eta: 2 days, 13:19:13, time: 2.749, data_time: 0.022, memory: 31494, decode.loss_cls: 0.6115, decode.loss_mask: 0.8137, decode.loss_dice: 1.4527, decode.d0.loss_cls: 5.6507, decode.d0.loss_mask: 0.8363, decode.d0.loss_dice: 1.6113, decode.d1.loss_cls: 0.5890, decode.d1.loss_mask: 0.8401, decode.d1.loss_dice: 1.5604, decode.d2.loss_cls: 0.5624, decode.d2.loss_mask: 0.8207, decode.d2.loss_dice: 1.5084, decode.d3.loss_cls: 0.5950, decode.d3.loss_mask: 0.8250, decode.d3.loss_dice: 1.4671, decode.d4.loss_cls: 0.5831, decode.d4.loss_mask: 0.8201, decode.d4.loss_dice: 1.4680, decode.d5.loss_cls: 0.6023, decode.d5.loss_mask: 0.8008, decode.d5.loss_dice: 1.4574, decode.d6.loss_cls: 0.6149, decode.d6.loss_mask: 0.8026, decode.d6.loss_dice: 1.4437, decode.d7.loss_cls: 0.6041, decode.d7.loss_mask: 0.8089, decode.d7.loss_dice: 1.4397, decode.d8.loss_cls: 0.6151, decode.d8.loss_mask: 0.8112, decode.d8.loss_dice: 1.4451, loss: 34.0613
2023-02-21 22:52:20,657 - mmseg - INFO - Iter [350/80000]	lr: 3.326e-07, eta: 2 days, 13:13:53, time: 2.754, data_time: 0.022, memory: 31494, decode.loss_cls: 0.5238, decode.loss_mask: 0.7729, decode.loss_dice: 1.2599, decode.d0.loss_cls: 5.6713, decode.d0.loss_mask: 0.8179, decode.d0.loss_dice: 1.3628, decode.d1.loss_cls: 0.5409, decode.d1.loss_mask: 0.7871, decode.d1.loss_dice: 1.3105, decode.d2.loss_cls: 0.5251, decode.d2.loss_mask: 0.7811, decode.d2.loss_dice: 1.2633, decode.d3.loss_cls: 0.5287, decode.d3.loss_mask: 0.7725, decode.d3.loss_dice: 1.2374, decode.d4.loss_cls: 0.5338, decode.d4.loss_mask: 0.7752, decode.d4.loss_dice: 1.2394, decode.d5.loss_cls: 0.5342, decode.d5.loss_mask: 0.7646, decode.d5.loss_dice: 1.2476, decode.d6.loss_cls: 0.5293, decode.d6.loss_mask: 0.7660, decode.d6.loss_dice: 1.2470, decode.d7.loss_cls: 0.4928, decode.d7.loss_mask: 0.7678, decode.d7.loss_dice: 1.2487, decode.d8.loss_cls: 0.5037, decode.d8.loss_mask: 0.7553, decode.d8.loss_dice: 1.2394, loss: 30.8001
2023-02-21 22:54:37,983 - mmseg - INFO - Iter [400/80000]	lr: 3.800e-07, eta: 2 days, 13:08:05, time: 2.747, data_time: 0.021, memory: 31494, decode.loss_cls: 0.5483, decode.loss_mask: 0.7759, decode.loss_dice: 1.3108, decode.d0.loss_cls: 5.6480, decode.d0.loss_mask: 0.8236, decode.d0.loss_dice: 1.4944, decode.d1.loss_cls: 0.5945, decode.d1.loss_mask: 0.7940, decode.d1.loss_dice: 1.3999, decode.d2.loss_cls: 0.5674, decode.d2.loss_mask: 0.7729, decode.d2.loss_dice: 1.3458, decode.d3.loss_cls: 0.6001, decode.d3.loss_mask: 0.7692, decode.d3.loss_dice: 1.3095, decode.d4.loss_cls: 0.5746, decode.d4.loss_mask: 0.7719, decode.d4.loss_dice: 1.3126, decode.d5.loss_cls: 0.5708, decode.d5.loss_mask: 0.7744, decode.d5.loss_dice: 1.3087, decode.d6.loss_cls: 0.5770, decode.d6.loss_mask: 0.7717, decode.d6.loss_dice: 1.2981, decode.d7.loss_cls: 0.5460, decode.d7.loss_mask: 0.7734, decode.d7.loss_dice: 1.2968, decode.d8.loss_cls: 0.5268, decode.d8.loss_mask: 0.7684, decode.d8.loss_dice: 1.3175, loss: 31.9430
2023-02-21 22:56:55,715 - mmseg - INFO - Iter [450/80000]	lr: 4.274e-07, eta: 2 days, 13:04:16, time: 2.755, data_time: 0.021, memory: 31494, decode.loss_cls: 0.5598, decode.loss_mask: 0.7833, decode.loss_dice: 1.3295, decode.d0.loss_cls: 5.6222, decode.d0.loss_mask: 0.7897, decode.d0.loss_dice: 1.5070, decode.d1.loss_cls: 0.5157, decode.d1.loss_mask: 0.8323, decode.d1.loss_dice: 1.4500, decode.d2.loss_cls: 0.5509, decode.d2.loss_mask: 0.7698, decode.d2.loss_dice: 1.3504, decode.d3.loss_cls: 0.5324, decode.d3.loss_mask: 0.7671, decode.d3.loss_dice: 1.3368, decode.d4.loss_cls: 0.5188, decode.d4.loss_mask: 0.7753, decode.d4.loss_dice: 1.3511, decode.d5.loss_cls: 0.5173, decode.d5.loss_mask: 0.7676, decode.d5.loss_dice: 1.3547, decode.d6.loss_cls: 0.5481, decode.d6.loss_mask: 0.7720, decode.d6.loss_dice: 1.3297, decode.d7.loss_cls: 0.5275, decode.d7.loss_mask: 0.7768, decode.d7.loss_dice: 1.3317, decode.d8.loss_cls: 0.5306, decode.d8.loss_mask: 0.7813, decode.d8.loss_dice: 1.3336, loss: 31.9131
2023-02-21 22:59:13,468 - mmseg - INFO - Iter [500/80000]	lr: 4.747e-07, eta: 2 days, 13:00:48, time: 2.755, data_time: 0.023, memory: 31494, decode.loss_cls: 0.5586, decode.loss_mask: 0.7655, decode.loss_dice: 1.3157, decode.d0.loss_cls: 5.6081, decode.d0.loss_mask: 0.7918, decode.d0.loss_dice: 1.4542, decode.d1.loss_cls: 0.5481, decode.d1.loss_mask: 0.7887, decode.d1.loss_dice: 1.4346, decode.d2.loss_cls: 0.5470, decode.d2.loss_mask: 0.7745, decode.d2.loss_dice: 1.3646, decode.d3.loss_cls: 0.5789, decode.d3.loss_mask: 0.7731, decode.d3.loss_dice: 1.3457, decode.d4.loss_cls: 0.5544, decode.d4.loss_mask: 0.7598, decode.d4.loss_dice: 1.3395, decode.d5.loss_cls: 0.5596, decode.d5.loss_mask: 0.7637, decode.d5.loss_dice: 1.3345, decode.d6.loss_cls: 0.5438, decode.d6.loss_mask: 0.7616, decode.d6.loss_dice: 1.3256, decode.d7.loss_cls: 0.5446, decode.d7.loss_mask: 0.7713, decode.d7.loss_dice: 1.3353, decode.d8.loss_cls: 0.5657, decode.d8.loss_mask: 0.7594, decode.d8.loss_dice: 1.3415, loss: 31.9095
2023-02-21 23:01:31,332 - mmseg - INFO - Iter [550/80000]	lr: 5.219e-07, eta: 2 days, 12:57:50, time: 2.757, data_time: 0.024, memory: 31494, decode.loss_cls: 0.5176, decode.loss_mask: 0.7313, decode.loss_dice: 1.3219, decode.d0.loss_cls: 5.5903, decode.d0.loss_mask: 0.7406, decode.d0.loss_dice: 1.4494, decode.d1.loss_cls: 0.5147, decode.d1.loss_mask: 0.7431, decode.d1.loss_dice: 1.4241, decode.d2.loss_cls: 0.4914, decode.d2.loss_mask: 0.7285, decode.d2.loss_dice: 1.3596, decode.d3.loss_cls: 0.5199, decode.d3.loss_mask: 0.7173, decode.d3.loss_dice: 1.3277, decode.d4.loss_cls: 0.5231, decode.d4.loss_mask: 0.7212, decode.d4.loss_dice: 1.3230, decode.d5.loss_cls: 0.5238, decode.d5.loss_mask: 0.7266, decode.d5.loss_dice: 1.3148, decode.d6.loss_cls: 0.5384, decode.d6.loss_mask: 0.7309, decode.d6.loss_dice: 1.3025, decode.d7.loss_cls: 0.4848, decode.d7.loss_mask: 0.7437, decode.d7.loss_dice: 1.3410, decode.d8.loss_cls: 0.5128, decode.d8.loss_mask: 0.7316, decode.d8.loss_dice: 1.3448, loss: 31.0403
2023-02-21 23:03:49,448 - mmseg - INFO - Iter [600/80000]	lr: 5.691e-07, eta: 2 days, 12:55:31, time: 2.762, data_time: 0.023, memory: 31494, decode.loss_cls: 0.5772, decode.loss_mask: 0.7581, decode.loss_dice: 1.4131, decode.d0.loss_cls: 5.5818, decode.d0.loss_mask: 0.8168, decode.d0.loss_dice: 1.5860, decode.d1.loss_cls: 0.6210, decode.d1.loss_mask: 0.7886, decode.d1.loss_dice: 1.4881, decode.d2.loss_cls: 0.5879, decode.d2.loss_mask: 0.7672, decode.d2.loss_dice: 1.4396, decode.d3.loss_cls: 0.6226, decode.d3.loss_mask: 0.7526, decode.d3.loss_dice: 1.3937, decode.d4.loss_cls: 0.5882, decode.d4.loss_mask: 0.7602, decode.d4.loss_dice: 1.3891, decode.d5.loss_cls: 0.5761, decode.d5.loss_mask: 0.7574, decode.d5.loss_dice: 1.4004, decode.d6.loss_cls: 0.5825, decode.d6.loss_mask: 0.7567, decode.d6.loss_dice: 1.3908, decode.d7.loss_cls: 0.5763, decode.d7.loss_mask: 0.7516, decode.d7.loss_dice: 1.4190, decode.d8.loss_cls: 0.5727, decode.d8.loss_mask: 0.7635, decode.d8.loss_dice: 1.3963, loss: 32.8752
2023-02-21 23:06:07,184 - mmseg - INFO - Iter [650/80000]	lr: 6.162e-07, eta: 2 days, 12:52:26, time: 2.755, data_time: 0.022, memory: 31494, decode.loss_cls: 0.5658, decode.loss_mask: 0.7666, decode.loss_dice: 1.2756, decode.d0.loss_cls: 5.5479, decode.d0.loss_mask: 0.7754, decode.d0.loss_dice: 1.4193, decode.d1.loss_cls: 0.5611, decode.d1.loss_mask: 0.7990, decode.d1.loss_dice: 1.3586, decode.d2.loss_cls: 0.5303, decode.d2.loss_mask: 0.7750, decode.d2.loss_dice: 1.3049, decode.d3.loss_cls: 0.5625, decode.d3.loss_mask: 0.7675, decode.d3.loss_dice: 1.2808, decode.d4.loss_cls: 0.5648, decode.d4.loss_mask: 0.7664, decode.d4.loss_dice: 1.2771, decode.d5.loss_cls: 0.5599, decode.d5.loss_mask: 0.7735, decode.d5.loss_dice: 1.2752, decode.d6.loss_cls: 0.5648, decode.d6.loss_mask: 0.7629, decode.d6.loss_dice: 1.2808, decode.d7.loss_cls: 0.5563, decode.d7.loss_mask: 0.7610, decode.d7.loss_dice: 1.2830, decode.d8.loss_cls: 0.5631, decode.d8.loss_mask: 0.7621, decode.d8.loss_dice: 1.2858, loss: 31.3268
2023-02-21 23:08:24,826 - mmseg - INFO - Iter [700/80000]	lr: 6.632e-07, eta: 2 days, 12:49:17, time: 2.753, data_time: 0.021, memory: 31494, decode.loss_cls: 0.5194, decode.loss_mask: 0.7672, decode.loss_dice: 1.3198, decode.d0.loss_cls: 5.5661, decode.d0.loss_mask: 0.7957, decode.d0.loss_dice: 1.4346, decode.d1.loss_cls: 0.5025, decode.d1.loss_mask: 0.7962, decode.d1.loss_dice: 1.4067, decode.d2.loss_cls: 0.4947, decode.d2.loss_mask: 0.7802, decode.d2.loss_dice: 1.3504, decode.d3.loss_cls: 0.5142, decode.d3.loss_mask: 0.7841, decode.d3.loss_dice: 1.3332, decode.d4.loss_cls: 0.5377, decode.d4.loss_mask: 0.7796, decode.d4.loss_dice: 1.3053, decode.d5.loss_cls: 0.4998, decode.d5.loss_mask: 0.7946, decode.d5.loss_dice: 1.3158, decode.d6.loss_cls: 0.4981, decode.d6.loss_mask: 0.7784, decode.d6.loss_dice: 1.3250, decode.d7.loss_cls: 0.5226, decode.d7.loss_mask: 0.7655, decode.d7.loss_dice: 1.3269, decode.d8.loss_cls: 0.5222, decode.d8.loss_mask: 0.7635, decode.d8.loss_dice: 1.3217, loss: 31.4218
2023-02-21 23:10:45,257 - mmseg - INFO - Iter [750/80000]	lr: 7.102e-07, eta: 2 days, 12:51:10, time: 2.809, data_time: 0.071, memory: 31494, decode.loss_cls: 0.5971, decode.loss_mask: 0.8101, decode.loss_dice: 1.4261, decode.d0.loss_cls: 5.5094, decode.d0.loss_mask: 0.8378, decode.d0.loss_dice: 1.5856, decode.d1.loss_cls: 0.6277, decode.d1.loss_mask: 0.8229, decode.d1.loss_dice: 1.5368, decode.d2.loss_cls: 0.6184, decode.d2.loss_mask: 0.8108, decode.d2.loss_dice: 1.4638, decode.d3.loss_cls: 0.6412, decode.d3.loss_mask: 0.7967, decode.d3.loss_dice: 1.4290, decode.d4.loss_cls: 0.6275, decode.d4.loss_mask: 0.8072, decode.d4.loss_dice: 1.4250, decode.d5.loss_cls: 0.6431, decode.d5.loss_mask: 0.7971, decode.d5.loss_dice: 1.4311, decode.d6.loss_cls: 0.5991, decode.d6.loss_mask: 0.8008, decode.d6.loss_dice: 1.4165, decode.d7.loss_cls: 0.5814, decode.d7.loss_mask: 0.8090, decode.d7.loss_dice: 1.4229, decode.d8.loss_cls: 0.6138, decode.d8.loss_mask: 0.8116, decode.d8.loss_dice: 1.4224, loss: 33.7220
2023-02-21 23:13:08,144 - mmseg - INFO - Iter [800/80000]	lr: 7.572e-07, eta: 2 days, 12:56:34, time: 2.858, data_time: 0.025, memory: 31494, decode.loss_cls: 0.4849, decode.loss_mask: 0.7540, decode.loss_dice: 1.3040, decode.d0.loss_cls: 5.5145, decode.d0.loss_mask: 0.7911, decode.d0.loss_dice: 1.4389, decode.d1.loss_cls: 0.4855, decode.d1.loss_mask: 0.7636, decode.d1.loss_dice: 1.4000, decode.d2.loss_cls: 0.5038, decode.d2.loss_mask: 0.7538, decode.d2.loss_dice: 1.3488, decode.d3.loss_cls: 0.5004, decode.d3.loss_mask: 0.7503, decode.d3.loss_dice: 1.3186, decode.d4.loss_cls: 0.5100, decode.d4.loss_mask: 0.7555, decode.d4.loss_dice: 1.3157, decode.d5.loss_cls: 0.4726, decode.d5.loss_mask: 0.7580, decode.d5.loss_dice: 1.3183, decode.d6.loss_cls: 0.4836, decode.d6.loss_mask: 0.7632, decode.d6.loss_dice: 1.3102, decode.d7.loss_cls: 0.4795, decode.d7.loss_mask: 0.7614, decode.d7.loss_dice: 1.3230, decode.d8.loss_cls: 0.4955, decode.d8.loss_mask: 0.7615, decode.d8.loss_dice: 1.3020, loss: 30.9220
2023-02-21 23:15:44,506 - mmseg - INFO - Iter [850/80000]	lr: 8.040e-07, eta: 2 days, 13:21:58, time: 3.127, data_time: 0.026, memory: 31494, decode.loss_cls: 0.5651, decode.loss_mask: 0.7586, decode.loss_dice: 1.3175, decode.d0.loss_cls: 5.4638, decode.d0.loss_mask: 0.7727, decode.d0.loss_dice: 1.4913, decode.d1.loss_cls: 0.5408, decode.d1.loss_mask: 0.7833, decode.d1.loss_dice: 1.4514, decode.d2.loss_cls: 0.5304, decode.d2.loss_mask: 0.7574, decode.d2.loss_dice: 1.3818, decode.d3.loss_cls: 0.5723, decode.d3.loss_mask: 0.7481, decode.d3.loss_dice: 1.3290, decode.d4.loss_cls: 0.5720, decode.d4.loss_mask: 0.7559, decode.d4.loss_dice: 1.3432, decode.d5.loss_cls: 0.5586, decode.d5.loss_mask: 0.7655, decode.d5.loss_dice: 1.3444, decode.d6.loss_cls: 0.5901, decode.d6.loss_mask: 0.7602, decode.d6.loss_dice: 1.3143, decode.d7.loss_cls: 0.6058, decode.d7.loss_mask: 0.7549, decode.d7.loss_dice: 1.3263, decode.d8.loss_cls: 0.5593, decode.d8.loss_mask: 0.7547, decode.d8.loss_dice: 1.3253, loss: 31.7938
2023-02-21 23:18:08,680 - mmseg - INFO - Iter [900/80000]	lr: 8.509e-07, eta: 2 days, 13:26:24, time: 2.884, data_time: 0.022, memory: 31494, decode.loss_cls: 0.4621, decode.loss_mask: 0.7865, decode.loss_dice: 1.2266, decode.d0.loss_cls: 5.4518, decode.d0.loss_mask: 0.8183, decode.d0.loss_dice: 1.3352, decode.d1.loss_cls: 0.4381, decode.d1.loss_mask: 0.7995, decode.d1.loss_dice: 1.3131, decode.d2.loss_cls: 0.4354, decode.d2.loss_mask: 0.7937, decode.d2.loss_dice: 1.2702, decode.d3.loss_cls: 0.4599, decode.d3.loss_mask: 0.7961, decode.d3.loss_dice: 1.2394, decode.d4.loss_cls: 0.4646, decode.d4.loss_mask: 0.7920, decode.d4.loss_dice: 1.2393, decode.d5.loss_cls: 0.4422, decode.d5.loss_mask: 0.7901, decode.d5.loss_dice: 1.2405, decode.d6.loss_cls: 0.4605, decode.d6.loss_mask: 0.7929, decode.d6.loss_dice: 1.2283, decode.d7.loss_cls: 0.4461, decode.d7.loss_mask: 0.7985, decode.d7.loss_dice: 1.2515, decode.d8.loss_cls: 0.4783, decode.d8.loss_mask: 0.7972, decode.d8.loss_dice: 1.2461, loss: 30.0939
2023-02-21 23:20:45,272 - mmseg - INFO - Iter [950/80000]	lr: 8.976e-07, eta: 2 days, 13:47:19, time: 3.131, data_time: 0.028, memory: 31494, decode.loss_cls: 0.5211, decode.loss_mask: 0.7675, decode.loss_dice: 1.3021, decode.d0.loss_cls: 5.3886, decode.d0.loss_mask: 0.8030, decode.d0.loss_dice: 1.4006, decode.d1.loss_cls: 0.4702, decode.d1.loss_mask: 0.8036, decode.d1.loss_dice: 1.3992, decode.d2.loss_cls: 0.5062, decode.d2.loss_mask: 0.7881, decode.d2.loss_dice: 1.3368, decode.d3.loss_cls: 0.5158, decode.d3.loss_mask: 0.7810, decode.d3.loss_dice: 1.3053, decode.d4.loss_cls: 0.5253, decode.d4.loss_mask: 0.7777, decode.d4.loss_dice: 1.3056, decode.d5.loss_cls: 0.5199, decode.d5.loss_mask: 0.7794, decode.d5.loss_dice: 1.3223, decode.d6.loss_cls: 0.5024, decode.d6.loss_mask: 0.7775, decode.d6.loss_dice: 1.3094, decode.d7.loss_cls: 0.5013, decode.d7.loss_mask: 0.7742, decode.d7.loss_dice: 1.3228, decode.d8.loss_cls: 0.4879, decode.d8.loss_mask: 0.7715, decode.d8.loss_dice: 1.3102, loss: 31.0764
2023-02-21 23:23:02,854 - mmseg - INFO - Saving checkpoint at 1000 iterations
2023-02-21 23:23:26,996 - mmseg - INFO - Exp name: my_city.py
2023-02-21 23:23:26,996 - mmseg - INFO - Iter [1000/80000]	lr: 9.443e-07, eta: 2 days, 14:12:40, time: 3.235, data_time: 0.024, memory: 31494, decode.loss_cls: 0.5459, decode.loss_mask: 0.7462, decode.loss_dice: 1.3323, decode.d0.loss_cls: 5.3802, decode.d0.loss_mask: 0.7781, decode.d0.loss_dice: 1.4733, decode.d1.loss_cls: 0.5656, decode.d1.loss_mask: 0.7709, decode.d1.loss_dice: 1.4256, decode.d2.loss_cls: 0.5372, decode.d2.loss_mask: 0.7546, decode.d2.loss_dice: 1.3809, decode.d3.loss_cls: 0.5783, decode.d3.loss_mask: 0.7546, decode.d3.loss_dice: 1.3434, decode.d4.loss_cls: 0.5816, decode.d4.loss_mask: 0.7372, decode.d4.loss_dice: 1.3354, decode.d5.loss_cls: 0.5392, decode.d5.loss_mask: 0.7441, decode.d5.loss_dice: 1.3323, decode.d6.loss_cls: 0.5729, decode.d6.loss_mask: 0.7467, decode.d6.loss_dice: 1.3310, decode.d7.loss_cls: 0.5504, decode.d7.loss_mask: 0.7527, decode.d7.loss_dice: 1.3309, decode.d8.loss_cls: 0.5427, decode.d8.loss_mask: 0.7558, decode.d8.loss_dice: 1.3449, loss: 31.5650
[                                                  ] 0/357, elapsed: 0s, ETA:[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[                                ] 1/357, 0.0 task/s, elapsed: 66s, ETA: 23657s[                               ] 2/357, 0.0 task/s, elapsed: 129s, ETA: 22953s[                               ] 3/357, 0.0 task/s, elapsed: 192s, ETA: 22620s[                               ] 4/357, 0.0 task/s, elapsed: 254s, ETA: 22426s[                               ] 5/357, 0.0 task/s, elapsed: 317s, ETA: 22297s[                               ] 6/357, 0.0 task/s, elapsed: 384s, ETA: 22448s[                               ] 7/357, 0.0 task/s, elapsed: 450s, ETA: 22509s[                               ] 8/357, 0.0 task/s, elapsed: 517s, ETA: 22533s[                               ] 9/357, 0.0 task/s, elapsed: 579s, ETA: 22382s2023-02-21 23:35:43,195 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.10 (default, Jun  4 2021, 15:09:15) [GCC 7.5.0]
CUDA available: True
GPU 0: NVIDIA A100-PCIE-40GB
CUDA_HOME: /usr/local/cuda
NVCC: Build cuda_11.1.TC455_06.29190527_0
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.9.0+cu111
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.10.0+cu111
OpenCV: 4.6.0
MMCV: 1.4.2
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.1
MMSegmentation: 0.20.2+a1b84a3
------------------------------------------------------------

2023-02-21 23:35:43,195 - mmseg - INFO - Distributed training: False
2023-02-21 23:35:43,824 - mmseg - INFO - Config:
num_things_classes = 0
num_stuff_classes = 16
num_classes = 16
norm_cfg = dict(type='SyncBN', requires_grad=True)
model = dict(
    type='EncoderDecoderMask2FormerAug',
    pretrained='pretrained/beit_large_patch16_224_pt22k_ft22k.pth',
    backbone=dict(
        type='BEiTAdapter',
        patch_size=16,
        embed_dim=1024,
        depth=24,
        num_heads=16,
        mlp_ratio=4,
        qkv_bias=True,
        use_abs_pos_emb=False,
        use_rel_pos_bias=True,
        img_size=896,
        init_values=1e-06,
        drop_path_rate=0.3,
        conv_inplane=64,
        n_points=4,
        deform_num_heads=16,
        cffn_ratio=0.25,
        deform_ratio=0.5,
        with_cp=True,
        interaction_indexes=[[0, 5], [6, 11], [12, 17], [18, 23]]),
    decode_head=dict(
        type='Mask2FormerHead',
        in_channels=[1024, 1024, 1024, 1024],
        feat_channels=1024,
        out_channels=1024,
        in_index=[0, 1, 2, 3],
        num_things_classes=0,
        num_stuff_classes=16,
        num_queries=100,
        num_transformer_feat_level=3,
        pixel_decoder=dict(
            type='MSDeformAttnPixelDecoder',
            num_outs=3,
            norm_cfg=dict(type='GN', num_groups=32),
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=1024,
                        num_heads=32,
                        num_levels=3,
                        num_points=4,
                        im2col_step=64,
                        dropout=0.0,
                        batch_first=False,
                        norm_cfg=None,
                        init_cfg=None),
                    ffn_cfgs=dict(
                        type='FFN',
                        embed_dims=1024,
                        feedforward_channels=4096,
                        num_fcs=2,
                        ffn_drop=0.0,
                        act_cfg=dict(type='ReLU', inplace=True),
                        with_cp=True),
                    operation_order=('self_attn', 'norm', 'ffn', 'norm')),
                init_cfg=None),
            positional_encoding=dict(
                type='SinePositionalEncoding', num_feats=512, normalize=True),
            init_cfg=None),
        enforce_decoder_input_project=False,
        positional_encoding=dict(
            type='SinePositionalEncoding', num_feats=512, normalize=True),
        transformer_decoder=dict(
            type='DetrTransformerDecoder',
            return_intermediate=True,
            num_layers=9,
            transformerlayers=dict(
                type='DetrTransformerDecoderLayer',
                attn_cfgs=dict(
                    type='MultiheadAttention',
                    embed_dims=1024,
                    num_heads=32,
                    attn_drop=0.0,
                    proj_drop=0.0,
                    dropout_layer=None,
                    batch_first=False),
                ffn_cfgs=dict(
                    embed_dims=1024,
                    feedforward_channels=4096,
                    num_fcs=2,
                    act_cfg=dict(type='ReLU', inplace=True),
                    ffn_drop=0.0,
                    dropout_layer=None,
                    add_identity=True,
                    with_cp=True),
                feedforward_channels=4096,
                operation_order=('cross_attn', 'norm', 'self_attn', 'norm',
                                 'ffn', 'norm')),
            init_cfg=None),
        loss_cls=dict(
            type='CrossEntropyLoss',
            use_sigmoid=False,
            loss_weight=2.0,
            reduction='mean',
            class_weight=[
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 0.1
            ]),
        loss_mask=dict(
            type='CrossEntropyLoss',
            use_sigmoid=True,
            reduction='mean',
            loss_weight=5.0),
        loss_dice=dict(
            type='DiceLoss',
            use_sigmoid=True,
            activate=True,
            reduction='mean',
            naive_dice=True,
            eps=1.0,
            loss_weight=5.0)),
    train_cfg=dict(
        num_points=12544,
        oversample_ratio=3.0,
        importance_sample_ratio=0.75,
        assigner=dict(
            type='MaskHungarianAssigner',
            cls_cost=dict(type='ClassificationCost', weight=2.0),
            mask_cost=dict(
                type='CrossEntropyLossCost', weight=5.0, use_sigmoid=True),
            dice_cost=dict(
                type='DiceCost', weight=5.0, pred_act=True, eps=1.0)),
        sampler=dict(type='MaskPseudoSampler')),
    test_cfg=dict(
        panoptic_on=True,
        semantic_on=False,
        instance_on=True,
        max_per_image=100,
        iou_thr=0.8,
        filter_low_score=True,
        mode='slide',
        crop_size=(896, 896),
        stride=(512, 512)),
    init_cfg=None)
dataset_type = 'MyDataset'
data_root = '/root/autodl-tmp/data'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (896, 896)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=(896, 896), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(896, 896), pad_val=0, seg_pad_val=255),
    dict(type='ToMask'),
    dict(type='DefaultFormatBundle'),
    dict(
        type='Collect',
        keys=['img', 'gt_semantic_seg', 'gt_masks', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(2048, 1024),
        img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0],
        flip=True,
        transforms=[
            dict(
                type='SETR_Resize',
                keep_ratio=True,
                crop_size=(896, 896),
                setr_multi_scale=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=1,
    train=dict(
        type='MyDataset',
        data_root='/root/autodl-tmp/data',
        img_dir='images/train',
        ann_dir='annotations/train',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(
                type='Resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),
            dict(type='RandomCrop', crop_size=(896, 896), cat_max_ratio=0.75),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(896, 896), pad_val=0, seg_pad_val=255),
            dict(type='ToMask'),
            dict(type='DefaultFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'gt_semantic_seg', 'gt_masks', 'gt_labels'])
        ]),
    val=dict(
        type='MyDataset',
        data_root='/root/autodl-tmp/data',
        img_dir='images/val',
        ann_dir='annotations/val',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(2048, 1024),
                img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0],
                flip=True,
                transforms=[
                    dict(
                        type='SETR_Resize',
                        keep_ratio=True,
                        crop_size=(896, 896),
                        setr_multi_scale=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='MyDataset',
        data_root='/root/autodl-tmp/data',
        img_dir='images/val',
        ann_dir='annotations/val',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(2048, 1024),
                img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0],
                flip=True,
                transforms=[
                    dict(
                        type='SETR_Resize',
                        keep_ratio=True,
                        crop_size=(896, 896),
                        setr_multi_scale=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
log_config = dict(
    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = 'work_dirs/my_city/latest.pth'
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
optimizer = dict(
    type='AdamW',
    lr=2e-05,
    betas=(0.9, 0.999),
    weight_decay=0.05,
    constructor='LayerDecayOptimizerConstructor',
    paramwise_cfg=dict(num_layers=24, layer_decay_rate=0.9))
optimizer_config = dict()
lr_config = dict(
    policy='poly',
    warmup='linear',
    warmup_iters=1500,
    warmup_ratio=1e-06,
    power=1.0,
    min_lr=0.0,
    by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=80000)
checkpoint_config = dict(by_epoch=False, interval=1000, max_keep_ckpts=1)
evaluation = dict(
    interval=10000, metric='mIoU', pre_eval=True, save_best='mIoU')
pretrained = 'pretrained/beit_large_patch16_224_pt22k_ft22k.pth'
work_dir = './work_dirs/my_city'
gpu_ids = range(0, 1)
auto_resume = False

2023-02-21 23:35:43,828 - mmseg - INFO - Set random seed to 385385733, deterministic: False
Position interpolate for blocks.0.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.1.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.2.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.3.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.4.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.5.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.6.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.7.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.8.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.9.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.10.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.11.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.12.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.13.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.14.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.15.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.16.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.17.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.18.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.19.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.20.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.21.attn.relative_position_bias_table from 27x27 to 111x111
2023-02-21 23:35:52,514 - mmseg - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc_norm.weight, fc_norm.bias, head.weight, head.bias

missing keys in source state_dict: blocks.0.attn.relative_position_index, blocks.1.attn.relative_position_index, blocks.2.attn.relative_position_index, blocks.3.attn.relative_position_index, blocks.4.attn.relative_position_index, blocks.5.attn.relative_position_index, blocks.6.attn.relative_position_index, blocks.7.attn.relative_position_index, blocks.8.attn.relative_position_index, blocks.9.attn.relative_position_index, blocks.10.attn.relative_position_index, blocks.11.attn.relative_position_index, blocks.12.attn.relative_position_index, blocks.13.attn.relative_position_index, blocks.14.attn.relative_position_index, blocks.15.attn.relative_position_index, blocks.16.attn.relative_position_index, blocks.17.attn.relative_position_index, blocks.18.attn.relative_position_index, blocks.19.attn.relative_position_index, blocks.20.attn.relative_position_index, blocks.21.attn.relative_position_index, blocks.22.attn.relative_position_index, blocks.23.attn.relative_position_index

/root/autodl-tmp/ViT-Adapter/segmentation/mmseg_custom/models/losses/cross_entropy_loss.py:230: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.
  warnings.warn(
train.py:179: UserWarning: SyncBN is only supported with DDP. To be compatible with DP, we convert SyncBN to BN. Please use dist_train.sh which can avoid this error.
  warnings.warn(
2023-02-21 23:35:56,491 - mmseg - INFO - EncoderDecoderMask2FormerAug(
  (backbone): BEiTAdapter(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.013043479062616825)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.02608695812523365)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.03913043811917305)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.0521739162504673)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.06521739810705185)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.0782608762383461)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.09130435436964035)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1043478325009346)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.11739131063222885)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1304347962141037)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.14347827434539795)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (12): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1565217524766922)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (13): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.16956523060798645)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (14): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1826087087392807)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (15): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.19565218687057495)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (16): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.2086956650018692)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (17): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.22173914313316345)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (18): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.2347826212644577)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (19): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.24782609939575195)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (20): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.260869562625885)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (21): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.27391305565834045)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (22): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.2869565188884735)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (23): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=1024, out_features=3072, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.30000001192092896)
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (spm): SpatialPriorModule(
      (stem): Sequential(
        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (7): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (8): ReLU(inplace=True)
        (9): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      )
      (conv2): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (conv3): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (conv4): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (fc1): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))
      (fc2): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))
      (fc3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
      (fc4): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
    )
    (interactions): Sequential(
      (0): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=192, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=64, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1024, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
      (1): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=192, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=64, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1024, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
      (2): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=192, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=64, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1024, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
      (3): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=384, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=192, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=1024, out_features=128, bias=True)
            (attention_weights): Linear(in_features=1024, out_features=64, bias=True)
            (value_proj): Linear(in_features=1024, out_features=512, bias=True)
            (output_proj): Linear(in_features=512, out_features=1024, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=1024, out_features=256, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            )
            (act): GELU()
            (fc2): Linear(in_features=256, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (extra_extractors): Sequential(
          (0): Extractor(
            (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=1024, out_features=128, bias=True)
              (attention_weights): Linear(in_features=1024, out_features=64, bias=True)
              (value_proj): Linear(in_features=1024, out_features=512, bias=True)
              (output_proj): Linear(in_features=512, out_features=1024, bias=True)
            )
            (ffn): ConvFFN(
              (fc1): Linear(in_features=1024, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (drop_path): DropPath()
          )
          (1): Extractor(
            (query_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (feat_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=1024, out_features=128, bias=True)
              (attention_weights): Linear(in_features=1024, out_features=64, bias=True)
              (value_proj): Linear(in_features=1024, out_features=512, bias=True)
              (output_proj): Linear(in_features=512, out_features=1024, bias=True)
            )
            (ffn): ConvFFN(
              (fc1): Linear(in_features=1024, out_features=256, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              )
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (drop_path): DropPath()
          )
        )
      )
    )
    (up): ConvTranspose2d(1024, 1024, kernel_size=(2, 2), stride=(2, 2))
    (norm1): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm2): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm3): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm4): _BatchNormXd(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (decode_head): Mask2FormerHead(
    input_transform=multiple_select, ignore_index=255, align_corners=False
    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)
    (conv_seg): None
    (dropout): Dropout2d(p=0.1, inplace=False)
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
        (1): ConvModule(
          (conv): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
        (2): ConvModule(
          (conv): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
      )
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=1024, out_features=768, bias=True)
                (attention_weights): Linear(in_features=1024, out_features=384, bias=True)
                (value_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (output_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=4096, out_features=1024, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (postional_encoding): SinePositionalEncoding(num_feats=512, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
      (level_encoding): Embedding(3, 1024)
      (lateral_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
        )
      )
      (output_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)
          (activate): ReLU(inplace=True)
        )
      )
      (mask_feature): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
    )
    (transformer_decoder): DetrTransformerDecoder(
      (layers): ModuleList(
        (0): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (6): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (7): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (8): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=1024, out_features=4096, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=4096, out_features=1024, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (decoder_input_projs): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
    )
    (decoder_positional_encoding): SinePositionalEncoding(num_feats=512, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (query_embed): Embedding(100, 1024)
    (query_feat): Embedding(100, 1024)
    (level_embed): Embedding(3, 1024)
    (cls_embed): Linear(in_features=1024, out_features=17, bias=True)
    (mask_embed): Sequential(
      (0): Linear(in_features=1024, out_features=1024, bias=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=1024, out_features=1024, bias=True)
      (3): ReLU(inplace=True)
      (4): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (loss_cls): CrossEntropyLoss(avg_non_ignore=False)
    (loss_mask): CrossEntropyLoss(avg_non_ignore=False)
    (loss_dice): DiceLoss()
  )
)
2023-02-21 23:35:56,513 - mmseg - INFO - Loaded 1429 images
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.22.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
Position interpolate for blocks.23.attn.relative_position_bias_table from 27x27 to 111x111
x = [-54.999898443794464, -44.33237151632927, -35.57463714019768, -28.384787803571555, -22.482127222283516, -17.63621177863096, -13.657853856192851, -10.39173583513709, -7.71034639370451, -5.509002385228651, -3.701761941608667, -2.2180692481994626, -1, 0, 1, 2.2180692481994626, 3.701761941608667, 5.509002385228651, 7.71034639370451, 10.39173583513709, 13.657853856192851, 17.63621177863096, 22.482127222283516, 28.384787803571555, 35.57463714019768, 44.33237151632927, 54.999898443794464]
dx = [-55. -54. -53. -52. -51. -50. -49. -48. -47. -46. -45. -44. -43. -42.
 -41. -40. -39. -38. -37. -36. -35. -34. -33. -32. -31. -30. -29. -28.
 -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.
 -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.
   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.
  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.
  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.]
{'num_layers': 24, 'layer_decay_rate': 0.9}
Build LayerDecayOptimizerConstructor 0.900000 - 26
Param groups = {
  "layer_0_decay": {
    "param_names": [
      "backbone.cls_token",
      "backbone.patch_embed.proj.weight",
      "decode_head.query_embed.weight",
      "decode_head.query_feat.weight",
      "decode_head.level_embed.weight",
      "decode_head.cls_embed.weight",
      "decode_head.mask_embed.0.weight",
      "decode_head.mask_embed.2.weight",
      "decode_head.mask_embed.4.weight"
    ],
    "lr_scale": 0.0717897987691853,
    "lr": 1.4357959753837061e-06,
    "weight_decay": 0.05
  },
  "layer_25_decay": {
    "param_names": [
      "backbone.level_embed",
      "backbone.spm.stem.0.weight",
      "backbone.spm.stem.3.weight",
      "backbone.spm.stem.6.weight",
      "backbone.spm.conv2.0.weight",
      "backbone.spm.conv3.0.weight",
      "backbone.spm.conv4.0.weight",
      "backbone.spm.fc1.weight",
      "backbone.spm.fc2.weight",
      "backbone.spm.fc3.weight",
      "backbone.spm.fc4.weight",
      "backbone.interactions.0.injector.attn.sampling_offsets.weight",
      "backbone.interactions.0.injector.attn.attention_weights.weight",
      "backbone.interactions.0.injector.attn.value_proj.weight",
      "backbone.interactions.0.injector.attn.output_proj.weight",
      "backbone.interactions.0.extractor.attn.sampling_offsets.weight",
      "backbone.interactions.0.extractor.attn.attention_weights.weight",
      "backbone.interactions.0.extractor.attn.value_proj.weight",
      "backbone.interactions.0.extractor.attn.output_proj.weight",
      "backbone.interactions.0.extractor.ffn.fc1.weight",
      "backbone.interactions.0.extractor.ffn.dwconv.dwconv.weight",
      "backbone.interactions.0.extractor.ffn.fc2.weight",
      "backbone.interactions.1.injector.attn.sampling_offsets.weight",
      "backbone.interactions.1.injector.attn.attention_weights.weight",
      "backbone.interactions.1.injector.attn.value_proj.weight",
      "backbone.interactions.1.injector.attn.output_proj.weight",
      "backbone.interactions.1.extractor.attn.sampling_offsets.weight",
      "backbone.interactions.1.extractor.attn.attention_weights.weight",
      "backbone.interactions.1.extractor.attn.value_proj.weight",
      "backbone.interactions.1.extractor.attn.output_proj.weight",
      "backbone.interactions.1.extractor.ffn.fc1.weight",
      "backbone.interactions.1.extractor.ffn.dwconv.dwconv.weight",
      "backbone.interactions.1.extractor.ffn.fc2.weight",
      "backbone.interactions.2.injector.attn.sampling_offsets.weight",
      "backbone.interactions.2.injector.attn.attention_weights.weight",
      "backbone.interactions.2.injector.attn.value_proj.weight",
      "backbone.interactions.2.injector.attn.output_proj.weight",
      "backbone.interactions.2.extractor.attn.sampling_offsets.weight",
      "backbone.interactions.2.extractor.attn.attention_weights.weight",
      "backbone.interactions.2.extractor.attn.value_proj.weight",
      "backbone.interactions.2.extractor.attn.output_proj.weight",
      "backbone.interactions.2.extractor.ffn.fc1.weight",
      "backbone.interactions.2.extractor.ffn.dwconv.dwconv.weight",
      "backbone.interactions.2.extractor.ffn.fc2.weight",
      "backbone.interactions.3.injector.attn.sampling_offsets.weight",
      "backbone.interactions.3.injector.attn.attention_weights.weight",
      "backbone.interactions.3.injector.attn.value_proj.weight",
      "backbone.interactions.3.injector.attn.output_proj.weight",
      "backbone.interactions.3.extractor.attn.sampling_offsets.weight",
      "backbone.interactions.3.extractor.attn.attention_weights.weight",
      "backbone.interactions.3.extractor.attn.value_proj.weight",
      "backbone.interactions.3.extractor.attn.output_proj.weight",
      "backbone.interactions.3.extractor.ffn.fc1.weight",
      "backbone.interactions.3.extractor.ffn.dwconv.dwconv.weight",
      "backbone.interactions.3.extractor.ffn.fc2.weight",
      "backbone.interactions.3.extra_extractors.0.attn.sampling_offsets.weight",
      "backbone.interactions.3.extra_extractors.0.attn.attention_weights.weight",
      "backbone.interactions.3.extra_extractors.0.attn.value_proj.weight",
      "backbone.interactions.3.extra_extractors.0.attn.output_proj.weight",
      "backbone.interactions.3.extra_extractors.0.ffn.fc1.weight",
      "backbone.interactions.3.extra_extractors.0.ffn.dwconv.dwconv.weight",
      "backbone.interactions.3.extra_extractors.0.ffn.fc2.weight",
      "backbone.interactions.3.extra_extractors.1.attn.sampling_offsets.weight",
      "backbone.interactions.3.extra_extractors.1.attn.attention_weights.weight",
      "backbone.interactions.3.extra_extractors.1.attn.value_proj.weight",
      "backbone.interactions.3.extra_extractors.1.attn.output_proj.weight",
      "backbone.interactions.3.extra_extractors.1.ffn.fc1.weight",
      "backbone.interactions.3.extra_extractors.1.ffn.dwconv.dwconv.weight",
      "backbone.interactions.3.extra_extractors.1.ffn.fc2.weight",
      "backbone.up.weight",
      "decode_head.pixel_decoder.input_convs.0.conv.weight",
      "decode_head.pixel_decoder.input_convs.1.conv.weight",
      "decode_head.pixel_decoder.input_convs.2.conv.weight",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.level_encoding.weight",
      "decode_head.pixel_decoder.lateral_convs.0.conv.weight",
      "decode_head.pixel_decoder.output_convs.0.conv.weight",
      "decode_head.pixel_decoder.mask_feature.weight",
      "decode_head.transformer_decoder.layers.0.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.0.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.0.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.0.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.0.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.0.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.1.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.1.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.1.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.1.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.1.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.1.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.2.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.2.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.2.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.2.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.2.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.2.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.3.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.3.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.3.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.3.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.3.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.3.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.4.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.4.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.4.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.4.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.4.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.4.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.5.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.5.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.5.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.5.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.5.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.5.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.6.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.6.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.6.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.6.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.6.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.6.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.7.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.7.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.7.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.7.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.7.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.7.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.8.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.8.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.8.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.8.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.8.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.8.ffns.0.layers.1.weight"
    ],
    "lr_scale": 1.0,
    "lr": 2e-05,
    "weight_decay": 0.05
  },
  "layer_0_no_decay": {
    "param_names": [
      "backbone.patch_embed.proj.bias",
      "decode_head.cls_embed.bias",
      "decode_head.mask_embed.0.bias",
      "decode_head.mask_embed.2.bias",
      "decode_head.mask_embed.4.bias"
    ],
    "lr_scale": 0.0717897987691853,
    "lr": 1.4357959753837061e-06,
    "weight_decay": 0.0
  },
  "layer_1_no_decay": {
    "param_names": [
      "backbone.blocks.0.gamma_1",
      "backbone.blocks.0.gamma_2",
      "backbone.blocks.0.norm1.weight",
      "backbone.blocks.0.norm1.bias",
      "backbone.blocks.0.attn.q_bias",
      "backbone.blocks.0.attn.v_bias",
      "backbone.blocks.0.attn.proj.bias",
      "backbone.blocks.0.norm2.weight",
      "backbone.blocks.0.norm2.bias",
      "backbone.blocks.0.mlp.fc1.bias",
      "backbone.blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.07976644307687256,
    "lr": 1.5953288615374513e-06,
    "weight_decay": 0.0
  },
  "layer_1_decay": {
    "param_names": [
      "backbone.blocks.0.attn.relative_position_bias_table",
      "backbone.blocks.0.attn.qkv.weight",
      "backbone.blocks.0.attn.proj.weight",
      "backbone.blocks.0.mlp.fc1.weight",
      "backbone.blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.07976644307687256,
    "lr": 1.5953288615374513e-06,
    "weight_decay": 0.05
  },
  "layer_2_no_decay": {
    "param_names": [
      "backbone.blocks.1.gamma_1",
      "backbone.blocks.1.gamma_2",
      "backbone.blocks.1.norm1.weight",
      "backbone.blocks.1.norm1.bias",
      "backbone.blocks.1.attn.q_bias",
      "backbone.blocks.1.attn.v_bias",
      "backbone.blocks.1.attn.proj.bias",
      "backbone.blocks.1.norm2.weight",
      "backbone.blocks.1.norm2.bias",
      "backbone.blocks.1.mlp.fc1.bias",
      "backbone.blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.08862938119652507,
    "lr": 1.7725876239305016e-06,
    "weight_decay": 0.0
  },
  "layer_2_decay": {
    "param_names": [
      "backbone.blocks.1.attn.relative_position_bias_table",
      "backbone.blocks.1.attn.qkv.weight",
      "backbone.blocks.1.attn.proj.weight",
      "backbone.blocks.1.mlp.fc1.weight",
      "backbone.blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.08862938119652507,
    "lr": 1.7725876239305016e-06,
    "weight_decay": 0.05
  },
  "layer_3_no_decay": {
    "param_names": [
      "backbone.blocks.2.gamma_1",
      "backbone.blocks.2.gamma_2",
      "backbone.blocks.2.norm1.weight",
      "backbone.blocks.2.norm1.bias",
      "backbone.blocks.2.attn.q_bias",
      "backbone.blocks.2.attn.v_bias",
      "backbone.blocks.2.attn.proj.bias",
      "backbone.blocks.2.norm2.weight",
      "backbone.blocks.2.norm2.bias",
      "backbone.blocks.2.mlp.fc1.bias",
      "backbone.blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.09847709021836118,
    "lr": 1.969541804367224e-06,
    "weight_decay": 0.0
  },
  "layer_3_decay": {
    "param_names": [
      "backbone.blocks.2.attn.relative_position_bias_table",
      "backbone.blocks.2.attn.qkv.weight",
      "backbone.blocks.2.attn.proj.weight",
      "backbone.blocks.2.mlp.fc1.weight",
      "backbone.blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.09847709021836118,
    "lr": 1.969541804367224e-06,
    "weight_decay": 0.05
  },
  "layer_4_no_decay": {
    "param_names": [
      "backbone.blocks.3.gamma_1",
      "backbone.blocks.3.gamma_2",
      "backbone.blocks.3.norm1.weight",
      "backbone.blocks.3.norm1.bias",
      "backbone.blocks.3.attn.q_bias",
      "backbone.blocks.3.attn.v_bias",
      "backbone.blocks.3.attn.proj.bias",
      "backbone.blocks.3.norm2.weight",
      "backbone.blocks.3.norm2.bias",
      "backbone.blocks.3.mlp.fc1.bias",
      "backbone.blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.10941898913151242,
    "lr": 2.1883797826302486e-06,
    "weight_decay": 0.0
  },
  "layer_4_decay": {
    "param_names": [
      "backbone.blocks.3.attn.relative_position_bias_table",
      "backbone.blocks.3.attn.qkv.weight",
      "backbone.blocks.3.attn.proj.weight",
      "backbone.blocks.3.mlp.fc1.weight",
      "backbone.blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.10941898913151242,
    "lr": 2.1883797826302486e-06,
    "weight_decay": 0.05
  },
  "layer_5_no_decay": {
    "param_names": [
      "backbone.blocks.4.gamma_1",
      "backbone.blocks.4.gamma_2",
      "backbone.blocks.4.norm1.weight",
      "backbone.blocks.4.norm1.bias",
      "backbone.blocks.4.attn.q_bias",
      "backbone.blocks.4.attn.v_bias",
      "backbone.blocks.4.attn.proj.bias",
      "backbone.blocks.4.norm2.weight",
      "backbone.blocks.4.norm2.bias",
      "backbone.blocks.4.mlp.fc1.bias",
      "backbone.blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.12157665459056935,
    "lr": 2.431533091811387e-06,
    "weight_decay": 0.0
  },
  "layer_5_decay": {
    "param_names": [
      "backbone.blocks.4.attn.relative_position_bias_table",
      "backbone.blocks.4.attn.qkv.weight",
      "backbone.blocks.4.attn.proj.weight",
      "backbone.blocks.4.mlp.fc1.weight",
      "backbone.blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.12157665459056935,
    "lr": 2.431533091811387e-06,
    "weight_decay": 0.05
  },
  "layer_6_no_decay": {
    "param_names": [
      "backbone.blocks.5.gamma_1",
      "backbone.blocks.5.gamma_2",
      "backbone.blocks.5.norm1.weight",
      "backbone.blocks.5.norm1.bias",
      "backbone.blocks.5.attn.q_bias",
      "backbone.blocks.5.attn.v_bias",
      "backbone.blocks.5.attn.proj.bias",
      "backbone.blocks.5.norm2.weight",
      "backbone.blocks.5.norm2.bias",
      "backbone.blocks.5.mlp.fc1.bias",
      "backbone.blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.13508517176729928,
    "lr": 2.701703435345986e-06,
    "weight_decay": 0.0
  },
  "layer_6_decay": {
    "param_names": [
      "backbone.blocks.5.attn.relative_position_bias_table",
      "backbone.blocks.5.attn.qkv.weight",
      "backbone.blocks.5.attn.proj.weight",
      "backbone.blocks.5.mlp.fc1.weight",
      "backbone.blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.13508517176729928,
    "lr": 2.701703435345986e-06,
    "weight_decay": 0.05
  },
  "layer_7_no_decay": {
    "param_names": [
      "backbone.blocks.6.gamma_1",
      "backbone.blocks.6.gamma_2",
      "backbone.blocks.6.norm1.weight",
      "backbone.blocks.6.norm1.bias",
      "backbone.blocks.6.attn.q_bias",
      "backbone.blocks.6.attn.v_bias",
      "backbone.blocks.6.attn.proj.bias",
      "backbone.blocks.6.norm2.weight",
      "backbone.blocks.6.norm2.bias",
      "backbone.blocks.6.mlp.fc1.bias",
      "backbone.blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.15009463529699918,
    "lr": 3.001892705939984e-06,
    "weight_decay": 0.0
  },
  "layer_7_decay": {
    "param_names": [
      "backbone.blocks.6.attn.relative_position_bias_table",
      "backbone.blocks.6.attn.qkv.weight",
      "backbone.blocks.6.attn.proj.weight",
      "backbone.blocks.6.mlp.fc1.weight",
      "backbone.blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.15009463529699918,
    "lr": 3.001892705939984e-06,
    "weight_decay": 0.05
  },
  "layer_8_no_decay": {
    "param_names": [
      "backbone.blocks.7.gamma_1",
      "backbone.blocks.7.gamma_2",
      "backbone.blocks.7.norm1.weight",
      "backbone.blocks.7.norm1.bias",
      "backbone.blocks.7.attn.q_bias",
      "backbone.blocks.7.attn.v_bias",
      "backbone.blocks.7.attn.proj.bias",
      "backbone.blocks.7.norm2.weight",
      "backbone.blocks.7.norm2.bias",
      "backbone.blocks.7.mlp.fc1.bias",
      "backbone.blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.16677181699666577,
    "lr": 3.3354363399333156e-06,
    "weight_decay": 0.0
  },
  "layer_8_decay": {
    "param_names": [
      "backbone.blocks.7.attn.relative_position_bias_table",
      "backbone.blocks.7.attn.qkv.weight",
      "backbone.blocks.7.attn.proj.weight",
      "backbone.blocks.7.mlp.fc1.weight",
      "backbone.blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.16677181699666577,
    "lr": 3.3354363399333156e-06,
    "weight_decay": 0.05
  },
  "layer_9_no_decay": {
    "param_names": [
      "backbone.blocks.8.gamma_1",
      "backbone.blocks.8.gamma_2",
      "backbone.blocks.8.norm1.weight",
      "backbone.blocks.8.norm1.bias",
      "backbone.blocks.8.attn.q_bias",
      "backbone.blocks.8.attn.v_bias",
      "backbone.blocks.8.attn.proj.bias",
      "backbone.blocks.8.norm2.weight",
      "backbone.blocks.8.norm2.bias",
      "backbone.blocks.8.mlp.fc1.bias",
      "backbone.blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.18530201888518416,
    "lr": 3.7060403777036836e-06,
    "weight_decay": 0.0
  },
  "layer_9_decay": {
    "param_names": [
      "backbone.blocks.8.attn.relative_position_bias_table",
      "backbone.blocks.8.attn.qkv.weight",
      "backbone.blocks.8.attn.proj.weight",
      "backbone.blocks.8.mlp.fc1.weight",
      "backbone.blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.18530201888518416,
    "lr": 3.7060403777036836e-06,
    "weight_decay": 0.05
  },
  "layer_10_no_decay": {
    "param_names": [
      "backbone.blocks.9.gamma_1",
      "backbone.blocks.9.gamma_2",
      "backbone.blocks.9.norm1.weight",
      "backbone.blocks.9.norm1.bias",
      "backbone.blocks.9.attn.q_bias",
      "backbone.blocks.9.attn.v_bias",
      "backbone.blocks.9.attn.proj.bias",
      "backbone.blocks.9.norm2.weight",
      "backbone.blocks.9.norm2.bias",
      "backbone.blocks.9.mlp.fc1.bias",
      "backbone.blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.20589113209464907,
    "lr": 4.117822641892982e-06,
    "weight_decay": 0.0
  },
  "layer_10_decay": {
    "param_names": [
      "backbone.blocks.9.attn.relative_position_bias_table",
      "backbone.blocks.9.attn.qkv.weight",
      "backbone.blocks.9.attn.proj.weight",
      "backbone.blocks.9.mlp.fc1.weight",
      "backbone.blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.20589113209464907,
    "lr": 4.117822641892982e-06,
    "weight_decay": 0.05
  },
  "layer_11_no_decay": {
    "param_names": [
      "backbone.blocks.10.gamma_1",
      "backbone.blocks.10.gamma_2",
      "backbone.blocks.10.norm1.weight",
      "backbone.blocks.10.norm1.bias",
      "backbone.blocks.10.attn.q_bias",
      "backbone.blocks.10.attn.v_bias",
      "backbone.blocks.10.attn.proj.bias",
      "backbone.blocks.10.norm2.weight",
      "backbone.blocks.10.norm2.bias",
      "backbone.blocks.10.mlp.fc1.bias",
      "backbone.blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.2287679245496101,
    "lr": 4.575358490992202e-06,
    "weight_decay": 0.0
  },
  "layer_11_decay": {
    "param_names": [
      "backbone.blocks.10.attn.relative_position_bias_table",
      "backbone.blocks.10.attn.qkv.weight",
      "backbone.blocks.10.attn.proj.weight",
      "backbone.blocks.10.mlp.fc1.weight",
      "backbone.blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.2287679245496101,
    "lr": 4.575358490992202e-06,
    "weight_decay": 0.05
  },
  "layer_12_no_decay": {
    "param_names": [
      "backbone.blocks.11.gamma_1",
      "backbone.blocks.11.gamma_2",
      "backbone.blocks.11.norm1.weight",
      "backbone.blocks.11.norm1.bias",
      "backbone.blocks.11.attn.q_bias",
      "backbone.blocks.11.attn.v_bias",
      "backbone.blocks.11.attn.proj.bias",
      "backbone.blocks.11.norm2.weight",
      "backbone.blocks.11.norm2.bias",
      "backbone.blocks.11.mlp.fc1.bias",
      "backbone.blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.2541865828329001,
    "lr": 5.083731656658002e-06,
    "weight_decay": 0.0
  },
  "layer_12_decay": {
    "param_names": [
      "backbone.blocks.11.attn.relative_position_bias_table",
      "backbone.blocks.11.attn.qkv.weight",
      "backbone.blocks.11.attn.proj.weight",
      "backbone.blocks.11.mlp.fc1.weight",
      "backbone.blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.2541865828329001,
    "lr": 5.083731656658002e-06,
    "weight_decay": 0.05
  },
  "layer_13_no_decay": {
    "param_names": [
      "backbone.blocks.12.gamma_1",
      "backbone.blocks.12.gamma_2",
      "backbone.blocks.12.norm1.weight",
      "backbone.blocks.12.norm1.bias",
      "backbone.blocks.12.attn.q_bias",
      "backbone.blocks.12.attn.v_bias",
      "backbone.blocks.12.attn.proj.bias",
      "backbone.blocks.12.norm2.weight",
      "backbone.blocks.12.norm2.bias",
      "backbone.blocks.12.mlp.fc1.bias",
      "backbone.blocks.12.mlp.fc2.bias"
    ],
    "lr_scale": 0.2824295364810001,
    "lr": 5.648590729620003e-06,
    "weight_decay": 0.0
  },
  "layer_13_decay": {
    "param_names": [
      "backbone.blocks.12.attn.relative_position_bias_table",
      "backbone.blocks.12.attn.qkv.weight",
      "backbone.blocks.12.attn.proj.weight",
      "backbone.blocks.12.mlp.fc1.weight",
      "backbone.blocks.12.mlp.fc2.weight"
    ],
    "lr_scale": 0.2824295364810001,
    "lr": 5.648590729620003e-06,
    "weight_decay": 0.05
  },
  "layer_14_no_decay": {
    "param_names": [
      "backbone.blocks.13.gamma_1",
      "backbone.blocks.13.gamma_2",
      "backbone.blocks.13.norm1.weight",
      "backbone.blocks.13.norm1.bias",
      "backbone.blocks.13.attn.q_bias",
      "backbone.blocks.13.attn.v_bias",
      "backbone.blocks.13.attn.proj.bias",
      "backbone.blocks.13.norm2.weight",
      "backbone.blocks.13.norm2.bias",
      "backbone.blocks.13.mlp.fc1.bias",
      "backbone.blocks.13.mlp.fc2.bias"
    ],
    "lr_scale": 0.31381059609000006,
    "lr": 6.276211921800002e-06,
    "weight_decay": 0.0
  },
  "layer_14_decay": {
    "param_names": [
      "backbone.blocks.13.attn.relative_position_bias_table",
      "backbone.blocks.13.attn.qkv.weight",
      "backbone.blocks.13.attn.proj.weight",
      "backbone.blocks.13.mlp.fc1.weight",
      "backbone.blocks.13.mlp.fc2.weight"
    ],
    "lr_scale": 0.31381059609000006,
    "lr": 6.276211921800002e-06,
    "weight_decay": 0.05
  },
  "layer_15_no_decay": {
    "param_names": [
      "backbone.blocks.14.gamma_1",
      "backbone.blocks.14.gamma_2",
      "backbone.blocks.14.norm1.weight",
      "backbone.blocks.14.norm1.bias",
      "backbone.blocks.14.attn.q_bias",
      "backbone.blocks.14.attn.v_bias",
      "backbone.blocks.14.attn.proj.bias",
      "backbone.blocks.14.norm2.weight",
      "backbone.blocks.14.norm2.bias",
      "backbone.blocks.14.mlp.fc1.bias",
      "backbone.blocks.14.mlp.fc2.bias"
    ],
    "lr_scale": 0.3486784401000001,
    "lr": 6.973568802000002e-06,
    "weight_decay": 0.0
  },
  "layer_15_decay": {
    "param_names": [
      "backbone.blocks.14.attn.relative_position_bias_table",
      "backbone.blocks.14.attn.qkv.weight",
      "backbone.blocks.14.attn.proj.weight",
      "backbone.blocks.14.mlp.fc1.weight",
      "backbone.blocks.14.mlp.fc2.weight"
    ],
    "lr_scale": 0.3486784401000001,
    "lr": 6.973568802000002e-06,
    "weight_decay": 0.05
  },
  "layer_16_no_decay": {
    "param_names": [
      "backbone.blocks.15.gamma_1",
      "backbone.blocks.15.gamma_2",
      "backbone.blocks.15.norm1.weight",
      "backbone.blocks.15.norm1.bias",
      "backbone.blocks.15.attn.q_bias",
      "backbone.blocks.15.attn.v_bias",
      "backbone.blocks.15.attn.proj.bias",
      "backbone.blocks.15.norm2.weight",
      "backbone.blocks.15.norm2.bias",
      "backbone.blocks.15.mlp.fc1.bias",
      "backbone.blocks.15.mlp.fc2.bias"
    ],
    "lr_scale": 0.3874204890000001,
    "lr": 7.748409780000003e-06,
    "weight_decay": 0.0
  },
  "layer_16_decay": {
    "param_names": [
      "backbone.blocks.15.attn.relative_position_bias_table",
      "backbone.blocks.15.attn.qkv.weight",
      "backbone.blocks.15.attn.proj.weight",
      "backbone.blocks.15.mlp.fc1.weight",
      "backbone.blocks.15.mlp.fc2.weight"
    ],
    "lr_scale": 0.3874204890000001,
    "lr": 7.748409780000003e-06,
    "weight_decay": 0.05
  },
  "layer_17_no_decay": {
    "param_names": [
      "backbone.blocks.16.gamma_1",
      "backbone.blocks.16.gamma_2",
      "backbone.blocks.16.norm1.weight",
      "backbone.blocks.16.norm1.bias",
      "backbone.blocks.16.attn.q_bias",
      "backbone.blocks.16.attn.v_bias",
      "backbone.blocks.16.attn.proj.bias",
      "backbone.blocks.16.norm2.weight",
      "backbone.blocks.16.norm2.bias",
      "backbone.blocks.16.mlp.fc1.bias",
      "backbone.blocks.16.mlp.fc2.bias"
    ],
    "lr_scale": 0.4304672100000001,
    "lr": 8.609344200000003e-06,
    "weight_decay": 0.0
  },
  "layer_17_decay": {
    "param_names": [
      "backbone.blocks.16.attn.relative_position_bias_table",
      "backbone.blocks.16.attn.qkv.weight",
      "backbone.blocks.16.attn.proj.weight",
      "backbone.blocks.16.mlp.fc1.weight",
      "backbone.blocks.16.mlp.fc2.weight"
    ],
    "lr_scale": 0.4304672100000001,
    "lr": 8.609344200000003e-06,
    "weight_decay": 0.05
  },
  "layer_18_no_decay": {
    "param_names": [
      "backbone.blocks.17.gamma_1",
      "backbone.blocks.17.gamma_2",
      "backbone.blocks.17.norm1.weight",
      "backbone.blocks.17.norm1.bias",
      "backbone.blocks.17.attn.q_bias",
      "backbone.blocks.17.attn.v_bias",
      "backbone.blocks.17.attn.proj.bias",
      "backbone.blocks.17.norm2.weight",
      "backbone.blocks.17.norm2.bias",
      "backbone.blocks.17.mlp.fc1.bias",
      "backbone.blocks.17.mlp.fc2.bias"
    ],
    "lr_scale": 0.4782969000000001,
    "lr": 9.565938000000002e-06,
    "weight_decay": 0.0
  },
  "layer_18_decay": {
    "param_names": [
      "backbone.blocks.17.attn.relative_position_bias_table",
      "backbone.blocks.17.attn.qkv.weight",
      "backbone.blocks.17.attn.proj.weight",
      "backbone.blocks.17.mlp.fc1.weight",
      "backbone.blocks.17.mlp.fc2.weight"
    ],
    "lr_scale": 0.4782969000000001,
    "lr": 9.565938000000002e-06,
    "weight_decay": 0.05
  },
  "layer_19_no_decay": {
    "param_names": [
      "backbone.blocks.18.gamma_1",
      "backbone.blocks.18.gamma_2",
      "backbone.blocks.18.norm1.weight",
      "backbone.blocks.18.norm1.bias",
      "backbone.blocks.18.attn.q_bias",
      "backbone.blocks.18.attn.v_bias",
      "backbone.blocks.18.attn.proj.bias",
      "backbone.blocks.18.norm2.weight",
      "backbone.blocks.18.norm2.bias",
      "backbone.blocks.18.mlp.fc1.bias",
      "backbone.blocks.18.mlp.fc2.bias"
    ],
    "lr_scale": 0.531441,
    "lr": 1.0628820000000002e-05,
    "weight_decay": 0.0
  },
  "layer_19_decay": {
    "param_names": [
      "backbone.blocks.18.attn.relative_position_bias_table",
      "backbone.blocks.18.attn.qkv.weight",
      "backbone.blocks.18.attn.proj.weight",
      "backbone.blocks.18.mlp.fc1.weight",
      "backbone.blocks.18.mlp.fc2.weight"
    ],
    "lr_scale": 0.531441,
    "lr": 1.0628820000000002e-05,
    "weight_decay": 0.05
  },
  "layer_20_no_decay": {
    "param_names": [
      "backbone.blocks.19.gamma_1",
      "backbone.blocks.19.gamma_2",
      "backbone.blocks.19.norm1.weight",
      "backbone.blocks.19.norm1.bias",
      "backbone.blocks.19.attn.q_bias",
      "backbone.blocks.19.attn.v_bias",
      "backbone.blocks.19.attn.proj.bias",
      "backbone.blocks.19.norm2.weight",
      "backbone.blocks.19.norm2.bias",
      "backbone.blocks.19.mlp.fc1.bias",
      "backbone.blocks.19.mlp.fc2.bias"
    ],
    "lr_scale": 0.5904900000000001,
    "lr": 1.1809800000000002e-05,
    "weight_decay": 0.0
  },
  "layer_20_decay": {
    "param_names": [
      "backbone.blocks.19.attn.relative_position_bias_table",
      "backbone.blocks.19.attn.qkv.weight",
      "backbone.blocks.19.attn.proj.weight",
      "backbone.blocks.19.mlp.fc1.weight",
      "backbone.blocks.19.mlp.fc2.weight"
    ],
    "lr_scale": 0.5904900000000001,
    "lr": 1.1809800000000002e-05,
    "weight_decay": 0.05
  },
  "layer_21_no_decay": {
    "param_names": [
      "backbone.blocks.20.gamma_1",
      "backbone.blocks.20.gamma_2",
      "backbone.blocks.20.norm1.weight",
      "backbone.blocks.20.norm1.bias",
      "backbone.blocks.20.attn.q_bias",
      "backbone.blocks.20.attn.v_bias",
      "backbone.blocks.20.attn.proj.bias",
      "backbone.blocks.20.norm2.weight",
      "backbone.blocks.20.norm2.bias",
      "backbone.blocks.20.mlp.fc1.bias",
      "backbone.blocks.20.mlp.fc2.bias"
    ],
    "lr_scale": 0.6561,
    "lr": 1.3122e-05,
    "weight_decay": 0.0
  },
  "layer_21_decay": {
    "param_names": [
      "backbone.blocks.20.attn.relative_position_bias_table",
      "backbone.blocks.20.attn.qkv.weight",
      "backbone.blocks.20.attn.proj.weight",
      "backbone.blocks.20.mlp.fc1.weight",
      "backbone.blocks.20.mlp.fc2.weight"
    ],
    "lr_scale": 0.6561,
    "lr": 1.3122e-05,
    "weight_decay": 0.05
  },
  "layer_22_no_decay": {
    "param_names": [
      "backbone.blocks.21.gamma_1",
      "backbone.blocks.21.gamma_2",
      "backbone.blocks.21.norm1.weight",
      "backbone.blocks.21.norm1.bias",
      "backbone.blocks.21.attn.q_bias",
      "backbone.blocks.21.attn.v_bias",
      "backbone.blocks.21.attn.proj.bias",
      "backbone.blocks.21.norm2.weight",
      "backbone.blocks.21.norm2.bias",
      "backbone.blocks.21.mlp.fc1.bias",
      "backbone.blocks.21.mlp.fc2.bias"
    ],
    "lr_scale": 0.7290000000000001,
    "lr": 1.4580000000000003e-05,
    "weight_decay": 0.0
  },
  "layer_22_decay": {
    "param_names": [
      "backbone.blocks.21.attn.relative_position_bias_table",
      "backbone.blocks.21.attn.qkv.weight",
      "backbone.blocks.21.attn.proj.weight",
      "backbone.blocks.21.mlp.fc1.weight",
      "backbone.blocks.21.mlp.fc2.weight"
    ],
    "lr_scale": 0.7290000000000001,
    "lr": 1.4580000000000003e-05,
    "weight_decay": 0.05
  },
  "layer_23_no_decay": {
    "param_names": [
      "backbone.blocks.22.gamma_1",
      "backbone.blocks.22.gamma_2",
      "backbone.blocks.22.norm1.weight",
      "backbone.blocks.22.norm1.bias",
      "backbone.blocks.22.attn.q_bias",
      "backbone.blocks.22.attn.v_bias",
      "backbone.blocks.22.attn.proj.bias",
      "backbone.blocks.22.norm2.weight",
      "backbone.blocks.22.norm2.bias",
      "backbone.blocks.22.mlp.fc1.bias",
      "backbone.blocks.22.mlp.fc2.bias"
    ],
    "lr_scale": 0.81,
    "lr": 1.62e-05,
    "weight_decay": 0.0
  },
  "layer_23_decay": {
    "param_names": [
      "backbone.blocks.22.attn.relative_position_bias_table",
      "backbone.blocks.22.attn.qkv.weight",
      "backbone.blocks.22.attn.proj.weight",
      "backbone.blocks.22.mlp.fc1.weight",
      "backbone.blocks.22.mlp.fc2.weight"
    ],
    "lr_scale": 0.81,
    "lr": 1.62e-05,
    "weight_decay": 0.05
  },
  "layer_24_no_decay": {
    "param_names": [
      "backbone.blocks.23.gamma_1",
      "backbone.blocks.23.gamma_2",
      "backbone.blocks.23.norm1.weight",
      "backbone.blocks.23.norm1.bias",
      "backbone.blocks.23.attn.q_bias",
      "backbone.blocks.23.attn.v_bias",
      "backbone.blocks.23.attn.proj.bias",
      "backbone.blocks.23.norm2.weight",
      "backbone.blocks.23.norm2.bias",
      "backbone.blocks.23.mlp.fc1.bias",
      "backbone.blocks.23.mlp.fc2.bias"
    ],
    "lr_scale": 0.9,
    "lr": 1.8e-05,
    "weight_decay": 0.0
  },
  "layer_24_decay": {
    "param_names": [
      "backbone.blocks.23.attn.relative_position_bias_table",
      "backbone.blocks.23.attn.qkv.weight",
      "backbone.blocks.23.attn.proj.weight",
      "backbone.blocks.23.mlp.fc1.weight",
      "backbone.blocks.23.mlp.fc2.weight"
    ],
    "lr_scale": 0.9,
    "lr": 1.8e-05,
    "weight_decay": 0.05
  },
  "layer_25_no_decay": {
    "param_names": [
      "backbone.spm.stem.1.weight",
      "backbone.spm.stem.1.bias",
      "backbone.spm.stem.4.weight",
      "backbone.spm.stem.4.bias",
      "backbone.spm.stem.7.weight",
      "backbone.spm.stem.7.bias",
      "backbone.spm.conv2.1.weight",
      "backbone.spm.conv2.1.bias",
      "backbone.spm.conv3.1.weight",
      "backbone.spm.conv3.1.bias",
      "backbone.spm.conv4.1.weight",
      "backbone.spm.conv4.1.bias",
      "backbone.spm.fc1.bias",
      "backbone.spm.fc2.bias",
      "backbone.spm.fc3.bias",
      "backbone.spm.fc4.bias",
      "backbone.interactions.0.injector.gamma",
      "backbone.interactions.0.injector.query_norm.weight",
      "backbone.interactions.0.injector.query_norm.bias",
      "backbone.interactions.0.injector.feat_norm.weight",
      "backbone.interactions.0.injector.feat_norm.bias",
      "backbone.interactions.0.injector.attn.sampling_offsets.bias",
      "backbone.interactions.0.injector.attn.attention_weights.bias",
      "backbone.interactions.0.injector.attn.value_proj.bias",
      "backbone.interactions.0.injector.attn.output_proj.bias",
      "backbone.interactions.0.extractor.query_norm.weight",
      "backbone.interactions.0.extractor.query_norm.bias",
      "backbone.interactions.0.extractor.feat_norm.weight",
      "backbone.interactions.0.extractor.feat_norm.bias",
      "backbone.interactions.0.extractor.attn.sampling_offsets.bias",
      "backbone.interactions.0.extractor.attn.attention_weights.bias",
      "backbone.interactions.0.extractor.attn.value_proj.bias",
      "backbone.interactions.0.extractor.attn.output_proj.bias",
      "backbone.interactions.0.extractor.ffn.fc1.bias",
      "backbone.interactions.0.extractor.ffn.dwconv.dwconv.bias",
      "backbone.interactions.0.extractor.ffn.fc2.bias",
      "backbone.interactions.0.extractor.ffn_norm.weight",
      "backbone.interactions.0.extractor.ffn_norm.bias",
      "backbone.interactions.1.injector.gamma",
      "backbone.interactions.1.injector.query_norm.weight",
      "backbone.interactions.1.injector.query_norm.bias",
      "backbone.interactions.1.injector.feat_norm.weight",
      "backbone.interactions.1.injector.feat_norm.bias",
      "backbone.interactions.1.injector.attn.sampling_offsets.bias",
      "backbone.interactions.1.injector.attn.attention_weights.bias",
      "backbone.interactions.1.injector.attn.value_proj.bias",
      "backbone.interactions.1.injector.attn.output_proj.bias",
      "backbone.interactions.1.extractor.query_norm.weight",
      "backbone.interactions.1.extractor.query_norm.bias",
      "backbone.interactions.1.extractor.feat_norm.weight",
      "backbone.interactions.1.extractor.feat_norm.bias",
      "backbone.interactions.1.extractor.attn.sampling_offsets.bias",
      "backbone.interactions.1.extractor.attn.attention_weights.bias",
      "backbone.interactions.1.extractor.attn.value_proj.bias",
      "backbone.interactions.1.extractor.attn.output_proj.bias",
      "backbone.interactions.1.extractor.ffn.fc1.bias",
      "backbone.interactions.1.extractor.ffn.dwconv.dwconv.bias",
      "backbone.interactions.1.extractor.ffn.fc2.bias",
      "backbone.interactions.1.extractor.ffn_norm.weight",
      "backbone.interactions.1.extractor.ffn_norm.bias",
      "backbone.interactions.2.injector.gamma",
      "backbone.interactions.2.injector.query_norm.weight",
      "backbone.interactions.2.injector.query_norm.bias",
      "backbone.interactions.2.injector.feat_norm.weight",
      "backbone.interactions.2.injector.feat_norm.bias",
      "backbone.interactions.2.injector.attn.sampling_offsets.bias",
      "backbone.interactions.2.injector.attn.attention_weights.bias",
      "backbone.interactions.2.injector.attn.value_proj.bias",
      "backbone.interactions.2.injector.attn.output_proj.bias",
      "backbone.interactions.2.extractor.query_norm.weight",
      "backbone.interactions.2.extractor.query_norm.bias",
      "backbone.interactions.2.extractor.feat_norm.weight",
      "backbone.interactions.2.extractor.feat_norm.bias",
      "backbone.interactions.2.extractor.attn.sampling_offsets.bias",
      "backbone.interactions.2.extractor.attn.attention_weights.bias",
      "backbone.interactions.2.extractor.attn.value_proj.bias",
      "backbone.interactions.2.extractor.attn.output_proj.bias",
      "backbone.interactions.2.extractor.ffn.fc1.bias",
      "backbone.interactions.2.extractor.ffn.dwconv.dwconv.bias",
      "backbone.interactions.2.extractor.ffn.fc2.bias",
      "backbone.interactions.2.extractor.ffn_norm.weight",
      "backbone.interactions.2.extractor.ffn_norm.bias",
      "backbone.interactions.3.injector.gamma",
      "backbone.interactions.3.injector.query_norm.weight",
      "backbone.interactions.3.injector.query_norm.bias",
      "backbone.interactions.3.injector.feat_norm.weight",
      "backbone.interactions.3.injector.feat_norm.bias",
      "backbone.interactions.3.injector.attn.sampling_offsets.bias",
      "backbone.interactions.3.injector.attn.attention_weights.bias",
      "backbone.interactions.3.injector.attn.value_proj.bias",
      "backbone.interactions.3.injector.attn.output_proj.bias",
      "backbone.interactions.3.extractor.query_norm.weight",
      "backbone.interactions.3.extractor.query_norm.bias",
      "backbone.interactions.3.extractor.feat_norm.weight",
      "backbone.interactions.3.extractor.feat_norm.bias",
      "backbone.interactions.3.extractor.attn.sampling_offsets.bias",
      "backbone.interactions.3.extractor.attn.attention_weights.bias",
      "backbone.interactions.3.extractor.attn.value_proj.bias",
      "backbone.interactions.3.extractor.attn.output_proj.bias",
      "backbone.interactions.3.extractor.ffn.fc1.bias",
      "backbone.interactions.3.extractor.ffn.dwconv.dwconv.bias",
      "backbone.interactions.3.extractor.ffn.fc2.bias",
      "backbone.interactions.3.extractor.ffn_norm.weight",
      "backbone.interactions.3.extractor.ffn_norm.bias",
      "backbone.interactions.3.extra_extractors.0.query_norm.weight",
      "backbone.interactions.3.extra_extractors.0.query_norm.bias",
      "backbone.interactions.3.extra_extractors.0.feat_norm.weight",
      "backbone.interactions.3.extra_extractors.0.feat_norm.bias",
      "backbone.interactions.3.extra_extractors.0.attn.sampling_offsets.bias",
      "backbone.interactions.3.extra_extractors.0.attn.attention_weights.bias",
      "backbone.interactions.3.extra_extractors.0.attn.value_proj.bias",
      "backbone.interactions.3.extra_extractors.0.attn.output_proj.bias",
      "backbone.interactions.3.extra_extractors.0.ffn.fc1.bias",
      "backbone.interactions.3.extra_extractors.0.ffn.dwconv.dwconv.bias",
      "backbone.interactions.3.extra_extractors.0.ffn.fc2.bias",
      "backbone.interactions.3.extra_extractors.0.ffn_norm.weight",
      "backbone.interactions.3.extra_extractors.0.ffn_norm.bias",
      "backbone.interactions.3.extra_extractors.1.query_norm.weight",
      "backbone.interactions.3.extra_extractors.1.query_norm.bias",
      "backbone.interactions.3.extra_extractors.1.feat_norm.weight",
      "backbone.interactions.3.extra_extractors.1.feat_norm.bias",
      "backbone.interactions.3.extra_extractors.1.attn.sampling_offsets.bias",
      "backbone.interactions.3.extra_extractors.1.attn.attention_weights.bias",
      "backbone.interactions.3.extra_extractors.1.attn.value_proj.bias",
      "backbone.interactions.3.extra_extractors.1.attn.output_proj.bias",
      "backbone.interactions.3.extra_extractors.1.ffn.fc1.bias",
      "backbone.interactions.3.extra_extractors.1.ffn.dwconv.dwconv.bias",
      "backbone.interactions.3.extra_extractors.1.ffn.fc2.bias",
      "backbone.interactions.3.extra_extractors.1.ffn_norm.weight",
      "backbone.interactions.3.extra_extractors.1.ffn_norm.bias",
      "backbone.up.bias",
      "backbone.norm1.weight",
      "backbone.norm1.bias",
      "backbone.norm2.weight",
      "backbone.norm2.bias",
      "backbone.norm3.weight",
      "backbone.norm3.bias",
      "backbone.norm4.weight",
      "backbone.norm4.bias",
      "decode_head.pixel_decoder.input_convs.0.conv.bias",
      "decode_head.pixel_decoder.input_convs.0.gn.weight",
      "decode_head.pixel_decoder.input_convs.0.gn.bias",
      "decode_head.pixel_decoder.input_convs.1.conv.bias",
      "decode_head.pixel_decoder.input_convs.1.gn.weight",
      "decode_head.pixel_decoder.input_convs.1.gn.bias",
      "decode_head.pixel_decoder.input_convs.2.conv.bias",
      "decode_head.pixel_decoder.input_convs.2.gn.weight",
      "decode_head.pixel_decoder.input_convs.2.gn.bias",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.0.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.0.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.0.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.0.norms.1.bias",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.1.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.1.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.1.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.1.norms.1.bias",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.2.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.2.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.2.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.2.norms.1.bias",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.3.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.3.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.3.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.3.norms.1.bias",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.4.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.4.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.4.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.4.norms.1.bias",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.5.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.5.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.5.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.5.norms.1.bias",
      "decode_head.pixel_decoder.lateral_convs.0.gn.weight",
      "decode_head.pixel_decoder.lateral_convs.0.gn.bias",
      "decode_head.pixel_decoder.output_convs.0.gn.weight",
      "decode_head.pixel_decoder.output_convs.0.gn.bias",
      "decode_head.pixel_decoder.mask_feature.bias",
      "decode_head.transformer_decoder.layers.0.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.0.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.0.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.0.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.0.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.0.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.0.norms.0.weight",
      "decode_head.transformer_decoder.layers.0.norms.0.bias",
      "decode_head.transformer_decoder.layers.0.norms.1.weight",
      "decode_head.transformer_decoder.layers.0.norms.1.bias",
      "decode_head.transformer_decoder.layers.0.norms.2.weight",
      "decode_head.transformer_decoder.layers.0.norms.2.bias",
      "decode_head.transformer_decoder.layers.1.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.1.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.1.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.1.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.1.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.1.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.1.norms.0.weight",
      "decode_head.transformer_decoder.layers.1.norms.0.bias",
      "decode_head.transformer_decoder.layers.1.norms.1.weight",
      "decode_head.transformer_decoder.layers.1.norms.1.bias",
      "decode_head.transformer_decoder.layers.1.norms.2.weight",
      "decode_head.transformer_decoder.layers.1.norms.2.bias",
      "decode_head.transformer_decoder.layers.2.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.2.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.2.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.2.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.2.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.2.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.2.norms.0.weight",
      "decode_head.transformer_decoder.layers.2.norms.0.bias",
      "decode_head.transformer_decoder.layers.2.norms.1.weight",
      "decode_head.transformer_decoder.layers.2.norms.1.bias",
      "decode_head.transformer_decoder.layers.2.norms.2.weight",
      "decode_head.transformer_decoder.layers.2.norms.2.bias",
      "decode_head.transformer_decoder.layers.3.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.3.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.3.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.3.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.3.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.3.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.3.norms.0.weight",
      "decode_head.transformer_decoder.layers.3.norms.0.bias",
      "decode_head.transformer_decoder.layers.3.norms.1.weight",
      "decode_head.transformer_decoder.layers.3.norms.1.bias",
      "decode_head.transformer_decoder.layers.3.norms.2.weight",
      "decode_head.transformer_decoder.layers.3.norms.2.bias",
      "decode_head.transformer_decoder.layers.4.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.4.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.4.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.4.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.4.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.4.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.4.norms.0.weight",
      "decode_head.transformer_decoder.layers.4.norms.0.bias",
      "decode_head.transformer_decoder.layers.4.norms.1.weight",
      "decode_head.transformer_decoder.layers.4.norms.1.bias",
      "decode_head.transformer_decoder.layers.4.norms.2.weight",
      "decode_head.transformer_decoder.layers.4.norms.2.bias",
      "decode_head.transformer_decoder.layers.5.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.5.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.5.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.5.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.5.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.5.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.5.norms.0.weight",
      "decode_head.transformer_decoder.layers.5.norms.0.bias",
      "decode_head.transformer_decoder.layers.5.norms.1.weight",
      "decode_head.transformer_decoder.layers.5.norms.1.bias",
      "decode_head.transformer_decoder.layers.5.norms.2.weight",
      "decode_head.transformer_decoder.layers.5.norms.2.bias",
      "decode_head.transformer_decoder.layers.6.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.6.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.6.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.6.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.6.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.6.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.6.norms.0.weight",
      "decode_head.transformer_decoder.layers.6.norms.0.bias",
      "decode_head.transformer_decoder.layers.6.norms.1.weight",
      "decode_head.transformer_decoder.layers.6.norms.1.bias",
      "decode_head.transformer_decoder.layers.6.norms.2.weight",
      "decode_head.transformer_decoder.layers.6.norms.2.bias",
      "decode_head.transformer_decoder.layers.7.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.7.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.7.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.7.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.7.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.7.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.7.norms.0.weight",
      "decode_head.transformer_decoder.layers.7.norms.0.bias",
      "decode_head.transformer_decoder.layers.7.norms.1.weight",
      "decode_head.transformer_decoder.layers.7.norms.1.bias",
      "decode_head.transformer_decoder.layers.7.norms.2.weight",
      "decode_head.transformer_decoder.layers.7.norms.2.bias",
      "decode_head.transformer_decoder.layers.8.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.8.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.8.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.8.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.8.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.8.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.8.norms.0.weight",
      "decode_head.transformer_decoder.layers.8.norms.0.bias",
      "decode_head.transformer_decoder.layers.8.norms.1.weight",
      "decode_head.transformer_decoder.layers.8.norms.1.bias",
      "decode_head.transformer_decoder.layers.8.norms.2.weight",
      "decode_head.transformer_decoder.layers.8.norms.2.bias",
      "decode_head.transformer_decoder.post_norm.weight",
      "decode_head.transformer_decoder.post_norm.bias"
    ],
    "lr_scale": 1.0,
    "lr": 2e-05,
    "weight_decay": 0.0
  }
}2023-02-21 23:36:04,466 - mmseg - INFO - Loaded 357 images
2023-02-21 23:36:04,467 - mmseg - INFO - load checkpoint from local path: work_dirs/my_city/latest.pth
2023-02-21 23:36:13,402 - mmseg - INFO - Start running, host: root@autodl-container-cd46119efa-5e1dcbf2, work_dir: /root/autodl-tmp/ViT-Adapter/segmentation/work_dirs/my_city
2023-02-21 23:36:13,402 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-02-21 23:36:13,403 - mmseg - INFO - workflow: [('train', 1)], max: 80000 iters
2023-02-21 23:36:13,403 - mmseg - INFO - Checkpoints will be saved to /root/autodl-tmp/ViT-Adapter/segmentation/work_dirs/my_city by HardDiskBackend.

/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:3657: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/root/miniconda3/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
2023-02-21 23:38:37,051 - mmseg - INFO - Iter [50/80000]	lr: 4.688e-08, eta: 2 days, 15:45:02, time: 2.871, data_time: 0.036, memory: 31493, decode.loss_cls: 0.4826, decode.loss_mask: 0.8142, decode.loss_dice: 1.2933, decode.d0.loss_cls: 5.3601, decode.d0.loss_mask: 0.8388, decode.d0.loss_dice: 1.4431, decode.d1.loss_cls: 0.4747, decode.d1.loss_mask: 0.8347, decode.d1.loss_dice: 1.3591, decode.d2.loss_cls: 0.4674, decode.d2.loss_mask: 0.8105, decode.d2.loss_dice: 1.3487, decode.d3.loss_cls: 0.5088, decode.d3.loss_mask: 0.7869, decode.d3.loss_dice: 1.2775, decode.d4.loss_cls: 0.4945, decode.d4.loss_mask: 0.7953, decode.d4.loss_dice: 1.2876, decode.d5.loss_cls: 0.4854, decode.d5.loss_mask: 0.8136, decode.d5.loss_dice: 1.3024, decode.d6.loss_cls: 0.4906, decode.d6.loss_mask: 0.7846, decode.d6.loss_dice: 1.2909, decode.d7.loss_cls: 0.4823, decode.d7.loss_mask: 0.7864, decode.d7.loss_dice: 1.2992, decode.d8.loss_cls: 0.4924, decode.d8.loss_mask: 0.7838, decode.d8.loss_dice: 1.2957, loss: 30.9849
2023-02-21 23:40:55,292 - mmseg - INFO - Iter [100/80000]	lr: 9.465e-08, eta: 2 days, 14:32:13, time: 2.765, data_time: 0.025, memory: 31493, decode.loss_cls: 0.5845, decode.loss_mask: 0.7883, decode.loss_dice: 1.3546, decode.d0.loss_cls: 5.3802, decode.d0.loss_mask: 0.8060, decode.d0.loss_dice: 1.5126, decode.d1.loss_cls: 0.6255, decode.d1.loss_mask: 0.8017, decode.d1.loss_dice: 1.4286, decode.d2.loss_cls: 0.5682, decode.d2.loss_mask: 0.7814, decode.d2.loss_dice: 1.3936, decode.d3.loss_cls: 0.6142, decode.d3.loss_mask: 0.7811, decode.d3.loss_dice: 1.3630, decode.d4.loss_cls: 0.5938, decode.d4.loss_mask: 0.7844, decode.d4.loss_dice: 1.3629, decode.d5.loss_cls: 0.6171, decode.d5.loss_mask: 0.7850, decode.d5.loss_dice: 1.3472, decode.d6.loss_cls: 0.6120, decode.d6.loss_mask: 0.7794, decode.d6.loss_dice: 1.3357, decode.d7.loss_cls: 0.5772, decode.d7.loss_mask: 0.7849, decode.d7.loss_dice: 1.3700, decode.d8.loss_cls: 0.5749, decode.d8.loss_mask: 0.7845, decode.d8.loss_dice: 1.3721, loss: 32.4645
2023-02-21 23:43:13,057 - mmseg - INFO - Iter [150/80000]	lr: 1.424e-07, eta: 2 days, 14:02:11, time: 2.755, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4588, decode.loss_mask: 0.7067, decode.loss_dice: 1.2651, decode.d0.loss_cls: 5.3730, decode.d0.loss_mask: 0.7610, decode.d0.loss_dice: 1.4183, decode.d1.loss_cls: 0.5030, decode.d1.loss_mask: 0.7390, decode.d1.loss_dice: 1.3510, decode.d2.loss_cls: 0.4601, decode.d2.loss_mask: 0.7269, decode.d2.loss_dice: 1.3133, decode.d3.loss_cls: 0.4883, decode.d3.loss_mask: 0.7076, decode.d3.loss_dice: 1.2731, decode.d4.loss_cls: 0.4786, decode.d4.loss_mask: 0.7101, decode.d4.loss_dice: 1.2689, decode.d5.loss_cls: 0.4448, decode.d5.loss_mask: 0.7099, decode.d5.loss_dice: 1.2746, decode.d6.loss_cls: 0.4571, decode.d6.loss_mask: 0.7067, decode.d6.loss_dice: 1.2612, decode.d7.loss_cls: 0.4530, decode.d7.loss_mask: 0.6989, decode.d7.loss_dice: 1.2796, decode.d8.loss_cls: 0.4530, decode.d8.loss_mask: 0.7022, decode.d8.loss_dice: 1.2629, loss: 29.7064
2023-02-21 23:45:30,771 - mmseg - INFO - Iter [200/80000]	lr: 1.900e-07, eta: 2 days, 13:45:40, time: 2.754, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4886, decode.loss_mask: 0.6887, decode.loss_dice: 1.2714, decode.d0.loss_cls: 5.3276, decode.d0.loss_mask: 0.7159, decode.d0.loss_dice: 1.4132, decode.d1.loss_cls: 0.5205, decode.d1.loss_mask: 0.7054, decode.d1.loss_dice: 1.3129, decode.d2.loss_cls: 0.5244, decode.d2.loss_mask: 0.7163, decode.d2.loss_dice: 1.3075, decode.d3.loss_cls: 0.5280, decode.d3.loss_mask: 0.6957, decode.d3.loss_dice: 1.2759, decode.d4.loss_cls: 0.5655, decode.d4.loss_mask: 0.6793, decode.d4.loss_dice: 1.2716, decode.d5.loss_cls: 0.5137, decode.d5.loss_mask: 0.6877, decode.d5.loss_dice: 1.2559, decode.d6.loss_cls: 0.5143, decode.d6.loss_mask: 0.6894, decode.d6.loss_dice: 1.2759, decode.d7.loss_cls: 0.5195, decode.d7.loss_mask: 0.6838, decode.d7.loss_dice: 1.2700, decode.d8.loss_cls: 0.5106, decode.d8.loss_mask: 0.6848, decode.d8.loss_dice: 1.2596, loss: 29.8735
2023-02-21 23:47:48,526 - mmseg - INFO - Iter [250/80000]	lr: 2.376e-07, eta: 2 days, 13:35:03, time: 2.755, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4967, decode.loss_mask: 0.7211, decode.loss_dice: 1.2324, decode.d0.loss_cls: 5.3186, decode.d0.loss_mask: 0.7442, decode.d0.loss_dice: 1.3456, decode.d1.loss_cls: 0.5247, decode.d1.loss_mask: 0.7546, decode.d1.loss_dice: 1.2984, decode.d2.loss_cls: 0.5364, decode.d2.loss_mask: 0.7199, decode.d2.loss_dice: 1.2399, decode.d3.loss_cls: 0.5326, decode.d3.loss_mask: 0.7130, decode.d3.loss_dice: 1.2312, decode.d4.loss_cls: 0.5011, decode.d4.loss_mask: 0.7112, decode.d4.loss_dice: 1.2264, decode.d5.loss_cls: 0.5149, decode.d5.loss_mask: 0.7117, decode.d5.loss_dice: 1.2206, decode.d6.loss_cls: 0.5242, decode.d6.loss_mask: 0.7163, decode.d6.loss_dice: 1.2158, decode.d7.loss_cls: 0.4949, decode.d7.loss_mask: 0.7249, decode.d7.loss_dice: 1.2505, decode.d8.loss_cls: 0.4731, decode.d8.loss_mask: 0.7222, decode.d8.loss_dice: 1.2525, loss: 29.6695
2023-02-21 23:50:06,116 - mmseg - INFO - Iter [300/80000]	lr: 2.851e-07, eta: 2 days, 13:26:30, time: 2.752, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4580, decode.loss_mask: 0.7392, decode.loss_dice: 1.2232, decode.d0.loss_cls: 5.3401, decode.d0.loss_mask: 0.7542, decode.d0.loss_dice: 1.3523, decode.d1.loss_cls: 0.4280, decode.d1.loss_mask: 0.7585, decode.d1.loss_dice: 1.3168, decode.d2.loss_cls: 0.4365, decode.d2.loss_mask: 0.7493, decode.d2.loss_dice: 1.2741, decode.d3.loss_cls: 0.5002, decode.d3.loss_mask: 0.7449, decode.d3.loss_dice: 1.2245, decode.d4.loss_cls: 0.4763, decode.d4.loss_mask: 0.7497, decode.d4.loss_dice: 1.2285, decode.d5.loss_cls: 0.4890, decode.d5.loss_mask: 0.7352, decode.d5.loss_dice: 1.2131, decode.d6.loss_cls: 0.4854, decode.d6.loss_mask: 0.7389, decode.d6.loss_dice: 1.2064, decode.d7.loss_cls: 0.4794, decode.d7.loss_mask: 0.7391, decode.d7.loss_dice: 1.2321, decode.d8.loss_cls: 0.4851, decode.d8.loss_mask: 0.7350, decode.d8.loss_dice: 1.2333, loss: 29.5263
2023-02-21 23:52:23,826 - mmseg - INFO - Iter [350/80000]	lr: 3.326e-07, eta: 2 days, 13:20:11, time: 2.754, data_time: 0.026, memory: 31493, decode.loss_cls: 0.5073, decode.loss_mask: 0.7570, decode.loss_dice: 1.3387, decode.d0.loss_cls: 5.3263, decode.d0.loss_mask: 0.7938, decode.d0.loss_dice: 1.4565, decode.d1.loss_cls: 0.4885, decode.d1.loss_mask: 0.7910, decode.d1.loss_dice: 1.4356, decode.d2.loss_cls: 0.4732, decode.d2.loss_mask: 0.7708, decode.d2.loss_dice: 1.3868, decode.d3.loss_cls: 0.5324, decode.d3.loss_mask: 0.7634, decode.d3.loss_dice: 1.3481, decode.d4.loss_cls: 0.5066, decode.d4.loss_mask: 0.7638, decode.d4.loss_dice: 1.3593, decode.d5.loss_cls: 0.4818, decode.d5.loss_mask: 0.7673, decode.d5.loss_dice: 1.3541, decode.d6.loss_cls: 0.4901, decode.d6.loss_mask: 0.7659, decode.d6.loss_dice: 1.3442, decode.d7.loss_cls: 0.4832, decode.d7.loss_mask: 0.7672, decode.d7.loss_dice: 1.3573, decode.d8.loss_cls: 0.4988, decode.d8.loss_mask: 0.7608, decode.d8.loss_dice: 1.3373, loss: 31.2071
2023-02-21 23:54:41,709 - mmseg - INFO - Iter [400/80000]	lr: 3.800e-07, eta: 2 days, 13:15:27, time: 2.758, data_time: 0.025, memory: 31493, decode.loss_cls: 0.5231, decode.loss_mask: 0.7422, decode.loss_dice: 1.2652, decode.d0.loss_cls: 5.2523, decode.d0.loss_mask: 0.7723, decode.d0.loss_dice: 1.4003, decode.d1.loss_cls: 0.5136, decode.d1.loss_mask: 0.7605, decode.d1.loss_dice: 1.3735, decode.d2.loss_cls: 0.5491, decode.d2.loss_mask: 0.7563, decode.d2.loss_dice: 1.3106, decode.d3.loss_cls: 0.5279, decode.d3.loss_mask: 0.7508, decode.d3.loss_dice: 1.2894, decode.d4.loss_cls: 0.5528, decode.d4.loss_mask: 0.7430, decode.d4.loss_dice: 1.2828, decode.d5.loss_cls: 0.4887, decode.d5.loss_mask: 0.7523, decode.d5.loss_dice: 1.2696, decode.d6.loss_cls: 0.5143, decode.d6.loss_mask: 0.7503, decode.d6.loss_dice: 1.2705, decode.d7.loss_cls: 0.5377, decode.d7.loss_mask: 0.7400, decode.d7.loss_dice: 1.2703, decode.d8.loss_cls: 0.5233, decode.d8.loss_mask: 0.7408, decode.d8.loss_dice: 1.2661, loss: 30.4893
2023-02-21 23:56:59,314 - mmseg - INFO - Iter [450/80000]	lr: 4.274e-07, eta: 2 days, 13:10:26, time: 2.752, data_time: 0.024, memory: 31493, decode.loss_cls: 0.5065, decode.loss_mask: 0.7826, decode.loss_dice: 1.1849, decode.d0.loss_cls: 5.3043, decode.d0.loss_mask: 0.8009, decode.d0.loss_dice: 1.3155, decode.d1.loss_cls: 0.5067, decode.d1.loss_mask: 0.8171, decode.d1.loss_dice: 1.3069, decode.d2.loss_cls: 0.5116, decode.d2.loss_mask: 0.7612, decode.d2.loss_dice: 1.2164, decode.d3.loss_cls: 0.5074, decode.d3.loss_mask: 0.7800, decode.d3.loss_dice: 1.1985, decode.d4.loss_cls: 0.5161, decode.d4.loss_mask: 0.7900, decode.d4.loss_dice: 1.1996, decode.d5.loss_cls: 0.4987, decode.d5.loss_mask: 0.7879, decode.d5.loss_dice: 1.2030, decode.d6.loss_cls: 0.5011, decode.d6.loss_mask: 0.7789, decode.d6.loss_dice: 1.1865, decode.d7.loss_cls: 0.5066, decode.d7.loss_mask: 0.7903, decode.d7.loss_dice: 1.1937, decode.d8.loss_cls: 0.5081, decode.d8.loss_mask: 0.7793, decode.d8.loss_dice: 1.1893, loss: 29.9297
2023-02-21 23:59:17,180 - mmseg - INFO - Iter [500/80000]	lr: 4.747e-07, eta: 2 days, 13:06:39, time: 2.757, data_time: 0.025, memory: 31493, decode.loss_cls: 0.5322, decode.loss_mask: 0.7530, decode.loss_dice: 1.2549, decode.d0.loss_cls: 5.2660, decode.d0.loss_mask: 0.7807, decode.d0.loss_dice: 1.4110, decode.d1.loss_cls: 0.6026, decode.d1.loss_mask: 0.7636, decode.d1.loss_dice: 1.3360, decode.d2.loss_cls: 0.5803, decode.d2.loss_mask: 0.7533, decode.d2.loss_dice: 1.3021, decode.d3.loss_cls: 0.5950, decode.d3.loss_mask: 0.7432, decode.d3.loss_dice: 1.2763, decode.d4.loss_cls: 0.5816, decode.d4.loss_mask: 0.7599, decode.d4.loss_dice: 1.2748, decode.d5.loss_cls: 0.5752, decode.d5.loss_mask: 0.7579, decode.d5.loss_dice: 1.3027, decode.d6.loss_cls: 0.5442, decode.d6.loss_mask: 0.7407, decode.d6.loss_dice: 1.2791, decode.d7.loss_cls: 0.5549, decode.d7.loss_mask: 0.7414, decode.d7.loss_dice: 1.2624, decode.d8.loss_cls: 0.5492, decode.d8.loss_mask: 0.7531, decode.d8.loss_dice: 1.2584, loss: 30.8856
2023-02-22 00:01:36,226 - mmseg - INFO - Iter [550/80000]	lr: 5.219e-07, eta: 2 days, 13:05:59, time: 2.781, data_time: 0.028, memory: 31493, decode.loss_cls: 0.4891, decode.loss_mask: 0.7436, decode.loss_dice: 1.2224, decode.d0.loss_cls: 5.2639, decode.d0.loss_mask: 0.7993, decode.d0.loss_dice: 1.3452, decode.d1.loss_cls: 0.5144, decode.d1.loss_mask: 0.7913, decode.d1.loss_dice: 1.3451, decode.d2.loss_cls: 0.5076, decode.d2.loss_mask: 0.7667, decode.d2.loss_dice: 1.2597, decode.d3.loss_cls: 0.4969, decode.d3.loss_mask: 0.7605, decode.d3.loss_dice: 1.2362, decode.d4.loss_cls: 0.5017, decode.d4.loss_mask: 0.7578, decode.d4.loss_dice: 1.2312, decode.d5.loss_cls: 0.4791, decode.d5.loss_mask: 0.7611, decode.d5.loss_dice: 1.2448, decode.d6.loss_cls: 0.4847, decode.d6.loss_mask: 0.7523, decode.d6.loss_dice: 1.2361, decode.d7.loss_cls: 0.4922, decode.d7.loss_mask: 0.7461, decode.d7.loss_dice: 1.2373, decode.d8.loss_cls: 0.4966, decode.d8.loss_mask: 0.7401, decode.d8.loss_dice: 1.2302, loss: 29.9335
2023-02-22 00:03:54,957 - mmseg - INFO - Iter [600/80000]	lr: 5.691e-07, eta: 2 days, 13:04:21, time: 2.775, data_time: 0.027, memory: 31493, decode.loss_cls: 0.5205, decode.loss_mask: 0.7053, decode.loss_dice: 1.2546, decode.d0.loss_cls: 5.2168, decode.d0.loss_mask: 0.7342, decode.d0.loss_dice: 1.3852, decode.d1.loss_cls: 0.5165, decode.d1.loss_mask: 0.7235, decode.d1.loss_dice: 1.3355, decode.d2.loss_cls: 0.5163, decode.d2.loss_mask: 0.7232, decode.d2.loss_dice: 1.2809, decode.d3.loss_cls: 0.5315, decode.d3.loss_mask: 0.7049, decode.d3.loss_dice: 1.2636, decode.d4.loss_cls: 0.5357, decode.d4.loss_mask: 0.7069, decode.d4.loss_dice: 1.2523, decode.d5.loss_cls: 0.5489, decode.d5.loss_mask: 0.7063, decode.d5.loss_dice: 1.2557, decode.d6.loss_cls: 0.5126, decode.d6.loss_mask: 0.7004, decode.d6.loss_dice: 1.2529, decode.d7.loss_cls: 0.5130, decode.d7.loss_mask: 0.6939, decode.d7.loss_dice: 1.2680, decode.d8.loss_cls: 0.5142, decode.d8.loss_mask: 0.6990, decode.d8.loss_dice: 1.2595, loss: 29.8317
2023-02-22 00:06:13,269 - mmseg - INFO - Iter [650/80000]	lr: 6.162e-07, eta: 2 days, 13:01:45, time: 2.766, data_time: 0.026, memory: 31493, decode.loss_cls: 0.5230, decode.loss_mask: 0.7623, decode.loss_dice: 1.3009, decode.d0.loss_cls: 5.2368, decode.d0.loss_mask: 0.7928, decode.d0.loss_dice: 1.4358, decode.d1.loss_cls: 0.5451, decode.d1.loss_mask: 0.7720, decode.d1.loss_dice: 1.3992, decode.d2.loss_cls: 0.5292, decode.d2.loss_mask: 0.7632, decode.d2.loss_dice: 1.3273, decode.d3.loss_cls: 0.5257, decode.d3.loss_mask: 0.7743, decode.d3.loss_dice: 1.2975, decode.d4.loss_cls: 0.5160, decode.d4.loss_mask: 0.7793, decode.d4.loss_dice: 1.3077, decode.d5.loss_cls: 0.5236, decode.d5.loss_mask: 0.7573, decode.d5.loss_dice: 1.2876, decode.d6.loss_cls: 0.5326, decode.d6.loss_mask: 0.7521, decode.d6.loss_dice: 1.2882, decode.d7.loss_cls: 0.5309, decode.d7.loss_mask: 0.7524, decode.d7.loss_dice: 1.2924, decode.d8.loss_cls: 0.5183, decode.d8.loss_mask: 0.7525, decode.d8.loss_dice: 1.3115, loss: 30.8877
2023-02-22 00:08:31,961 - mmseg - INFO - Iter [700/80000]	lr: 6.632e-07, eta: 2 days, 12:59:55, time: 2.774, data_time: 0.026, memory: 31493, decode.loss_cls: 0.5060, decode.loss_mask: 0.7194, decode.loss_dice: 1.2367, decode.d0.loss_cls: 5.1873, decode.d0.loss_mask: 0.7590, decode.d0.loss_dice: 1.3563, decode.d1.loss_cls: 0.5187, decode.d1.loss_mask: 0.7554, decode.d1.loss_dice: 1.3325, decode.d2.loss_cls: 0.5109, decode.d2.loss_mask: 0.7426, decode.d2.loss_dice: 1.2774, decode.d3.loss_cls: 0.5373, decode.d3.loss_mask: 0.7348, decode.d3.loss_dice: 1.2587, decode.d4.loss_cls: 0.5510, decode.d4.loss_mask: 0.7309, decode.d4.loss_dice: 1.2681, decode.d5.loss_cls: 0.5264, decode.d5.loss_mask: 0.7325, decode.d5.loss_dice: 1.2550, decode.d6.loss_cls: 0.5382, decode.d6.loss_mask: 0.7155, decode.d6.loss_dice: 1.2535, decode.d7.loss_cls: 0.5361, decode.d7.loss_mask: 0.7101, decode.d7.loss_dice: 1.2408, decode.d8.loss_cls: 0.5096, decode.d8.loss_mask: 0.7242, decode.d8.loss_dice: 1.2433, loss: 29.9681
2023-02-22 00:11:10,232 - mmseg - INFO - Iter [750/80000]	lr: 7.102e-07, eta: 2 days, 13:32:30, time: 3.165, data_time: 0.085, memory: 31493, decode.loss_cls: 0.4505, decode.loss_mask: 0.7720, decode.loss_dice: 1.2826, decode.d0.loss_cls: 5.1766, decode.d0.loss_mask: 0.7840, decode.d0.loss_dice: 1.4111, decode.d1.loss_cls: 0.4791, decode.d1.loss_mask: 0.7931, decode.d1.loss_dice: 1.3581, decode.d2.loss_cls: 0.4583, decode.d2.loss_mask: 0.7719, decode.d2.loss_dice: 1.3137, decode.d3.loss_cls: 0.4726, decode.d3.loss_mask: 0.7675, decode.d3.loss_dice: 1.2953, decode.d4.loss_cls: 0.4571, decode.d4.loss_mask: 0.7692, decode.d4.loss_dice: 1.3005, decode.d5.loss_cls: 0.4599, decode.d5.loss_mask: 0.7620, decode.d5.loss_dice: 1.2983, decode.d6.loss_cls: 0.4475, decode.d6.loss_mask: 0.7746, decode.d6.loss_dice: 1.3077, decode.d7.loss_cls: 0.4498, decode.d7.loss_mask: 0.7722, decode.d7.loss_dice: 1.3280, decode.d8.loss_cls: 0.4377, decode.d8.loss_mask: 0.7719, decode.d8.loss_dice: 1.3049, loss: 30.2277
2023-02-22 00:13:44,966 - mmseg - INFO - Iter [800/80000]	lr: 7.572e-07, eta: 2 days, 13:54:50, time: 3.095, data_time: 0.026, memory: 31493, decode.loss_cls: 0.5609, decode.loss_mask: 0.7644, decode.loss_dice: 1.2626, decode.d0.loss_cls: 5.0999, decode.d0.loss_mask: 0.7900, decode.d0.loss_dice: 1.3953, decode.d1.loss_cls: 0.5453, decode.d1.loss_mask: 0.7834, decode.d1.loss_dice: 1.3630, decode.d2.loss_cls: 0.5589, decode.d2.loss_mask: 0.7527, decode.d2.loss_dice: 1.3208, decode.d3.loss_cls: 0.5531, decode.d3.loss_mask: 0.7508, decode.d3.loss_dice: 1.2758, decode.d4.loss_cls: 0.5763, decode.d4.loss_mask: 0.7553, decode.d4.loss_dice: 1.2648, decode.d5.loss_cls: 0.5410, decode.d5.loss_mask: 0.7654, decode.d5.loss_dice: 1.2842, decode.d6.loss_cls: 0.5907, decode.d6.loss_mask: 0.7686, decode.d6.loss_dice: 1.2552, decode.d7.loss_cls: 0.5713, decode.d7.loss_mask: 0.7556, decode.d7.loss_dice: 1.2866, decode.d8.loss_cls: 0.5380, decode.d8.loss_mask: 0.7712, decode.d8.loss_dice: 1.2783, loss: 30.7794
2023-02-22 00:16:33,836 - mmseg - INFO - Iter [850/80000]	lr: 8.040e-07, eta: 2 days, 14:36:11, time: 3.377, data_time: 0.033, memory: 31493, decode.loss_cls: 0.5133, decode.loss_mask: 0.7238, decode.loss_dice: 1.2769, decode.d0.loss_cls: 5.0769, decode.d0.loss_mask: 0.7586, decode.d0.loss_dice: 1.4142, decode.d1.loss_cls: 0.4938, decode.d1.loss_mask: 0.7686, decode.d1.loss_dice: 1.3995, decode.d2.loss_cls: 0.5076, decode.d2.loss_mask: 0.7333, decode.d2.loss_dice: 1.3323, decode.d3.loss_cls: 0.5230, decode.d3.loss_mask: 0.7334, decode.d3.loss_dice: 1.2971, decode.d4.loss_cls: 0.5061, decode.d4.loss_mask: 0.7333, decode.d4.loss_dice: 1.2942, decode.d5.loss_cls: 0.5118, decode.d5.loss_mask: 0.7251, decode.d5.loss_dice: 1.2771, decode.d6.loss_cls: 0.4977, decode.d6.loss_mask: 0.7242, decode.d6.loss_dice: 1.2601, decode.d7.loss_cls: 0.5117, decode.d7.loss_mask: 0.7303, decode.d7.loss_dice: 1.2689, decode.d8.loss_cls: 0.4935, decode.d8.loss_mask: 0.7243, decode.d8.loss_dice: 1.2890, loss: 30.0997
2023-02-22 00:18:55,489 - mmseg - INFO - Iter [900/80000]	lr: 8.509e-07, eta: 2 days, 14:32:46, time: 2.833, data_time: 0.026, memory: 31493, decode.loss_cls: 0.5047, decode.loss_mask: 0.7282, decode.loss_dice: 1.3120, decode.d0.loss_cls: 5.0848, decode.d0.loss_mask: 0.7311, decode.d0.loss_dice: 1.4276, decode.d1.loss_cls: 0.5178, decode.d1.loss_mask: 0.7365, decode.d1.loss_dice: 1.3968, decode.d2.loss_cls: 0.5569, decode.d2.loss_mask: 0.7252, decode.d2.loss_dice: 1.3299, decode.d3.loss_cls: 0.5504, decode.d3.loss_mask: 0.7311, decode.d3.loss_dice: 1.3075, decode.d4.loss_cls: 0.5241, decode.d4.loss_mask: 0.7392, decode.d4.loss_dice: 1.3191, decode.d5.loss_cls: 0.5331, decode.d5.loss_mask: 0.7283, decode.d5.loss_dice: 1.3264, decode.d6.loss_cls: 0.5307, decode.d6.loss_mask: 0.7263, decode.d6.loss_dice: 1.3041, decode.d7.loss_cls: 0.5328, decode.d7.loss_mask: 0.7288, decode.d7.loss_dice: 1.3081, decode.d8.loss_cls: 0.5498, decode.d8.loss_mask: 0.7321, decode.d8.loss_dice: 1.2959, loss: 30.5191
2023-02-22 00:21:13,473 - mmseg - INFO - Iter [950/80000]	lr: 8.976e-07, eta: 2 days, 14:24:22, time: 2.760, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4719, decode.loss_mask: 0.6779, decode.loss_dice: 1.3004, decode.d0.loss_cls: 4.9786, decode.d0.loss_mask: 0.6708, decode.d0.loss_dice: 1.4078, decode.d1.loss_cls: 0.4889, decode.d1.loss_mask: 0.6821, decode.d1.loss_dice: 1.3746, decode.d2.loss_cls: 0.4645, decode.d2.loss_mask: 0.6790, decode.d2.loss_dice: 1.3197, decode.d3.loss_cls: 0.4907, decode.d3.loss_mask: 0.6844, decode.d3.loss_dice: 1.3085, decode.d4.loss_cls: 0.4715, decode.d4.loss_mask: 0.6850, decode.d4.loss_dice: 1.3059, decode.d5.loss_cls: 0.4770, decode.d5.loss_mask: 0.6760, decode.d5.loss_dice: 1.3053, decode.d6.loss_cls: 0.4887, decode.d6.loss_mask: 0.6805, decode.d6.loss_dice: 1.2797, decode.d7.loss_cls: 0.4798, decode.d7.loss_mask: 0.6838, decode.d7.loss_dice: 1.3030, decode.d8.loss_cls: 0.4643, decode.d8.loss_mask: 0.6878, decode.d8.loss_dice: 1.2967, loss: 29.2845
2023-02-22 00:23:30,900 - mmseg - INFO - Saving checkpoint at 1000 iterations
2023-02-22 00:23:54,512 - mmseg - INFO - Exp name: my_city.py
2023-02-22 00:23:54,512 - mmseg - INFO - Iter [1000/80000]	lr: 9.443e-07, eta: 2 days, 14:46:56, time: 3.221, data_time: 0.025, memory: 31493, decode.loss_cls: 0.5624, decode.loss_mask: 0.7311, decode.loss_dice: 1.3016, decode.d0.loss_cls: 5.0088, decode.d0.loss_mask: 0.7656, decode.d0.loss_dice: 1.4364, decode.d1.loss_cls: 0.5605, decode.d1.loss_mask: 0.7386, decode.d1.loss_dice: 1.4235, decode.d2.loss_cls: 0.5528, decode.d2.loss_mask: 0.7397, decode.d2.loss_dice: 1.3576, decode.d3.loss_cls: 0.5565, decode.d3.loss_mask: 0.7230, decode.d3.loss_dice: 1.3125, decode.d4.loss_cls: 0.5790, decode.d4.loss_mask: 0.7220, decode.d4.loss_dice: 1.3159, decode.d5.loss_cls: 0.5783, decode.d5.loss_mask: 0.7246, decode.d5.loss_dice: 1.3018, decode.d6.loss_cls: 0.5568, decode.d6.loss_mask: 0.7401, decode.d6.loss_dice: 1.3143, decode.d7.loss_cls: 0.5480, decode.d7.loss_mask: 0.7408, decode.d7.loss_dice: 1.3264, decode.d8.loss_cls: 0.5791, decode.d8.loss_mask: 0.7443, decode.d8.loss_dice: 1.2912, loss: 30.8331
2023-02-22 00:26:12,010 - mmseg - INFO - Iter [1050/80000]	lr: 9.909e-07, eta: 2 days, 14:37:35, time: 2.750, data_time: 0.023, memory: 31493, decode.loss_cls: 0.4264, decode.loss_mask: 0.8285, decode.loss_dice: 1.2319, decode.d0.loss_cls: 4.9303, decode.d0.loss_mask: 0.8274, decode.d0.loss_dice: 1.3543, decode.d1.loss_cls: 0.4232, decode.d1.loss_mask: 0.8498, decode.d1.loss_dice: 1.2965, decode.d2.loss_cls: 0.4155, decode.d2.loss_mask: 0.8263, decode.d2.loss_dice: 1.2495, decode.d3.loss_cls: 0.4293, decode.d3.loss_mask: 0.8307, decode.d3.loss_dice: 1.2341, decode.d4.loss_cls: 0.4441, decode.d4.loss_mask: 0.8327, decode.d4.loss_dice: 1.2218, decode.d5.loss_cls: 0.4336, decode.d5.loss_mask: 0.8171, decode.d5.loss_dice: 1.2213, decode.d6.loss_cls: 0.4344, decode.d6.loss_mask: 0.8123, decode.d6.loss_dice: 1.2092, decode.d7.loss_cls: 0.4525, decode.d7.loss_mask: 0.8145, decode.d7.loss_dice: 1.2203, decode.d8.loss_cls: 0.4407, decode.d8.loss_mask: 0.8244, decode.d8.loss_dice: 1.2204, loss: 29.5532
2023-02-22 00:28:28,964 - mmseg - INFO - Iter [1100/80000]	lr: 1.038e-06, eta: 2 days, 14:28:14, time: 2.739, data_time: 0.023, memory: 31493, decode.loss_cls: 0.4634, decode.loss_mask: 0.7602, decode.loss_dice: 1.2536, decode.d0.loss_cls: 4.9384, decode.d0.loss_mask: 0.7925, decode.d0.loss_dice: 1.3211, decode.d1.loss_cls: 0.4810, decode.d1.loss_mask: 0.7827, decode.d1.loss_dice: 1.3209, decode.d2.loss_cls: 0.4445, decode.d2.loss_mask: 0.7766, decode.d2.loss_dice: 1.3009, decode.d3.loss_cls: 0.4877, decode.d3.loss_mask: 0.7594, decode.d3.loss_dice: 1.2493, decode.d4.loss_cls: 0.4801, decode.d4.loss_mask: 0.7737, decode.d4.loss_dice: 1.2440, decode.d5.loss_cls: 0.4711, decode.d5.loss_mask: 0.7823, decode.d5.loss_dice: 1.2604, decode.d6.loss_cls: 0.4834, decode.d6.loss_mask: 0.7774, decode.d6.loss_dice: 1.2466, decode.d7.loss_cls: 0.4733, decode.d7.loss_mask: 0.7761, decode.d7.loss_dice: 1.2609, decode.d8.loss_cls: 0.4887, decode.d8.loss_mask: 0.7675, decode.d8.loss_dice: 1.2458, loss: 29.6636
2023-02-22 00:30:45,957 - mmseg - INFO - Iter [1150/80000]	lr: 1.084e-06, eta: 2 days, 14:19:33, time: 2.740, data_time: 0.023, memory: 31493, decode.loss_cls: 0.5522, decode.loss_mask: 0.7345, decode.loss_dice: 1.3355, decode.d0.loss_cls: 4.9133, decode.d0.loss_mask: 0.7608, decode.d0.loss_dice: 1.4950, decode.d1.loss_cls: 0.6466, decode.d1.loss_mask: 0.7391, decode.d1.loss_dice: 1.4170, decode.d2.loss_cls: 0.5548, decode.d2.loss_mask: 0.7436, decode.d2.loss_dice: 1.3789, decode.d3.loss_cls: 0.5917, decode.d3.loss_mask: 0.7453, decode.d3.loss_dice: 1.3498, decode.d4.loss_cls: 0.5312, decode.d4.loss_mask: 0.7534, decode.d4.loss_dice: 1.3643, decode.d5.loss_cls: 0.5795, decode.d5.loss_mask: 0.7468, decode.d5.loss_dice: 1.3493, decode.d6.loss_cls: 0.5689, decode.d6.loss_mask: 0.7429, decode.d6.loss_dice: 1.3475, decode.d7.loss_cls: 0.5649, decode.d7.loss_mask: 0.7468, decode.d7.loss_dice: 1.3484, decode.d8.loss_cls: 0.5534, decode.d8.loss_mask: 0.7408, decode.d8.loss_dice: 1.3484, loss: 31.2449
2023-02-22 00:33:03,007 - mmseg - INFO - Iter [1200/80000]	lr: 1.130e-06, eta: 2 days, 14:11:27, time: 2.741, data_time: 0.022, memory: 31493, decode.loss_cls: 0.4716, decode.loss_mask: 0.7844, decode.loss_dice: 1.3098, decode.d0.loss_cls: 4.8795, decode.d0.loss_mask: 0.7912, decode.d0.loss_dice: 1.3924, decode.d1.loss_cls: 0.4636, decode.d1.loss_mask: 0.8026, decode.d1.loss_dice: 1.3824, decode.d2.loss_cls: 0.4814, decode.d2.loss_mask: 0.7944, decode.d2.loss_dice: 1.3429, decode.d3.loss_cls: 0.4911, decode.d3.loss_mask: 0.7826, decode.d3.loss_dice: 1.3183, decode.d4.loss_cls: 0.4875, decode.d4.loss_mask: 0.7838, decode.d4.loss_dice: 1.3076, decode.d5.loss_cls: 0.4762, decode.d5.loss_mask: 0.7779, decode.d5.loss_dice: 1.3128, decode.d6.loss_cls: 0.4773, decode.d6.loss_mask: 0.7715, decode.d6.loss_dice: 1.3198, decode.d7.loss_cls: 0.4537, decode.d7.loss_mask: 0.7818, decode.d7.loss_dice: 1.3181, decode.d8.loss_cls: 0.4579, decode.d8.loss_mask: 0.7832, decode.d8.loss_dice: 1.3245, loss: 30.3219
2023-02-22 00:35:19,962 - mmseg - INFO - Iter [1250/80000]	lr: 1.177e-06, eta: 2 days, 14:03:43, time: 2.739, data_time: 0.022, memory: 31493, decode.loss_cls: 0.4700, decode.loss_mask: 0.7027, decode.loss_dice: 1.1869, decode.d0.loss_cls: 4.8187, decode.d0.loss_mask: 0.7305, decode.d0.loss_dice: 1.3403, decode.d1.loss_cls: 0.4384, decode.d1.loss_mask: 0.7322, decode.d1.loss_dice: 1.3015, decode.d2.loss_cls: 0.4671, decode.d2.loss_mask: 0.7129, decode.d2.loss_dice: 1.2426, decode.d3.loss_cls: 0.4899, decode.d3.loss_mask: 0.7062, decode.d3.loss_dice: 1.1974, decode.d4.loss_cls: 0.4699, decode.d4.loss_mask: 0.7107, decode.d4.loss_dice: 1.2050, decode.d5.loss_cls: 0.4625, decode.d5.loss_mask: 0.6997, decode.d5.loss_dice: 1.2214, decode.d6.loss_cls: 0.4680, decode.d6.loss_mask: 0.7068, decode.d6.loss_dice: 1.2044, decode.d7.loss_cls: 0.4809, decode.d7.loss_mask: 0.7022, decode.d7.loss_dice: 1.1912, decode.d8.loss_cls: 0.4826, decode.d8.loss_mask: 0.6929, decode.d8.loss_dice: 1.1892, loss: 28.4249
2023-02-22 00:37:36,883 - mmseg - INFO - Iter [1300/80000]	lr: 1.223e-06, eta: 2 days, 13:56:23, time: 2.738, data_time: 0.023, memory: 31493, decode.loss_cls: 0.4552, decode.loss_mask: 0.6960, decode.loss_dice: 1.2503, decode.d0.loss_cls: 4.8108, decode.d0.loss_mask: 0.7335, decode.d0.loss_dice: 1.3716, decode.d1.loss_cls: 0.4060, decode.d1.loss_mask: 0.7304, decode.d1.loss_dice: 1.3280, decode.d2.loss_cls: 0.4485, decode.d2.loss_mask: 0.7131, decode.d2.loss_dice: 1.2748, decode.d3.loss_cls: 0.4607, decode.d3.loss_mask: 0.7031, decode.d3.loss_dice: 1.2489, decode.d4.loss_cls: 0.4716, decode.d4.loss_mask: 0.6992, decode.d4.loss_dice: 1.2548, decode.d5.loss_cls: 0.4579, decode.d5.loss_mask: 0.6976, decode.d5.loss_dice: 1.2343, decode.d6.loss_cls: 0.4845, decode.d6.loss_mask: 0.6937, decode.d6.loss_dice: 1.2189, decode.d7.loss_cls: 0.4873, decode.d7.loss_mask: 0.7011, decode.d7.loss_dice: 1.2281, decode.d8.loss_cls: 0.4453, decode.d8.loss_mask: 0.7002, decode.d8.loss_dice: 1.2477, loss: 28.6532
2023-02-22 00:39:53,808 - mmseg - INFO - Iter [1350/80000]	lr: 1.269e-06, eta: 2 days, 13:49:25, time: 2.738, data_time: 0.022, memory: 31493, decode.loss_cls: 0.6251, decode.loss_mask: 0.8135, decode.loss_dice: 1.3178, decode.d0.loss_cls: 4.7726, decode.d0.loss_mask: 0.8389, decode.d0.loss_dice: 1.4183, decode.d1.loss_cls: 0.5930, decode.d1.loss_mask: 0.8291, decode.d1.loss_dice: 1.4156, decode.d2.loss_cls: 0.5958, decode.d2.loss_mask: 0.8099, decode.d2.loss_dice: 1.3681, decode.d3.loss_cls: 0.6498, decode.d3.loss_mask: 0.8034, decode.d3.loss_dice: 1.3378, decode.d4.loss_cls: 0.6700, decode.d4.loss_mask: 0.8066, decode.d4.loss_dice: 1.3448, decode.d5.loss_cls: 0.6500, decode.d5.loss_mask: 0.8125, decode.d5.loss_dice: 1.3390, decode.d6.loss_cls: 0.6530, decode.d6.loss_mask: 0.8182, decode.d6.loss_dice: 1.3424, decode.d7.loss_cls: 0.6295, decode.d7.loss_mask: 0.8202, decode.d7.loss_dice: 1.3538, decode.d8.loss_cls: 0.6301, decode.d8.loss_mask: 0.8289, decode.d8.loss_dice: 1.3317, loss: 32.2193
2023-02-22 00:42:10,965 - mmseg - INFO - Iter [1400/80000]	lr: 1.316e-06, eta: 2 days, 13:43:00, time: 2.743, data_time: 0.023, memory: 31493, decode.loss_cls: 0.5060, decode.loss_mask: 0.8342, decode.loss_dice: 1.3343, decode.d0.loss_cls: 4.7189, decode.d0.loss_mask: 0.8732, decode.d0.loss_dice: 1.4637, decode.d1.loss_cls: 0.4929, decode.d1.loss_mask: 0.8557, decode.d1.loss_dice: 1.4189, decode.d2.loss_cls: 0.5138, decode.d2.loss_mask: 0.8412, decode.d2.loss_dice: 1.3588, decode.d3.loss_cls: 0.5745, decode.d3.loss_mask: 0.8451, decode.d3.loss_dice: 1.3154, decode.d4.loss_cls: 0.5205, decode.d4.loss_mask: 0.8432, decode.d4.loss_dice: 1.3490, decode.d5.loss_cls: 0.5228, decode.d5.loss_mask: 0.8309, decode.d5.loss_dice: 1.3197, decode.d6.loss_cls: 0.5179, decode.d6.loss_mask: 0.8369, decode.d6.loss_dice: 1.3254, decode.d7.loss_cls: 0.5370, decode.d7.loss_mask: 0.8353, decode.d7.loss_dice: 1.3453, decode.d8.loss_cls: 0.5112, decode.d8.loss_mask: 0.8279, decode.d8.loss_dice: 1.3339, loss: 31.4034
2023-02-22 00:44:30,547 - mmseg - INFO - Iter [1450/80000]	lr: 1.362e-06, eta: 2 days, 13:39:03, time: 2.792, data_time: 0.072, memory: 31493, decode.loss_cls: 0.5069, decode.loss_mask: 0.7531, decode.loss_dice: 1.2494, decode.d0.loss_cls: 4.6767, decode.d0.loss_mask: 0.7666, decode.d0.loss_dice: 1.3499, decode.d1.loss_cls: 0.4937, decode.d1.loss_mask: 0.7683, decode.d1.loss_dice: 1.3270, decode.d2.loss_cls: 0.5028, decode.d2.loss_mask: 0.7646, decode.d2.loss_dice: 1.2896, decode.d3.loss_cls: 0.5020, decode.d3.loss_mask: 0.7513, decode.d3.loss_dice: 1.2575, decode.d4.loss_cls: 0.5390, decode.d4.loss_mask: 0.7497, decode.d4.loss_dice: 1.2504, decode.d5.loss_cls: 0.5193, decode.d5.loss_mask: 0.7473, decode.d5.loss_dice: 1.2444, decode.d6.loss_cls: 0.5036, decode.d6.loss_mask: 0.7681, decode.d6.loss_dice: 1.2555, decode.d7.loss_cls: 0.5083, decode.d7.loss_mask: 0.7667, decode.d7.loss_dice: 1.2659, decode.d8.loss_cls: 0.5098, decode.d8.loss_mask: 0.7573, decode.d8.loss_dice: 1.2419, loss: 29.5868
2023-02-22 00:46:48,136 - mmseg - INFO - Iter [1500/80000]	lr: 1.408e-06, eta: 2 days, 13:33:29, time: 2.752, data_time: 0.023, memory: 31493, decode.loss_cls: 0.5488, decode.loss_mask: 0.7925, decode.loss_dice: 1.2513, decode.d0.loss_cls: 4.6429, decode.d0.loss_mask: 0.8009, decode.d0.loss_dice: 1.3862, decode.d1.loss_cls: 0.5530, decode.d1.loss_mask: 0.7977, decode.d1.loss_dice: 1.3331, decode.d2.loss_cls: 0.5801, decode.d2.loss_mask: 0.7801, decode.d2.loss_dice: 1.2775, decode.d3.loss_cls: 0.5860, decode.d3.loss_mask: 0.7685, decode.d3.loss_dice: 1.2530, decode.d4.loss_cls: 0.5646, decode.d4.loss_mask: 0.7716, decode.d4.loss_dice: 1.2667, decode.d5.loss_cls: 0.5746, decode.d5.loss_mask: 0.7753, decode.d5.loss_dice: 1.2643, decode.d6.loss_cls: 0.5795, decode.d6.loss_mask: 0.7808, decode.d6.loss_dice: 1.2732, decode.d7.loss_cls: 0.5816, decode.d7.loss_mask: 0.7745, decode.d7.loss_dice: 1.2713, decode.d8.loss_cls: 0.5547, decode.d8.loss_mask: 0.7829, decode.d8.loss_dice: 1.2743, loss: 30.4416
2023-02-22 00:49:05,054 - mmseg - INFO - Iter [1550/80000]	lr: 1.408e-06, eta: 2 days, 13:27:34, time: 2.738, data_time: 0.022, memory: 31493, decode.loss_cls: 0.4571, decode.loss_mask: 0.7782, decode.loss_dice: 1.3243, decode.d0.loss_cls: 4.6172, decode.d0.loss_mask: 0.7563, decode.d0.loss_dice: 1.3644, decode.d1.loss_cls: 0.4830, decode.d1.loss_mask: 0.7724, decode.d1.loss_dice: 1.3902, decode.d2.loss_cls: 0.5127, decode.d2.loss_mask: 0.7672, decode.d2.loss_dice: 1.3398, decode.d3.loss_cls: 0.4837, decode.d3.loss_mask: 0.7606, decode.d3.loss_dice: 1.3180, decode.d4.loss_cls: 0.4658, decode.d4.loss_mask: 0.7718, decode.d4.loss_dice: 1.3102, decode.d5.loss_cls: 0.4892, decode.d5.loss_mask: 0.7657, decode.d5.loss_dice: 1.3203, decode.d6.loss_cls: 0.4733, decode.d6.loss_mask: 0.7815, decode.d6.loss_dice: 1.3058, decode.d7.loss_cls: 0.4961, decode.d7.loss_mask: 0.7756, decode.d7.loss_dice: 1.3234, decode.d8.loss_cls: 0.4631, decode.d8.loss_mask: 0.7818, decode.d8.loss_dice: 1.3151, loss: 29.9637
2023-02-22 00:51:22,282 - mmseg - INFO - Iter [1600/80000]	lr: 1.407e-06, eta: 2 days, 13:22:07, time: 2.745, data_time: 0.023, memory: 31493, decode.loss_cls: 0.4891, decode.loss_mask: 0.8231, decode.loss_dice: 1.2980, decode.d0.loss_cls: 4.5304, decode.d0.loss_mask: 0.8123, decode.d0.loss_dice: 1.3660, decode.d1.loss_cls: 0.4684, decode.d1.loss_mask: 0.8175, decode.d1.loss_dice: 1.3715, decode.d2.loss_cls: 0.4657, decode.d2.loss_mask: 0.8113, decode.d2.loss_dice: 1.3269, decode.d3.loss_cls: 0.4736, decode.d3.loss_mask: 0.8092, decode.d3.loss_dice: 1.3024, decode.d4.loss_cls: 0.4669, decode.d4.loss_mask: 0.8179, decode.d4.loss_dice: 1.3087, decode.d5.loss_cls: 0.4788, decode.d5.loss_mask: 0.8266, decode.d5.loss_dice: 1.3134, decode.d6.loss_cls: 0.4510, decode.d6.loss_mask: 0.8180, decode.d6.loss_dice: 1.3129, decode.d7.loss_cls: 0.4701, decode.d7.loss_mask: 0.8101, decode.d7.loss_dice: 1.3207, decode.d8.loss_cls: 0.4790, decode.d8.loss_mask: 0.8174, decode.d8.loss_dice: 1.3284, loss: 30.1852
2023-02-22 00:53:39,345 - mmseg - INFO - Iter [1650/80000]	lr: 1.406e-06, eta: 2 days, 13:16:44, time: 2.741, data_time: 0.022, memory: 31493, decode.loss_cls: 0.5224, decode.loss_mask: 0.7532, decode.loss_dice: 1.2684, decode.d0.loss_cls: 4.5180, decode.d0.loss_mask: 0.7693, decode.d0.loss_dice: 1.3528, decode.d1.loss_cls: 0.5107, decode.d1.loss_mask: 0.7806, decode.d1.loss_dice: 1.3549, decode.d2.loss_cls: 0.5069, decode.d2.loss_mask: 0.7523, decode.d2.loss_dice: 1.2960, decode.d3.loss_cls: 0.5297, decode.d3.loss_mask: 0.7385, decode.d3.loss_dice: 1.2589, decode.d4.loss_cls: 0.5312, decode.d4.loss_mask: 0.7400, decode.d4.loss_dice: 1.2638, decode.d5.loss_cls: 0.5274, decode.d5.loss_mask: 0.7496, decode.d5.loss_dice: 1.2761, decode.d6.loss_cls: 0.5156, decode.d6.loss_mask: 0.7475, decode.d6.loss_dice: 1.2615, decode.d7.loss_cls: 0.5210, decode.d7.loss_mask: 0.7581, decode.d7.loss_dice: 1.2693, decode.d8.loss_cls: 0.4939, decode.d8.loss_mask: 0.7506, decode.d8.loss_dice: 1.2695, loss: 29.5873
2023-02-22 00:55:56,300 - mmseg - INFO - Iter [1700/80000]	lr: 1.405e-06, eta: 2 days, 13:11:27, time: 2.739, data_time: 0.023, memory: 31493, decode.loss_cls: 0.4663, decode.loss_mask: 0.7963, decode.loss_dice: 1.2806, decode.d0.loss_cls: 4.5460, decode.d0.loss_mask: 0.7756, decode.d0.loss_dice: 1.3584, decode.d1.loss_cls: 0.5111, decode.d1.loss_mask: 0.8107, decode.d1.loss_dice: 1.3451, decode.d2.loss_cls: 0.5019, decode.d2.loss_mask: 0.8056, decode.d2.loss_dice: 1.2923, decode.d3.loss_cls: 0.4774, decode.d3.loss_mask: 0.7831, decode.d3.loss_dice: 1.2658, decode.d4.loss_cls: 0.4820, decode.d4.loss_mask: 0.7767, decode.d4.loss_dice: 1.2770, decode.d5.loss_cls: 0.4715, decode.d5.loss_mask: 0.7909, decode.d5.loss_dice: 1.2773, decode.d6.loss_cls: 0.4671, decode.d6.loss_mask: 0.7928, decode.d6.loss_dice: 1.2648, decode.d7.loss_cls: 0.4809, decode.d7.loss_mask: 0.7950, decode.d7.loss_dice: 1.2967, decode.d8.loss_cls: 0.4723, decode.d8.loss_mask: 0.7980, decode.d8.loss_dice: 1.2984, loss: 29.7577
2023-02-22 00:58:13,701 - mmseg - INFO - Iter [1750/80000]	lr: 1.404e-06, eta: 2 days, 13:06:40, time: 2.748, data_time: 0.023, memory: 31493, decode.loss_cls: 0.4958, decode.loss_mask: 0.7581, decode.loss_dice: 1.3621, decode.d0.loss_cls: 4.3866, decode.d0.loss_mask: 0.7956, decode.d0.loss_dice: 1.4629, decode.d1.loss_cls: 0.5144, decode.d1.loss_mask: 0.7800, decode.d1.loss_dice: 1.4119, decode.d2.loss_cls: 0.5082, decode.d2.loss_mask: 0.7640, decode.d2.loss_dice: 1.3757, decode.d3.loss_cls: 0.5119, decode.d3.loss_mask: 0.7575, decode.d3.loss_dice: 1.3465, decode.d4.loss_cls: 0.5412, decode.d4.loss_mask: 0.7534, decode.d4.loss_dice: 1.3567, decode.d5.loss_cls: 0.5048, decode.d5.loss_mask: 0.7516, decode.d5.loss_dice: 1.3426, decode.d6.loss_cls: 0.5125, decode.d6.loss_mask: 0.7605, decode.d6.loss_dice: 1.3349, decode.d7.loss_cls: 0.5131, decode.d7.loss_mask: 0.7565, decode.d7.loss_dice: 1.3512, decode.d8.loss_cls: 0.4725, decode.d8.loss_mask: 0.7641, decode.d8.loss_dice: 1.3553, loss: 30.3017
2023-02-22 01:00:30,860 - mmseg - INFO - Iter [1800/80000]	lr: 1.404e-06, eta: 2 days, 13:01:51, time: 2.743, data_time: 0.023, memory: 31493, decode.loss_cls: 0.5053, decode.loss_mask: 0.7121, decode.loss_dice: 1.1501, decode.d0.loss_cls: 4.3393, decode.d0.loss_mask: 0.7250, decode.d0.loss_dice: 1.2484, decode.d1.loss_cls: 0.5104, decode.d1.loss_mask: 0.7272, decode.d1.loss_dice: 1.2123, decode.d2.loss_cls: 0.5111, decode.d2.loss_mask: 0.7061, decode.d2.loss_dice: 1.1974, decode.d3.loss_cls: 0.5036, decode.d3.loss_mask: 0.7054, decode.d3.loss_dice: 1.1537, decode.d4.loss_cls: 0.4907, decode.d4.loss_mask: 0.7124, decode.d4.loss_dice: 1.1753, decode.d5.loss_cls: 0.4783, decode.d5.loss_mask: 0.7177, decode.d5.loss_dice: 1.1737, decode.d6.loss_cls: 0.4642, decode.d6.loss_mask: 0.7123, decode.d6.loss_dice: 1.1638, decode.d7.loss_cls: 0.4813, decode.d7.loss_mask: 0.7083, decode.d7.loss_dice: 1.1658, decode.d8.loss_cls: 0.4791, decode.d8.loss_mask: 0.7138, decode.d8.loss_dice: 1.1553, loss: 27.6993
2023-02-22 01:02:47,553 - mmseg - INFO - Iter [1850/80000]	lr: 1.403e-06, eta: 2 days, 12:56:51, time: 2.734, data_time: 0.021, memory: 31493, decode.loss_cls: 0.4898, decode.loss_mask: 0.7549, decode.loss_dice: 1.2277, decode.d0.loss_cls: 4.3485, decode.d0.loss_mask: 0.7752, decode.d0.loss_dice: 1.3336, decode.d1.loss_cls: 0.4687, decode.d1.loss_mask: 0.7749, decode.d1.loss_dice: 1.3060, decode.d2.loss_cls: 0.4759, decode.d2.loss_mask: 0.7666, decode.d2.loss_dice: 1.2467, decode.d3.loss_cls: 0.5142, decode.d3.loss_mask: 0.7480, decode.d3.loss_dice: 1.2120, decode.d4.loss_cls: 0.4911, decode.d4.loss_mask: 0.7440, decode.d4.loss_dice: 1.2400, decode.d5.loss_cls: 0.4815, decode.d5.loss_mask: 0.7485, decode.d5.loss_dice: 1.2493, decode.d6.loss_cls: 0.4890, decode.d6.loss_mask: 0.7466, decode.d6.loss_dice: 1.2280, decode.d7.loss_cls: 0.4892, decode.d7.loss_mask: 0.7570, decode.d7.loss_dice: 1.2314, decode.d8.loss_cls: 0.4617, decode.d8.loss_mask: 0.7486, decode.d8.loss_dice: 1.2396, loss: 28.7883
2023-02-22 01:05:04,792 - mmseg - INFO - Iter [1900/80000]	lr: 1.402e-06, eta: 2 days, 12:52:21, time: 2.745, data_time: 0.023, memory: 31493, decode.loss_cls: 0.4902, decode.loss_mask: 0.7336, decode.loss_dice: 1.1964, decode.d0.loss_cls: 4.2787, decode.d0.loss_mask: 0.7438, decode.d0.loss_dice: 1.2816, decode.d1.loss_cls: 0.4794, decode.d1.loss_mask: 0.7362, decode.d1.loss_dice: 1.2508, decode.d2.loss_cls: 0.5051, decode.d2.loss_mask: 0.7235, decode.d2.loss_dice: 1.2041, decode.d3.loss_cls: 0.5107, decode.d3.loss_mask: 0.7135, decode.d3.loss_dice: 1.1692, decode.d4.loss_cls: 0.5277, decode.d4.loss_mask: 0.7266, decode.d4.loss_dice: 1.1840, decode.d5.loss_cls: 0.5335, decode.d5.loss_mask: 0.7168, decode.d5.loss_dice: 1.1788, decode.d6.loss_cls: 0.4785, decode.d6.loss_mask: 0.7373, decode.d6.loss_dice: 1.1900, decode.d7.loss_cls: 0.4838, decode.d7.loss_mask: 0.7383, decode.d7.loss_dice: 1.1756, decode.d8.loss_cls: 0.4863, decode.d8.loss_mask: 0.7323, decode.d8.loss_dice: 1.1885, loss: 28.0948
2023-02-22 01:07:22,159 - mmseg - INFO - Iter [1950/80000]	lr: 1.401e-06, eta: 2 days, 12:48:03, time: 2.747, data_time: 0.023, memory: 31493, decode.loss_cls: 0.5278, decode.loss_mask: 0.6992, decode.loss_dice: 1.1883, decode.d0.loss_cls: 4.2809, decode.d0.loss_mask: 0.7151, decode.d0.loss_dice: 1.3228, decode.d1.loss_cls: 0.5038, decode.d1.loss_mask: 0.7193, decode.d1.loss_dice: 1.2794, decode.d2.loss_cls: 0.4940, decode.d2.loss_mask: 0.7074, decode.d2.loss_dice: 1.2422, decode.d3.loss_cls: 0.5090, decode.d3.loss_mask: 0.7058, decode.d3.loss_dice: 1.2168, decode.d4.loss_cls: 0.5245, decode.d4.loss_mask: 0.6958, decode.d4.loss_dice: 1.2060, decode.d5.loss_cls: 0.5144, decode.d5.loss_mask: 0.6971, decode.d5.loss_dice: 1.2080, decode.d6.loss_cls: 0.5173, decode.d6.loss_mask: 0.6919, decode.d6.loss_dice: 1.2127, decode.d7.loss_cls: 0.5193, decode.d7.loss_mask: 0.6824, decode.d7.loss_dice: 1.1985, decode.d8.loss_cls: 0.5055, decode.d8.loss_mask: 0.6912, decode.d8.loss_dice: 1.2002, loss: 28.1765
2023-02-22 01:09:40,430 - mmseg - INFO - Saving checkpoint at 2000 iterations
2023-02-22 01:10:05,203 - mmseg - INFO - Exp name: my_city.py
2023-02-22 01:10:05,203 - mmseg - INFO - Iter [2000/80000]	lr: 1.400e-06, eta: 2 days, 13:00:33, time: 3.261, data_time: 0.026, memory: 31493, decode.loss_cls: 0.4428, decode.loss_mask: 0.7816, decode.loss_dice: 1.3201, decode.d0.loss_cls: 4.2240, decode.d0.loss_mask: 0.8274, decode.d0.loss_dice: 1.3732, decode.d1.loss_cls: 0.3817, decode.d1.loss_mask: 0.8022, decode.d1.loss_dice: 1.3921, decode.d2.loss_cls: 0.3895, decode.d2.loss_mask: 0.7853, decode.d2.loss_dice: 1.3432, decode.d3.loss_cls: 0.4168, decode.d3.loss_mask: 0.7782, decode.d3.loss_dice: 1.3379, decode.d4.loss_cls: 0.4251, decode.d4.loss_mask: 0.7783, decode.d4.loss_dice: 1.3352, decode.d5.loss_cls: 0.4085, decode.d5.loss_mask: 0.7864, decode.d5.loss_dice: 1.3476, decode.d6.loss_cls: 0.4167, decode.d6.loss_mask: 0.7921, decode.d6.loss_dice: 1.3488, decode.d7.loss_cls: 0.4222, decode.d7.loss_mask: 0.7844, decode.d7.loss_dice: 1.3293, decode.d8.loss_cls: 0.4190, decode.d8.loss_mask: 0.7744, decode.d8.loss_dice: 1.3363, loss: 29.3003
2023-02-22 01:12:22,569 - mmseg - INFO - Iter [2050/80000]	lr: 1.399e-06, eta: 2 days, 12:56:02, time: 2.747, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4427, decode.loss_mask: 0.7244, decode.loss_dice: 1.2542, decode.d0.loss_cls: 4.1731, decode.d0.loss_mask: 0.7270, decode.d0.loss_dice: 1.3609, decode.d1.loss_cls: 0.4134, decode.d1.loss_mask: 0.7332, decode.d1.loss_dice: 1.3489, decode.d2.loss_cls: 0.4416, decode.d2.loss_mask: 0.7092, decode.d2.loss_dice: 1.2972, decode.d3.loss_cls: 0.4952, decode.d3.loss_mask: 0.7015, decode.d3.loss_dice: 1.2541, decode.d4.loss_cls: 0.4801, decode.d4.loss_mask: 0.7097, decode.d4.loss_dice: 1.2633, decode.d5.loss_cls: 0.4648, decode.d5.loss_mask: 0.6928, decode.d5.loss_dice: 1.2632, decode.d6.loss_cls: 0.4666, decode.d6.loss_mask: 0.6955, decode.d6.loss_dice: 1.2586, decode.d7.loss_cls: 0.4552, decode.d7.loss_mask: 0.7072, decode.d7.loss_dice: 1.2665, decode.d8.loss_cls: 0.4274, decode.d8.loss_mask: 0.7198, decode.d8.loss_dice: 1.2653, loss: 28.2126
2023-02-22 01:14:39,824 - mmseg - INFO - Iter [2100/80000]	lr: 1.398e-06, eta: 2 days, 12:51:33, time: 2.745, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4331, decode.loss_mask: 0.7023, decode.loss_dice: 1.1455, decode.d0.loss_cls: 4.1140, decode.d0.loss_mask: 0.7414, decode.d0.loss_dice: 1.2805, decode.d1.loss_cls: 0.4609, decode.d1.loss_mask: 0.7297, decode.d1.loss_dice: 1.2021, decode.d2.loss_cls: 0.4254, decode.d2.loss_mask: 0.7171, decode.d2.loss_dice: 1.1726, decode.d3.loss_cls: 0.4159, decode.d3.loss_mask: 0.7121, decode.d3.loss_dice: 1.1379, decode.d4.loss_cls: 0.4110, decode.d4.loss_mask: 0.7030, decode.d4.loss_dice: 1.1487, decode.d5.loss_cls: 0.3950, decode.d5.loss_mask: 0.7020, decode.d5.loss_dice: 1.1213, decode.d6.loss_cls: 0.4406, decode.d6.loss_mask: 0.6905, decode.d6.loss_dice: 1.1329, decode.d7.loss_cls: 0.4058, decode.d7.loss_mask: 0.7134, decode.d7.loss_dice: 1.1507, decode.d8.loss_cls: 0.4087, decode.d8.loss_mask: 0.7023, decode.d8.loss_dice: 1.1518, loss: 26.6681
2023-02-22 01:17:00,060 - mmseg - INFO - Iter [2150/80000]	lr: 1.397e-06, eta: 2 days, 12:48:59, time: 2.805, data_time: 0.074, memory: 31493, decode.loss_cls: 0.4741, decode.loss_mask: 0.7428, decode.loss_dice: 1.2398, decode.d0.loss_cls: 4.1162, decode.d0.loss_mask: 0.7481, decode.d0.loss_dice: 1.3118, decode.d1.loss_cls: 0.4938, decode.d1.loss_mask: 0.7552, decode.d1.loss_dice: 1.2968, decode.d2.loss_cls: 0.4877, decode.d2.loss_mask: 0.7426, decode.d2.loss_dice: 1.2604, decode.d3.loss_cls: 0.5032, decode.d3.loss_mask: 0.7209, decode.d3.loss_dice: 1.2123, decode.d4.loss_cls: 0.4875, decode.d4.loss_mask: 0.7245, decode.d4.loss_dice: 1.2249, decode.d5.loss_cls: 0.4679, decode.d5.loss_mask: 0.7244, decode.d5.loss_dice: 1.2191, decode.d6.loss_cls: 0.4728, decode.d6.loss_mask: 0.7353, decode.d6.loss_dice: 1.2084, decode.d7.loss_cls: 0.4861, decode.d7.loss_mask: 0.7373, decode.d7.loss_dice: 1.2316, decode.d8.loss_cls: 0.4849, decode.d8.loss_mask: 0.7357, decode.d8.loss_dice: 1.2330, loss: 28.2793
2023-02-22 01:19:17,458 - mmseg - INFO - Iter [2200/80000]	lr: 1.396e-06, eta: 2 days, 12:44:44, time: 2.748, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4395, decode.loss_mask: 0.7080, decode.loss_dice: 1.2889, decode.d0.loss_cls: 4.0292, decode.d0.loss_mask: 0.7300, decode.d0.loss_dice: 1.3741, decode.d1.loss_cls: 0.3807, decode.d1.loss_mask: 0.7245, decode.d1.loss_dice: 1.3200, decode.d2.loss_cls: 0.3932, decode.d2.loss_mask: 0.7133, decode.d2.loss_dice: 1.2986, decode.d3.loss_cls: 0.4404, decode.d3.loss_mask: 0.7077, decode.d3.loss_dice: 1.2785, decode.d4.loss_cls: 0.3934, decode.d4.loss_mask: 0.7171, decode.d4.loss_dice: 1.2806, decode.d5.loss_cls: 0.4007, decode.d5.loss_mask: 0.7074, decode.d5.loss_dice: 1.2758, decode.d6.loss_cls: 0.4319, decode.d6.loss_mask: 0.7033, decode.d6.loss_dice: 1.2716, decode.d7.loss_cls: 0.4342, decode.d7.loss_mask: 0.7067, decode.d7.loss_dice: 1.2896, decode.d8.loss_cls: 0.4315, decode.d8.loss_mask: 0.7032, decode.d8.loss_dice: 1.2776, loss: 27.8513
2023-02-22 01:21:34,798 - mmseg - INFO - Iter [2250/80000]	lr: 1.395e-06, eta: 2 days, 12:40:33, time: 2.747, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4838, decode.loss_mask: 0.7010, decode.loss_dice: 1.2115, decode.d0.loss_cls: 4.0073, decode.d0.loss_mask: 0.7270, decode.d0.loss_dice: 1.2958, decode.d1.loss_cls: 0.4865, decode.d1.loss_mask: 0.7120, decode.d1.loss_dice: 1.2766, decode.d2.loss_cls: 0.4752, decode.d2.loss_mask: 0.7114, decode.d2.loss_dice: 1.2482, decode.d3.loss_cls: 0.4829, decode.d3.loss_mask: 0.7030, decode.d3.loss_dice: 1.2081, decode.d4.loss_cls: 0.4987, decode.d4.loss_mask: 0.6935, decode.d4.loss_dice: 1.2127, decode.d5.loss_cls: 0.4802, decode.d5.loss_mask: 0.7049, decode.d5.loss_dice: 1.2249, decode.d6.loss_cls: 0.4878, decode.d6.loss_mask: 0.7104, decode.d6.loss_dice: 1.1975, decode.d7.loss_cls: 0.4762, decode.d7.loss_mask: 0.7133, decode.d7.loss_dice: 1.2064, decode.d8.loss_cls: 0.4540, decode.d8.loss_mask: 0.7048, decode.d8.loss_dice: 1.2316, loss: 27.7274
2023-02-22 01:23:52,074 - mmseg - INFO - Iter [2300/80000]	lr: 1.395e-06, eta: 2 days, 12:36:25, time: 2.746, data_time: 0.023, memory: 31493, decode.loss_cls: 0.4470, decode.loss_mask: 0.7484, decode.loss_dice: 1.2311, decode.d0.loss_cls: 4.0213, decode.d0.loss_mask: 0.7555, decode.d0.loss_dice: 1.3165, decode.d1.loss_cls: 0.4671, decode.d1.loss_mask: 0.7576, decode.d1.loss_dice: 1.2774, decode.d2.loss_cls: 0.4562, decode.d2.loss_mask: 0.7448, decode.d2.loss_dice: 1.2673, decode.d3.loss_cls: 0.4632, decode.d3.loss_mask: 0.7431, decode.d3.loss_dice: 1.2353, decode.d4.loss_cls: 0.4579, decode.d4.loss_mask: 0.7571, decode.d4.loss_dice: 1.2438, decode.d5.loss_cls: 0.4523, decode.d5.loss_mask: 0.7489, decode.d5.loss_dice: 1.2553, decode.d6.loss_cls: 0.4579, decode.d6.loss_mask: 0.7492, decode.d6.loss_dice: 1.2254, decode.d7.loss_cls: 0.4379, decode.d7.loss_mask: 0.7444, decode.d7.loss_dice: 1.2683, decode.d8.loss_cls: 0.4488, decode.d8.loss_mask: 0.7462, decode.d8.loss_dice: 1.2560, loss: 28.1813
2023-02-22 01:26:09,610 - mmseg - INFO - Iter [2350/80000]	lr: 1.394e-06, eta: 2 days, 12:32:29, time: 2.751, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3639, decode.loss_mask: 0.7189, decode.loss_dice: 1.2689, decode.d0.loss_cls: 3.9381, decode.d0.loss_mask: 0.7551, decode.d0.loss_dice: 1.3078, decode.d1.loss_cls: 0.4032, decode.d1.loss_mask: 0.7301, decode.d1.loss_dice: 1.3172, decode.d2.loss_cls: 0.4172, decode.d2.loss_mask: 0.7290, decode.d2.loss_dice: 1.2984, decode.d3.loss_cls: 0.3939, decode.d3.loss_mask: 0.7320, decode.d3.loss_dice: 1.2568, decode.d4.loss_cls: 0.3862, decode.d4.loss_mask: 0.7293, decode.d4.loss_dice: 1.2489, decode.d5.loss_cls: 0.3744, decode.d5.loss_mask: 0.7274, decode.d5.loss_dice: 1.2702, decode.d6.loss_cls: 0.3818, decode.d6.loss_mask: 0.7289, decode.d6.loss_dice: 1.2483, decode.d7.loss_cls: 0.3720, decode.d7.loss_mask: 0.7316, decode.d7.loss_dice: 1.2627, decode.d8.loss_cls: 0.3549, decode.d8.loss_mask: 0.7218, decode.d8.loss_dice: 1.2725, loss: 27.4412
2023-02-22 01:28:27,422 - mmseg - INFO - Iter [2400/80000]	lr: 1.393e-06, eta: 2 days, 12:28:47, time: 2.756, data_time: 0.025, memory: 31493, decode.loss_cls: 0.5136, decode.loss_mask: 0.7666, decode.loss_dice: 1.2584, decode.d0.loss_cls: 3.9001, decode.d0.loss_mask: 0.8051, decode.d0.loss_dice: 1.3740, decode.d1.loss_cls: 0.5389, decode.d1.loss_mask: 0.7799, decode.d1.loss_dice: 1.3273, decode.d2.loss_cls: 0.5022, decode.d2.loss_mask: 0.7791, decode.d2.loss_dice: 1.2989, decode.d3.loss_cls: 0.5319, decode.d3.loss_mask: 0.7658, decode.d3.loss_dice: 1.2659, decode.d4.loss_cls: 0.5358, decode.d4.loss_mask: 0.7746, decode.d4.loss_dice: 1.2710, decode.d5.loss_cls: 0.5090, decode.d5.loss_mask: 0.7648, decode.d5.loss_dice: 1.2629, decode.d6.loss_cls: 0.5071, decode.d6.loss_mask: 0.7624, decode.d6.loss_dice: 1.2662, decode.d7.loss_cls: 0.4820, decode.d7.loss_mask: 0.7665, decode.d7.loss_dice: 1.2661, decode.d8.loss_cls: 0.5113, decode.d8.loss_mask: 0.7638, decode.d8.loss_dice: 1.2658, loss: 29.1168
2023-02-22 01:30:45,045 - mmseg - INFO - Iter [2450/80000]	lr: 1.392e-06, eta: 2 days, 12:25:03, time: 2.752, data_time: 0.026, memory: 31493, decode.loss_cls: 0.4696, decode.loss_mask: 0.7498, decode.loss_dice: 1.2824, decode.d0.loss_cls: 3.8228, decode.d0.loss_mask: 0.7916, decode.d0.loss_dice: 1.3585, decode.d1.loss_cls: 0.4708, decode.d1.loss_mask: 0.7526, decode.d1.loss_dice: 1.3444, decode.d2.loss_cls: 0.4258, decode.d2.loss_mask: 0.7565, decode.d2.loss_dice: 1.3229, decode.d3.loss_cls: 0.4938, decode.d3.loss_mask: 0.7535, decode.d3.loss_dice: 1.2814, decode.d4.loss_cls: 0.4822, decode.d4.loss_mask: 0.7553, decode.d4.loss_dice: 1.2795, decode.d5.loss_cls: 0.4595, decode.d5.loss_mask: 0.7443, decode.d5.loss_dice: 1.2925, decode.d6.loss_cls: 0.4523, decode.d6.loss_mask: 0.7472, decode.d6.loss_dice: 1.2828, decode.d7.loss_cls: 0.4818, decode.d7.loss_mask: 0.7439, decode.d7.loss_dice: 1.2913, decode.d8.loss_cls: 0.4698, decode.d8.loss_mask: 0.7481, decode.d8.loss_dice: 1.2699, loss: 28.5769
2023-02-22 01:33:16,966 - mmseg - INFO - Iter [2500/80000]	lr: 1.391e-06, eta: 2 days, 12:28:44, time: 3.038, data_time: 0.032, memory: 31493, decode.loss_cls: 0.5091, decode.loss_mask: 0.6495, decode.loss_dice: 1.0859, decode.d0.loss_cls: 3.8399, decode.d0.loss_mask: 0.6618, decode.d0.loss_dice: 1.2581, decode.d1.loss_cls: 0.5679, decode.d1.loss_mask: 0.6712, decode.d1.loss_dice: 1.1427, decode.d2.loss_cls: 0.5069, decode.d2.loss_mask: 0.6605, decode.d2.loss_dice: 1.1044, decode.d3.loss_cls: 0.5164, decode.d3.loss_mask: 0.6471, decode.d3.loss_dice: 1.0932, decode.d4.loss_cls: 0.5076, decode.d4.loss_mask: 0.6512, decode.d4.loss_dice: 1.0779, decode.d5.loss_cls: 0.4958, decode.d5.loss_mask: 0.6472, decode.d5.loss_dice: 1.1049, decode.d6.loss_cls: 0.4841, decode.d6.loss_mask: 0.6519, decode.d6.loss_dice: 1.0912, decode.d7.loss_cls: 0.5049, decode.d7.loss_mask: 0.6567, decode.d7.loss_dice: 1.0941, decode.d8.loss_cls: 0.4981, decode.d8.loss_mask: 0.6507, decode.d8.loss_dice: 1.0726, loss: 26.1036
2023-02-22 01:35:35,621 - mmseg - INFO - Iter [2550/80000]	lr: 1.390e-06, eta: 2 days, 12:25:29, time: 2.773, data_time: 0.024, memory: 31493, decode.loss_cls: 0.5577, decode.loss_mask: 0.6374, decode.loss_dice: 1.1219, decode.d0.loss_cls: 3.8268, decode.d0.loss_mask: 0.6734, decode.d0.loss_dice: 1.2332, decode.d1.loss_cls: 0.5412, decode.d1.loss_mask: 0.6578, decode.d1.loss_dice: 1.1915, decode.d2.loss_cls: 0.5276, decode.d2.loss_mask: 0.6435, decode.d2.loss_dice: 1.1368, decode.d3.loss_cls: 0.5143, decode.d3.loss_mask: 0.6424, decode.d3.loss_dice: 1.1253, decode.d4.loss_cls: 0.5189, decode.d4.loss_mask: 0.6444, decode.d4.loss_dice: 1.1137, decode.d5.loss_cls: 0.5336, decode.d5.loss_mask: 0.6378, decode.d5.loss_dice: 1.1253, decode.d6.loss_cls: 0.5334, decode.d6.loss_mask: 0.6431, decode.d6.loss_dice: 1.1213, decode.d7.loss_cls: 0.5340, decode.d7.loss_mask: 0.6445, decode.d7.loss_dice: 1.1316, decode.d8.loss_cls: 0.5320, decode.d8.loss_mask: 0.6396, decode.d8.loss_dice: 1.1394, loss: 26.5235
2023-02-22 01:37:54,689 - mmseg - INFO - Iter [2600/80000]	lr: 1.389e-06, eta: 2 days, 12:22:28, time: 2.781, data_time: 0.026, memory: 31493, decode.loss_cls: 0.4293, decode.loss_mask: 0.7004, decode.loss_dice: 1.2104, decode.d0.loss_cls: 3.7905, decode.d0.loss_mask: 0.7055, decode.d0.loss_dice: 1.2865, decode.d1.loss_cls: 0.4815, decode.d1.loss_mask: 0.7009, decode.d1.loss_dice: 1.2576, decode.d2.loss_cls: 0.4536, decode.d2.loss_mask: 0.7064, decode.d2.loss_dice: 1.2266, decode.d3.loss_cls: 0.4630, decode.d3.loss_mask: 0.6950, decode.d3.loss_dice: 1.2131, decode.d4.loss_cls: 0.4487, decode.d4.loss_mask: 0.6829, decode.d4.loss_dice: 1.2224, decode.d5.loss_cls: 0.4526, decode.d5.loss_mask: 0.6998, decode.d5.loss_dice: 1.2132, decode.d6.loss_cls: 0.4401, decode.d6.loss_mask: 0.6997, decode.d6.loss_dice: 1.2192, decode.d7.loss_cls: 0.4495, decode.d7.loss_mask: 0.7087, decode.d7.loss_dice: 1.2254, decode.d8.loss_cls: 0.4338, decode.d8.loss_mask: 0.7008, decode.d8.loss_dice: 1.2182, loss: 27.1351
2023-02-22 01:40:19,950 - mmseg - INFO - Iter [2650/80000]	lr: 1.388e-06, eta: 2 days, 12:22:29, time: 2.905, data_time: 0.034, memory: 31493, decode.loss_cls: 0.4489, decode.loss_mask: 0.7123, decode.loss_dice: 1.1818, decode.d0.loss_cls: 3.7843, decode.d0.loss_mask: 0.7144, decode.d0.loss_dice: 1.2667, decode.d1.loss_cls: 0.5122, decode.d1.loss_mask: 0.7006, decode.d1.loss_dice: 1.2252, decode.d2.loss_cls: 0.4569, decode.d2.loss_mask: 0.6991, decode.d2.loss_dice: 1.2008, decode.d3.loss_cls: 0.4778, decode.d3.loss_mask: 0.6880, decode.d3.loss_dice: 1.1712, decode.d4.loss_cls: 0.4861, decode.d4.loss_mask: 0.7027, decode.d4.loss_dice: 1.1777, decode.d5.loss_cls: 0.4669, decode.d5.loss_mask: 0.7024, decode.d5.loss_dice: 1.1728, decode.d6.loss_cls: 0.4673, decode.d6.loss_mask: 0.6994, decode.d6.loss_dice: 1.1871, decode.d7.loss_cls: 0.4706, decode.d7.loss_mask: 0.7087, decode.d7.loss_dice: 1.1901, decode.d8.loss_cls: 0.4650, decode.d8.loss_mask: 0.7006, decode.d8.loss_dice: 1.1964, loss: 27.0339
2023-02-22 01:42:39,390 - mmseg - INFO - Iter [2700/80000]	lr: 1.387e-06, eta: 2 days, 12:19:38, time: 2.789, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4016, decode.loss_mask: 0.7036, decode.loss_dice: 1.1918, decode.d0.loss_cls: 3.7057, decode.d0.loss_mask: 0.7157, decode.d0.loss_dice: 1.2717, decode.d1.loss_cls: 0.4074, decode.d1.loss_mask: 0.7125, decode.d1.loss_dice: 1.2445, decode.d2.loss_cls: 0.3933, decode.d2.loss_mask: 0.6981, decode.d2.loss_dice: 1.1912, decode.d3.loss_cls: 0.4066, decode.d3.loss_mask: 0.6951, decode.d3.loss_dice: 1.2025, decode.d4.loss_cls: 0.4078, decode.d4.loss_mask: 0.6978, decode.d4.loss_dice: 1.2025, decode.d5.loss_cls: 0.4247, decode.d5.loss_mask: 0.7043, decode.d5.loss_dice: 1.2100, decode.d6.loss_cls: 0.4381, decode.d6.loss_mask: 0.6994, decode.d6.loss_dice: 1.1723, decode.d7.loss_cls: 0.4146, decode.d7.loss_mask: 0.7035, decode.d7.loss_dice: 1.1856, decode.d8.loss_cls: 0.3966, decode.d8.loss_mask: 0.7160, decode.d8.loss_dice: 1.1803, loss: 26.4950
2023-02-22 01:44:56,843 - mmseg - INFO - Iter [2750/80000]	lr: 1.386e-06, eta: 2 days, 12:15:53, time: 2.749, data_time: 0.025, memory: 31493, decode.loss_cls: 0.5164, decode.loss_mask: 0.7003, decode.loss_dice: 1.2142, decode.d0.loss_cls: 3.7155, decode.d0.loss_mask: 0.7298, decode.d0.loss_dice: 1.3015, decode.d1.loss_cls: 0.4652, decode.d1.loss_mask: 0.7280, decode.d1.loss_dice: 1.2891, decode.d2.loss_cls: 0.5193, decode.d2.loss_mask: 0.7092, decode.d2.loss_dice: 1.2278, decode.d3.loss_cls: 0.5260, decode.d3.loss_mask: 0.7172, decode.d3.loss_dice: 1.2197, decode.d4.loss_cls: 0.4906, decode.d4.loss_mask: 0.7217, decode.d4.loss_dice: 1.2255, decode.d5.loss_cls: 0.5156, decode.d5.loss_mask: 0.7088, decode.d5.loss_dice: 1.2322, decode.d6.loss_cls: 0.5178, decode.d6.loss_mask: 0.6994, decode.d6.loss_dice: 1.2233, decode.d7.loss_cls: 0.4998, decode.d7.loss_mask: 0.7113, decode.d7.loss_dice: 1.2249, decode.d8.loss_cls: 0.5062, decode.d8.loss_mask: 0.6918, decode.d8.loss_dice: 1.1997, loss: 27.7478
2023-02-22 01:47:14,071 - mmseg - INFO - Iter [2800/80000]	lr: 1.386e-06, eta: 2 days, 12:12:04, time: 2.745, data_time: 0.024, memory: 31493, decode.loss_cls: 0.3842, decode.loss_mask: 0.6667, decode.loss_dice: 1.1158, decode.d0.loss_cls: 3.6083, decode.d0.loss_mask: 0.6996, decode.d0.loss_dice: 1.2266, decode.d1.loss_cls: 0.3650, decode.d1.loss_mask: 0.6869, decode.d1.loss_dice: 1.1850, decode.d2.loss_cls: 0.3795, decode.d2.loss_mask: 0.6603, decode.d2.loss_dice: 1.1449, decode.d3.loss_cls: 0.3853, decode.d3.loss_mask: 0.6707, decode.d3.loss_dice: 1.1240, decode.d4.loss_cls: 0.3883, decode.d4.loss_mask: 0.6737, decode.d4.loss_dice: 1.1266, decode.d5.loss_cls: 0.3767, decode.d5.loss_mask: 0.6752, decode.d5.loss_dice: 1.1263, decode.d6.loss_cls: 0.3859, decode.d6.loss_mask: 0.6729, decode.d6.loss_dice: 1.1127, decode.d7.loss_cls: 0.3896, decode.d7.loss_mask: 0.6721, decode.d7.loss_dice: 1.1178, decode.d8.loss_cls: 0.4210, decode.d8.loss_mask: 0.6648, decode.d8.loss_dice: 1.0970, loss: 25.2036
2023-02-22 01:49:31,227 - mmseg - INFO - Iter [2850/80000]	lr: 1.385e-06, eta: 2 days, 12:08:17, time: 2.743, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4072, decode.loss_mask: 0.7751, decode.loss_dice: 1.2201, decode.d0.loss_cls: 3.5954, decode.d0.loss_mask: 0.7800, decode.d0.loss_dice: 1.2938, decode.d1.loss_cls: 0.4210, decode.d1.loss_mask: 0.7996, decode.d1.loss_dice: 1.2617, decode.d2.loss_cls: 0.4144, decode.d2.loss_mask: 0.7789, decode.d2.loss_dice: 1.2278, decode.d3.loss_cls: 0.4343, decode.d3.loss_mask: 0.7756, decode.d3.loss_dice: 1.2065, decode.d4.loss_cls: 0.4440, decode.d4.loss_mask: 0.7741, decode.d4.loss_dice: 1.1974, decode.d5.loss_cls: 0.4420, decode.d5.loss_mask: 0.7664, decode.d5.loss_dice: 1.2072, decode.d6.loss_cls: 0.4195, decode.d6.loss_mask: 0.7789, decode.d6.loss_dice: 1.2001, decode.d7.loss_cls: 0.4125, decode.d7.loss_mask: 0.7752, decode.d7.loss_dice: 1.2021, decode.d8.loss_cls: 0.4144, decode.d8.loss_mask: 0.7826, decode.d8.loss_dice: 1.2158, loss: 27.4237
2023-02-22 01:51:51,021 - mmseg - INFO - Iter [2900/80000]	lr: 1.384e-06, eta: 2 days, 12:05:43, time: 2.796, data_time: 0.074, memory: 31493, decode.loss_cls: 0.4041, decode.loss_mask: 0.7310, decode.loss_dice: 1.1503, decode.d0.loss_cls: 3.5574, decode.d0.loss_mask: 0.7534, decode.d0.loss_dice: 1.2012, decode.d1.loss_cls: 0.3815, decode.d1.loss_mask: 0.7332, decode.d1.loss_dice: 1.1864, decode.d2.loss_cls: 0.3917, decode.d2.loss_mask: 0.7182, decode.d2.loss_dice: 1.1428, decode.d3.loss_cls: 0.3810, decode.d3.loss_mask: 0.7261, decode.d3.loss_dice: 1.1273, decode.d4.loss_cls: 0.3918, decode.d4.loss_mask: 0.7305, decode.d4.loss_dice: 1.1109, decode.d5.loss_cls: 0.3726, decode.d5.loss_mask: 0.7410, decode.d5.loss_dice: 1.1448, decode.d6.loss_cls: 0.3717, decode.d6.loss_mask: 0.7392, decode.d6.loss_dice: 1.1403, decode.d7.loss_cls: 0.3788, decode.d7.loss_mask: 0.7280, decode.d7.loss_dice: 1.1458, decode.d8.loss_cls: 0.3880, decode.d8.loss_mask: 0.7214, decode.d8.loss_dice: 1.1452, loss: 25.8357
2023-02-22 01:54:08,408 - mmseg - INFO - Iter [2950/80000]	lr: 1.383e-06, eta: 2 days, 12:02:07, time: 2.748, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3601, decode.loss_mask: 0.7213, decode.loss_dice: 1.2601, decode.d0.loss_cls: 3.5480, decode.d0.loss_mask: 0.7193, decode.d0.loss_dice: 1.3158, decode.d1.loss_cls: 0.4329, decode.d1.loss_mask: 0.7364, decode.d1.loss_dice: 1.2884, decode.d2.loss_cls: 0.4155, decode.d2.loss_mask: 0.7249, decode.d2.loss_dice: 1.2513, decode.d3.loss_cls: 0.4122, decode.d3.loss_mask: 0.7137, decode.d3.loss_dice: 1.2464, decode.d4.loss_cls: 0.3999, decode.d4.loss_mask: 0.7147, decode.d4.loss_dice: 1.2248, decode.d5.loss_cls: 0.4146, decode.d5.loss_mask: 0.7128, decode.d5.loss_dice: 1.2337, decode.d6.loss_cls: 0.3890, decode.d6.loss_mask: 0.7124, decode.d6.loss_dice: 1.2363, decode.d7.loss_cls: 0.3731, decode.d7.loss_mask: 0.7252, decode.d7.loss_dice: 1.2493, decode.d8.loss_cls: 0.3750, decode.d8.loss_mask: 0.7218, decode.d8.loss_dice: 1.2506, loss: 26.8796
2023-02-22 01:56:25,568 - mmseg - INFO - Saving checkpoint at 3000 iterations
2023-02-22 01:56:47,409 - mmseg - INFO - Exp name: my_city.py
2023-02-22 01:56:47,409 - mmseg - INFO - Iter [3000/80000]	lr: 1.382e-06, eta: 2 days, 12:07:48, time: 3.180, data_time: 0.024, memory: 31493, decode.loss_cls: 0.3970, decode.loss_mask: 0.7716, decode.loss_dice: 1.2396, decode.d0.loss_cls: 3.5177, decode.d0.loss_mask: 0.7767, decode.d0.loss_dice: 1.2915, decode.d1.loss_cls: 0.4351, decode.d1.loss_mask: 0.7580, decode.d1.loss_dice: 1.2793, decode.d2.loss_cls: 0.3936, decode.d2.loss_mask: 0.7637, decode.d2.loss_dice: 1.2772, decode.d3.loss_cls: 0.4152, decode.d3.loss_mask: 0.7467, decode.d3.loss_dice: 1.2563, decode.d4.loss_cls: 0.4284, decode.d4.loss_mask: 0.7478, decode.d4.loss_dice: 1.2357, decode.d5.loss_cls: 0.4206, decode.d5.loss_mask: 0.7460, decode.d5.loss_dice: 1.2425, decode.d6.loss_cls: 0.4220, decode.d6.loss_mask: 0.7651, decode.d6.loss_dice: 1.2139, decode.d7.loss_cls: 0.4092, decode.d7.loss_mask: 0.7593, decode.d7.loss_dice: 1.2422, decode.d8.loss_cls: 0.4070, decode.d8.loss_mask: 0.7645, decode.d8.loss_dice: 1.2402, loss: 27.3637
2023-02-22 01:59:04,803 - mmseg - INFO - Iter [3050/80000]	lr: 1.381e-06, eta: 2 days, 12:04:07, time: 2.748, data_time: 0.026, memory: 31493, decode.loss_cls: 0.4270, decode.loss_mask: 0.7076, decode.loss_dice: 1.2944, decode.d0.loss_cls: 3.4984, decode.d0.loss_mask: 0.7249, decode.d0.loss_dice: 1.3537, decode.d1.loss_cls: 0.4926, decode.d1.loss_mask: 0.7000, decode.d1.loss_dice: 1.3230, decode.d2.loss_cls: 0.4609, decode.d2.loss_mask: 0.6887, decode.d2.loss_dice: 1.2934, decode.d3.loss_cls: 0.4641, decode.d3.loss_mask: 0.6928, decode.d3.loss_dice: 1.2820, decode.d4.loss_cls: 0.4625, decode.d4.loss_mask: 0.6923, decode.d4.loss_dice: 1.2844, decode.d5.loss_cls: 0.4517, decode.d5.loss_mask: 0.6999, decode.d5.loss_dice: 1.2998, decode.d6.loss_cls: 0.4916, decode.d6.loss_mask: 0.6942, decode.d6.loss_dice: 1.2820, decode.d7.loss_cls: 0.4635, decode.d7.loss_mask: 0.7041, decode.d7.loss_dice: 1.2894, decode.d8.loss_cls: 0.4564, decode.d8.loss_mask: 0.7036, decode.d8.loss_dice: 1.2832, loss: 27.6619
2023-02-22 02:01:22,179 - mmseg - INFO - Iter [3100/80000]	lr: 1.380e-06, eta: 2 days, 12:00:29, time: 2.748, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4212, decode.loss_mask: 0.7108, decode.loss_dice: 1.2720, decode.d0.loss_cls: 3.4150, decode.d0.loss_mask: 0.7460, decode.d0.loss_dice: 1.3181, decode.d1.loss_cls: 0.4385, decode.d1.loss_mask: 0.7187, decode.d1.loss_dice: 1.3312, decode.d2.loss_cls: 0.4204, decode.d2.loss_mask: 0.7125, decode.d2.loss_dice: 1.2760, decode.d3.loss_cls: 0.4651, decode.d3.loss_mask: 0.7139, decode.d3.loss_dice: 1.2452, decode.d4.loss_cls: 0.4615, decode.d4.loss_mask: 0.7121, decode.d4.loss_dice: 1.2479, decode.d5.loss_cls: 0.4157, decode.d5.loss_mask: 0.7123, decode.d5.loss_dice: 1.2704, decode.d6.loss_cls: 0.4072, decode.d6.loss_mask: 0.7099, decode.d6.loss_dice: 1.2602, decode.d7.loss_cls: 0.4112, decode.d7.loss_mask: 0.7156, decode.d7.loss_dice: 1.2707, decode.d8.loss_cls: 0.4200, decode.d8.loss_mask: 0.7141, decode.d8.loss_dice: 1.2579, loss: 27.1911
2023-02-22 02:03:40,025 - mmseg - INFO - Iter [3150/80000]	lr: 1.379e-06, eta: 2 days, 11:57:04, time: 2.757, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4097, decode.loss_mask: 0.7700, decode.loss_dice: 1.2026, decode.d0.loss_cls: 3.3943, decode.d0.loss_mask: 0.7789, decode.d0.loss_dice: 1.2819, decode.d1.loss_cls: 0.4979, decode.d1.loss_mask: 0.7613, decode.d1.loss_dice: 1.2504, decode.d2.loss_cls: 0.4257, decode.d2.loss_mask: 0.7818, decode.d2.loss_dice: 1.2165, decode.d3.loss_cls: 0.4306, decode.d3.loss_mask: 0.7828, decode.d3.loss_dice: 1.1753, decode.d4.loss_cls: 0.4015, decode.d4.loss_mask: 0.7902, decode.d4.loss_dice: 1.1914, decode.d5.loss_cls: 0.4298, decode.d5.loss_mask: 0.7737, decode.d5.loss_dice: 1.2038, decode.d6.loss_cls: 0.4019, decode.d6.loss_mask: 0.7786, decode.d6.loss_dice: 1.1924, decode.d7.loss_cls: 0.4326, decode.d7.loss_mask: 0.7767, decode.d7.loss_dice: 1.2156, decode.d8.loss_cls: 0.4446, decode.d8.loss_mask: 0.7658, decode.d8.loss_dice: 1.1942, loss: 27.1525
2023-02-22 02:05:57,854 - mmseg - INFO - Iter [3200/80000]	lr: 1.378e-06, eta: 2 days, 11:53:42, time: 2.757, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3721, decode.loss_mask: 0.6871, decode.loss_dice: 1.1270, decode.d0.loss_cls: 3.3721, decode.d0.loss_mask: 0.7064, decode.d0.loss_dice: 1.2145, decode.d1.loss_cls: 0.4117, decode.d1.loss_mask: 0.6853, decode.d1.loss_dice: 1.1685, decode.d2.loss_cls: 0.4020, decode.d2.loss_mask: 0.6768, decode.d2.loss_dice: 1.1360, decode.d3.loss_cls: 0.3733, decode.d3.loss_mask: 0.6754, decode.d3.loss_dice: 1.1320, decode.d4.loss_cls: 0.3750, decode.d4.loss_mask: 0.6819, decode.d4.loss_dice: 1.1106, decode.d5.loss_cls: 0.3827, decode.d5.loss_mask: 0.6788, decode.d5.loss_dice: 1.1260, decode.d6.loss_cls: 0.3740, decode.d6.loss_mask: 0.6827, decode.d6.loss_dice: 1.1173, decode.d7.loss_cls: 0.3845, decode.d7.loss_mask: 0.6859, decode.d7.loss_dice: 1.1285, decode.d8.loss_cls: 0.3851, decode.d8.loss_mask: 0.6809, decode.d8.loss_dice: 1.1293, loss: 25.0632
2023-02-22 02:08:15,734 - mmseg - INFO - Iter [3250/80000]	lr: 1.377e-06, eta: 2 days, 11:50:22, time: 2.758, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4084, decode.loss_mask: 0.7123, decode.loss_dice: 1.1705, decode.d0.loss_cls: 3.3444, decode.d0.loss_mask: 0.7345, decode.d0.loss_dice: 1.2585, decode.d1.loss_cls: 0.4044, decode.d1.loss_mask: 0.7171, decode.d1.loss_dice: 1.2284, decode.d2.loss_cls: 0.4098, decode.d2.loss_mask: 0.7215, decode.d2.loss_dice: 1.1847, decode.d3.loss_cls: 0.3964, decode.d3.loss_mask: 0.7126, decode.d3.loss_dice: 1.1801, decode.d4.loss_cls: 0.3930, decode.d4.loss_mask: 0.7169, decode.d4.loss_dice: 1.1703, decode.d5.loss_cls: 0.3985, decode.d5.loss_mask: 0.7092, decode.d5.loss_dice: 1.1747, decode.d6.loss_cls: 0.3959, decode.d6.loss_mask: 0.7134, decode.d6.loss_dice: 1.1571, decode.d7.loss_cls: 0.3911, decode.d7.loss_mask: 0.7104, decode.d7.loss_dice: 1.1682, decode.d8.loss_cls: 0.4096, decode.d8.loss_mask: 0.7073, decode.d8.loss_dice: 1.1723, loss: 25.9716
2023-02-22 02:10:33,713 - mmseg - INFO - Iter [3300/80000]	lr: 1.377e-06, eta: 2 days, 11:47:07, time: 2.760, data_time: 0.027, memory: 31493, decode.loss_cls: 0.4334, decode.loss_mask: 0.7158, decode.loss_dice: 1.1816, decode.d0.loss_cls: 3.3026, decode.d0.loss_mask: 0.7468, decode.d0.loss_dice: 1.2674, decode.d1.loss_cls: 0.4330, decode.d1.loss_mask: 0.7155, decode.d1.loss_dice: 1.2229, decode.d2.loss_cls: 0.4143, decode.d2.loss_mask: 0.7100, decode.d2.loss_dice: 1.1873, decode.d3.loss_cls: 0.4370, decode.d3.loss_mask: 0.7091, decode.d3.loss_dice: 1.1757, decode.d4.loss_cls: 0.4393, decode.d4.loss_mask: 0.7117, decode.d4.loss_dice: 1.1803, decode.d5.loss_cls: 0.4167, decode.d5.loss_mask: 0.7104, decode.d5.loss_dice: 1.1803, decode.d6.loss_cls: 0.4267, decode.d6.loss_mask: 0.6962, decode.d6.loss_dice: 1.1674, decode.d7.loss_cls: 0.4412, decode.d7.loss_mask: 0.7030, decode.d7.loss_dice: 1.1936, decode.d8.loss_cls: 0.4438, decode.d8.loss_mask: 0.7065, decode.d8.loss_dice: 1.1790, loss: 26.2485
2023-02-22 02:12:51,820 - mmseg - INFO - Iter [3350/80000]	lr: 1.376e-06, eta: 2 days, 11:43:56, time: 2.762, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4243, decode.loss_mask: 0.7129, decode.loss_dice: 1.2299, decode.d0.loss_cls: 3.2848, decode.d0.loss_mask: 0.7219, decode.d0.loss_dice: 1.2948, decode.d1.loss_cls: 0.4392, decode.d1.loss_mask: 0.7299, decode.d1.loss_dice: 1.2828, decode.d2.loss_cls: 0.4298, decode.d2.loss_mask: 0.7180, decode.d2.loss_dice: 1.2542, decode.d3.loss_cls: 0.4304, decode.d3.loss_mask: 0.7205, decode.d3.loss_dice: 1.2512, decode.d4.loss_cls: 0.4082, decode.d4.loss_mask: 0.7247, decode.d4.loss_dice: 1.2550, decode.d5.loss_cls: 0.4558, decode.d5.loss_mask: 0.7077, decode.d5.loss_dice: 1.2333, decode.d6.loss_cls: 0.4302, decode.d6.loss_mask: 0.7180, decode.d6.loss_dice: 1.2339, decode.d7.loss_cls: 0.4133, decode.d7.loss_mask: 0.7193, decode.d7.loss_dice: 1.2316, decode.d8.loss_cls: 0.4309, decode.d8.loss_mask: 0.7178, decode.d8.loss_dice: 1.2536, loss: 26.8580
2023-02-22 02:15:09,295 - mmseg - INFO - Iter [3400/80000]	lr: 1.375e-06, eta: 2 days, 11:40:33, time: 2.749, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3236, decode.loss_mask: 0.6285, decode.loss_dice: 1.0940, decode.d0.loss_cls: 3.1818, decode.d0.loss_mask: 0.6415, decode.d0.loss_dice: 1.1850, decode.d1.loss_cls: 0.3683, decode.d1.loss_mask: 0.6505, decode.d1.loss_dice: 1.1543, decode.d2.loss_cls: 0.3485, decode.d2.loss_mask: 0.6299, decode.d2.loss_dice: 1.1061, decode.d3.loss_cls: 0.3309, decode.d3.loss_mask: 0.6215, decode.d3.loss_dice: 1.0950, decode.d4.loss_cls: 0.3487, decode.d4.loss_mask: 0.6231, decode.d4.loss_dice: 1.0935, decode.d5.loss_cls: 0.3135, decode.d5.loss_mask: 0.6344, decode.d5.loss_dice: 1.0880, decode.d6.loss_cls: 0.3333, decode.d6.loss_mask: 0.6253, decode.d6.loss_dice: 1.0869, decode.d7.loss_cls: 0.3473, decode.d7.loss_mask: 0.6256, decode.d7.loss_dice: 1.0836, decode.d8.loss_cls: 0.3189, decode.d8.loss_mask: 0.6277, decode.d8.loss_dice: 1.0949, loss: 23.6040
2023-02-22 02:17:26,629 - mmseg - INFO - Iter [3450/80000]	lr: 1.374e-06, eta: 2 days, 11:37:09, time: 2.747, data_time: 0.024, memory: 31493, decode.loss_cls: 0.3937, decode.loss_mask: 0.7148, decode.loss_dice: 1.2128, decode.d0.loss_cls: 3.2228, decode.d0.loss_mask: 0.7227, decode.d0.loss_dice: 1.2380, decode.d1.loss_cls: 0.3941, decode.d1.loss_mask: 0.7206, decode.d1.loss_dice: 1.2533, decode.d2.loss_cls: 0.4279, decode.d2.loss_mask: 0.7154, decode.d2.loss_dice: 1.2026, decode.d3.loss_cls: 0.4114, decode.d3.loss_mask: 0.7007, decode.d3.loss_dice: 1.1989, decode.d4.loss_cls: 0.4073, decode.d4.loss_mask: 0.7084, decode.d4.loss_dice: 1.1984, decode.d5.loss_cls: 0.4081, decode.d5.loss_mask: 0.7111, decode.d5.loss_dice: 1.1899, decode.d6.loss_cls: 0.3768, decode.d6.loss_mask: 0.7197, decode.d6.loss_dice: 1.1995, decode.d7.loss_cls: 0.4034, decode.d7.loss_mask: 0.7139, decode.d7.loss_dice: 1.2005, decode.d8.loss_cls: 0.4057, decode.d8.loss_mask: 0.7201, decode.d8.loss_dice: 1.2140, loss: 26.1065
2023-02-22 02:19:44,292 - mmseg - INFO - Iter [3500/80000]	lr: 1.373e-06, eta: 2 days, 11:33:53, time: 2.753, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4401, decode.loss_mask: 0.6495, decode.loss_dice: 1.2122, decode.d0.loss_cls: 3.2264, decode.d0.loss_mask: 0.6620, decode.d0.loss_dice: 1.2816, decode.d1.loss_cls: 0.4610, decode.d1.loss_mask: 0.6687, decode.d1.loss_dice: 1.2519, decode.d2.loss_cls: 0.4473, decode.d2.loss_mask: 0.6573, decode.d2.loss_dice: 1.2318, decode.d3.loss_cls: 0.4437, decode.d3.loss_mask: 0.6663, decode.d3.loss_dice: 1.2082, decode.d4.loss_cls: 0.4444, decode.d4.loss_mask: 0.6635, decode.d4.loss_dice: 1.2242, decode.d5.loss_cls: 0.4222, decode.d5.loss_mask: 0.6546, decode.d5.loss_dice: 1.2166, decode.d6.loss_cls: 0.4109, decode.d6.loss_mask: 0.6518, decode.d6.loss_dice: 1.2198, decode.d7.loss_cls: 0.4417, decode.d7.loss_mask: 0.6552, decode.d7.loss_dice: 1.2111, decode.d8.loss_cls: 0.4331, decode.d8.loss_mask: 0.6585, decode.d8.loss_dice: 1.2263, loss: 26.0422
2023-02-22 02:22:01,773 - mmseg - INFO - Iter [3550/80000]	lr: 1.372e-06, eta: 2 days, 11:30:35, time: 2.750, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4157, decode.loss_mask: 0.6601, decode.loss_dice: 1.1375, decode.d0.loss_cls: 3.1769, decode.d0.loss_mask: 0.6796, decode.d0.loss_dice: 1.2362, decode.d1.loss_cls: 0.4469, decode.d1.loss_mask: 0.6654, decode.d1.loss_dice: 1.1815, decode.d2.loss_cls: 0.4377, decode.d2.loss_mask: 0.6601, decode.d2.loss_dice: 1.1568, decode.d3.loss_cls: 0.4468, decode.d3.loss_mask: 0.6606, decode.d3.loss_dice: 1.1131, decode.d4.loss_cls: 0.4608, decode.d4.loss_mask: 0.6531, decode.d4.loss_dice: 1.1000, decode.d5.loss_cls: 0.4314, decode.d5.loss_mask: 0.6625, decode.d5.loss_dice: 1.1079, decode.d6.loss_cls: 0.4276, decode.d6.loss_mask: 0.6642, decode.d6.loss_dice: 1.1139, decode.d7.loss_cls: 0.4210, decode.d7.loss_mask: 0.6740, decode.d7.loss_dice: 1.1271, decode.d8.loss_cls: 0.4227, decode.d8.loss_mask: 0.6696, decode.d8.loss_dice: 1.1270, loss: 25.1378
2023-02-22 02:24:21,740 - mmseg - INFO - Iter [3600/80000]	lr: 1.371e-06, eta: 2 days, 11:28:12, time: 2.799, data_time: 0.075, memory: 31493, decode.loss_cls: 0.3627, decode.loss_mask: 0.6798, decode.loss_dice: 1.2261, decode.d0.loss_cls: 3.0934, decode.d0.loss_mask: 0.6975, decode.d0.loss_dice: 1.2785, decode.d1.loss_cls: 0.3843, decode.d1.loss_mask: 0.6883, decode.d1.loss_dice: 1.2592, decode.d2.loss_cls: 0.3943, decode.d2.loss_mask: 0.6798, decode.d2.loss_dice: 1.2453, decode.d3.loss_cls: 0.3725, decode.d3.loss_mask: 0.6723, decode.d3.loss_dice: 1.2144, decode.d4.loss_cls: 0.3821, decode.d4.loss_mask: 0.6840, decode.d4.loss_dice: 1.2056, decode.d5.loss_cls: 0.4007, decode.d5.loss_mask: 0.6745, decode.d5.loss_dice: 1.2186, decode.d6.loss_cls: 0.3759, decode.d6.loss_mask: 0.6744, decode.d6.loss_dice: 1.2238, decode.d7.loss_cls: 0.3755, decode.d7.loss_mask: 0.6725, decode.d7.loss_dice: 1.2146, decode.d8.loss_cls: 0.3774, decode.d8.loss_mask: 0.6749, decode.d8.loss_dice: 1.2421, loss: 25.6448
2023-02-22 02:26:39,313 - mmseg - INFO - Iter [3650/80000]	lr: 1.370e-06, eta: 2 days, 11:24:59, time: 2.751, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4352, decode.loss_mask: 0.6558, decode.loss_dice: 1.1547, decode.d0.loss_cls: 3.0612, decode.d0.loss_mask: 0.6886, decode.d0.loss_dice: 1.2387, decode.d1.loss_cls: 0.4261, decode.d1.loss_mask: 0.6590, decode.d1.loss_dice: 1.2346, decode.d2.loss_cls: 0.4079, decode.d2.loss_mask: 0.6509, decode.d2.loss_dice: 1.1852, decode.d3.loss_cls: 0.4415, decode.d3.loss_mask: 0.6358, decode.d3.loss_dice: 1.1759, decode.d4.loss_cls: 0.4397, decode.d4.loss_mask: 0.6433, decode.d4.loss_dice: 1.1636, decode.d5.loss_cls: 0.4277, decode.d5.loss_mask: 0.6417, decode.d5.loss_dice: 1.1835, decode.d6.loss_cls: 0.4249, decode.d6.loss_mask: 0.6362, decode.d6.loss_dice: 1.1538, decode.d7.loss_cls: 0.4320, decode.d7.loss_mask: 0.6466, decode.d7.loss_dice: 1.1715, decode.d8.loss_cls: 0.4200, decode.d8.loss_mask: 0.6495, decode.d8.loss_dice: 1.1842, loss: 25.2691
2023-02-22 02:28:56,829 - mmseg - INFO - Iter [3700/80000]	lr: 1.369e-06, eta: 2 days, 11:21:46, time: 2.750, data_time: 0.026, memory: 31493, decode.loss_cls: 0.4331, decode.loss_mask: 0.6637, decode.loss_dice: 1.1894, decode.d0.loss_cls: 3.0984, decode.d0.loss_mask: 0.6691, decode.d0.loss_dice: 1.2585, decode.d1.loss_cls: 0.4228, decode.d1.loss_mask: 0.6786, decode.d1.loss_dice: 1.2589, decode.d2.loss_cls: 0.4404, decode.d2.loss_mask: 0.6721, decode.d2.loss_dice: 1.2209, decode.d3.loss_cls: 0.4208, decode.d3.loss_mask: 0.6723, decode.d3.loss_dice: 1.2323, decode.d4.loss_cls: 0.4069, decode.d4.loss_mask: 0.6769, decode.d4.loss_dice: 1.2126, decode.d5.loss_cls: 0.4066, decode.d5.loss_mask: 0.6683, decode.d5.loss_dice: 1.2202, decode.d6.loss_cls: 0.4272, decode.d6.loss_mask: 0.6678, decode.d6.loss_dice: 1.1929, decode.d7.loss_cls: 0.4273, decode.d7.loss_mask: 0.6638, decode.d7.loss_dice: 1.2015, decode.d8.loss_cls: 0.4380, decode.d8.loss_mask: 0.6643, decode.d8.loss_dice: 1.2026, loss: 25.8081
2023-02-22 02:31:14,376 - mmseg - INFO - Iter [3750/80000]	lr: 1.369e-06, eta: 2 days, 11:18:35, time: 2.751, data_time: 0.026, memory: 31493, decode.loss_cls: 0.3117, decode.loss_mask: 0.5964, decode.loss_dice: 1.1264, decode.d0.loss_cls: 3.0550, decode.d0.loss_mask: 0.6169, decode.d0.loss_dice: 1.2087, decode.d1.loss_cls: 0.3564, decode.d1.loss_mask: 0.6148, decode.d1.loss_dice: 1.1850, decode.d2.loss_cls: 0.3719, decode.d2.loss_mask: 0.5934, decode.d2.loss_dice: 1.1395, decode.d3.loss_cls: 0.3561, decode.d3.loss_mask: 0.5909, decode.d3.loss_dice: 1.1221, decode.d4.loss_cls: 0.3283, decode.d4.loss_mask: 0.5940, decode.d4.loss_dice: 1.1352, decode.d5.loss_cls: 0.3343, decode.d5.loss_mask: 0.5882, decode.d5.loss_dice: 1.1216, decode.d6.loss_cls: 0.3161, decode.d6.loss_mask: 0.5885, decode.d6.loss_dice: 1.1267, decode.d7.loss_cls: 0.3206, decode.d7.loss_mask: 0.5911, decode.d7.loss_dice: 1.1368, decode.d8.loss_cls: 0.3318, decode.d8.loss_mask: 0.6002, decode.d8.loss_dice: 1.1323, loss: 23.4911
2023-02-22 02:33:32,038 - mmseg - INFO - Iter [3800/80000]	lr: 1.368e-06, eta: 2 days, 11:15:28, time: 2.753, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3761, decode.loss_mask: 0.6198, decode.loss_dice: 1.0411, decode.d0.loss_cls: 3.0344, decode.d0.loss_mask: 0.6407, decode.d0.loss_dice: 1.1379, decode.d1.loss_cls: 0.3977, decode.d1.loss_mask: 0.6262, decode.d1.loss_dice: 1.0955, decode.d2.loss_cls: 0.3438, decode.d2.loss_mask: 0.6151, decode.d2.loss_dice: 1.0711, decode.d3.loss_cls: 0.3494, decode.d3.loss_mask: 0.6135, decode.d3.loss_dice: 1.0377, decode.d4.loss_cls: 0.3297, decode.d4.loss_mask: 0.6168, decode.d4.loss_dice: 1.0313, decode.d5.loss_cls: 0.3602, decode.d5.loss_mask: 0.6178, decode.d5.loss_dice: 1.0443, decode.d6.loss_cls: 0.3505, decode.d6.loss_mask: 0.6201, decode.d6.loss_dice: 1.0366, decode.d7.loss_cls: 0.3568, decode.d7.loss_mask: 0.6233, decode.d7.loss_dice: 1.0358, decode.d8.loss_cls: 0.3668, decode.d8.loss_mask: 0.6256, decode.d8.loss_dice: 1.0409, loss: 23.0563
2023-02-22 02:35:49,470 - mmseg - INFO - Iter [3850/80000]	lr: 1.367e-06, eta: 2 days, 11:12:18, time: 2.749, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4397, decode.loss_mask: 0.7018, decode.loss_dice: 1.2125, decode.d0.loss_cls: 2.9572, decode.d0.loss_mask: 0.6995, decode.d0.loss_dice: 1.2609, decode.d1.loss_cls: 0.4424, decode.d1.loss_mask: 0.6876, decode.d1.loss_dice: 1.2549, decode.d2.loss_cls: 0.4415, decode.d2.loss_mask: 0.6819, decode.d2.loss_dice: 1.2093, decode.d3.loss_cls: 0.4679, decode.d3.loss_mask: 0.6685, decode.d3.loss_dice: 1.1860, decode.d4.loss_cls: 0.4396, decode.d4.loss_mask: 0.6706, decode.d4.loss_dice: 1.1780, decode.d5.loss_cls: 0.4199, decode.d5.loss_mask: 0.6844, decode.d5.loss_dice: 1.2011, decode.d6.loss_cls: 0.4493, decode.d6.loss_mask: 0.6845, decode.d6.loss_dice: 1.2036, decode.d7.loss_cls: 0.4469, decode.d7.loss_mask: 0.6895, decode.d7.loss_dice: 1.2079, decode.d8.loss_cls: 0.4357, decode.d8.loss_mask: 0.6917, decode.d8.loss_dice: 1.1952, loss: 25.9096
2023-02-22 02:38:12,503 - mmseg - INFO - Iter [3900/80000]	lr: 1.366e-06, eta: 2 days, 11:10:58, time: 2.861, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3068, decode.loss_mask: 0.6101, decode.loss_dice: 1.1416, decode.d0.loss_cls: 2.9489, decode.d0.loss_mask: 0.6257, decode.d0.loss_dice: 1.1838, decode.d1.loss_cls: 0.3568, decode.d1.loss_mask: 0.6157, decode.d1.loss_dice: 1.1664, decode.d2.loss_cls: 0.3238, decode.d2.loss_mask: 0.6154, decode.d2.loss_dice: 1.1395, decode.d3.loss_cls: 0.3333, decode.d3.loss_mask: 0.6107, decode.d3.loss_dice: 1.1282, decode.d4.loss_cls: 0.3173, decode.d4.loss_mask: 0.6192, decode.d4.loss_dice: 1.1478, decode.d5.loss_cls: 0.2993, decode.d5.loss_mask: 0.6180, decode.d5.loss_dice: 1.1489, decode.d6.loss_cls: 0.3184, decode.d6.loss_mask: 0.6151, decode.d6.loss_dice: 1.1404, decode.d7.loss_cls: 0.3235, decode.d7.loss_mask: 0.6150, decode.d7.loss_dice: 1.1283, decode.d8.loss_cls: 0.3148, decode.d8.loss_mask: 0.6143, decode.d8.loss_dice: 1.1391, loss: 23.4661
2023-02-22 02:40:34,019 - mmseg - INFO - Iter [3950/80000]	lr: 1.365e-06, eta: 2 days, 11:09:07, time: 2.830, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3555, decode.loss_mask: 0.6589, decode.loss_dice: 1.1511, decode.d0.loss_cls: 2.9227, decode.d0.loss_mask: 0.6974, decode.d0.loss_dice: 1.2389, decode.d1.loss_cls: 0.3657, decode.d1.loss_mask: 0.6797, decode.d1.loss_dice: 1.2080, decode.d2.loss_cls: 0.3530, decode.d2.loss_mask: 0.6614, decode.d2.loss_dice: 1.1819, decode.d3.loss_cls: 0.3365, decode.d3.loss_mask: 0.6759, decode.d3.loss_dice: 1.1773, decode.d4.loss_cls: 0.3389, decode.d4.loss_mask: 0.6683, decode.d4.loss_dice: 1.1771, decode.d5.loss_cls: 0.3629, decode.d5.loss_mask: 0.6557, decode.d5.loss_dice: 1.1760, decode.d6.loss_cls: 0.3556, decode.d6.loss_mask: 0.6525, decode.d6.loss_dice: 1.1607, decode.d7.loss_cls: 0.3615, decode.d7.loss_mask: 0.6579, decode.d7.loss_dice: 1.1667, decode.d8.loss_cls: 0.3846, decode.d8.loss_mask: 0.6580, decode.d8.loss_dice: 1.1770, loss: 24.6170
2023-02-22 02:42:53,509 - mmseg - INFO - Saving checkpoint at 4000 iterations
2023-02-22 02:43:16,163 - mmseg - INFO - Exp name: my_city.py
2023-02-22 02:43:16,163 - mmseg - INFO - Iter [4000/80000]	lr: 1.364e-06, eta: 2 days, 11:13:48, time: 3.243, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3746, decode.loss_mask: 0.6609, decode.loss_dice: 1.0757, decode.d0.loss_cls: 2.9427, decode.d0.loss_mask: 0.6690, decode.d0.loss_dice: 1.1491, decode.d1.loss_cls: 0.3847, decode.d1.loss_mask: 0.6555, decode.d1.loss_dice: 1.1246, decode.d2.loss_cls: 0.3624, decode.d2.loss_mask: 0.6645, decode.d2.loss_dice: 1.1242, decode.d3.loss_cls: 0.3793, decode.d3.loss_mask: 0.6518, decode.d3.loss_dice: 1.0896, decode.d4.loss_cls: 0.3491, decode.d4.loss_mask: 0.6529, decode.d4.loss_dice: 1.0808, decode.d5.loss_cls: 0.4065, decode.d5.loss_mask: 0.6436, decode.d5.loss_dice: 1.0756, decode.d6.loss_cls: 0.3719, decode.d6.loss_mask: 0.6446, decode.d6.loss_dice: 1.0636, decode.d7.loss_cls: 0.3680, decode.d7.loss_mask: 0.6459, decode.d7.loss_dice: 1.0667, decode.d8.loss_cls: 0.3815, decode.d8.loss_mask: 0.6508, decode.d8.loss_dice: 1.0900, loss: 23.8002
2023-02-22 02:45:41,882 - mmseg - INFO - Iter [4050/80000]	lr: 1.363e-06, eta: 2 days, 11:13:10, time: 2.914, data_time: 0.028, memory: 31493, decode.loss_cls: 0.4203, decode.loss_mask: 0.7247, decode.loss_dice: 1.2064, decode.d0.loss_cls: 2.8916, decode.d0.loss_mask: 0.7496, decode.d0.loss_dice: 1.2898, decode.d1.loss_cls: 0.4189, decode.d1.loss_mask: 0.7395, decode.d1.loss_dice: 1.2457, decode.d2.loss_cls: 0.4537, decode.d2.loss_mask: 0.7066, decode.d2.loss_dice: 1.2010, decode.d3.loss_cls: 0.4275, decode.d3.loss_mask: 0.7098, decode.d3.loss_dice: 1.1886, decode.d4.loss_cls: 0.4238, decode.d4.loss_mask: 0.7119, decode.d4.loss_dice: 1.1899, decode.d5.loss_cls: 0.4217, decode.d5.loss_mask: 0.7052, decode.d5.loss_dice: 1.1788, decode.d6.loss_cls: 0.4107, decode.d6.loss_mask: 0.7230, decode.d6.loss_dice: 1.1851, decode.d7.loss_cls: 0.4292, decode.d7.loss_mask: 0.7095, decode.d7.loss_dice: 1.1849, decode.d8.loss_cls: 0.4254, decode.d8.loss_mask: 0.7123, decode.d8.loss_dice: 1.1934, loss: 25.9787
2023-02-22 02:48:01,522 - mmseg - INFO - Iter [4100/80000]	lr: 1.362e-06, eta: 2 days, 11:10:36, time: 2.793, data_time: 0.024, memory: 31493, decode.loss_cls: 0.3347, decode.loss_mask: 0.6946, decode.loss_dice: 1.1367, decode.d0.loss_cls: 2.8748, decode.d0.loss_mask: 0.7309, decode.d0.loss_dice: 1.2058, decode.d1.loss_cls: 0.3687, decode.d1.loss_mask: 0.7175, decode.d1.loss_dice: 1.1829, decode.d2.loss_cls: 0.3467, decode.d2.loss_mask: 0.7027, decode.d2.loss_dice: 1.1513, decode.d3.loss_cls: 0.3642, decode.d3.loss_mask: 0.7052, decode.d3.loss_dice: 1.1322, decode.d4.loss_cls: 0.3381, decode.d4.loss_mask: 0.7124, decode.d4.loss_dice: 1.1576, decode.d5.loss_cls: 0.3444, decode.d5.loss_mask: 0.7013, decode.d5.loss_dice: 1.1222, decode.d6.loss_cls: 0.3448, decode.d6.loss_mask: 0.6876, decode.d6.loss_dice: 1.1435, decode.d7.loss_cls: 0.3283, decode.d7.loss_mask: 0.6994, decode.d7.loss_dice: 1.1276, decode.d8.loss_cls: 0.3463, decode.d8.loss_mask: 0.6944, decode.d8.loss_dice: 1.1485, loss: 24.5453
2023-02-22 02:50:19,162 - mmseg - INFO - Iter [4150/80000]	lr: 1.361e-06, eta: 2 days, 11:07:26, time: 2.753, data_time: 0.025, memory: 31493, decode.loss_cls: 0.2897, decode.loss_mask: 0.6600, decode.loss_dice: 1.1400, decode.d0.loss_cls: 2.8538, decode.d0.loss_mask: 0.6748, decode.d0.loss_dice: 1.2411, decode.d1.loss_cls: 0.3161, decode.d1.loss_mask: 0.6646, decode.d1.loss_dice: 1.1889, decode.d2.loss_cls: 0.3498, decode.d2.loss_mask: 0.6662, decode.d2.loss_dice: 1.1502, decode.d3.loss_cls: 0.2978, decode.d3.loss_mask: 0.6549, decode.d3.loss_dice: 1.1633, decode.d4.loss_cls: 0.3182, decode.d4.loss_mask: 0.6549, decode.d4.loss_dice: 1.1587, decode.d5.loss_cls: 0.3036, decode.d5.loss_mask: 0.6613, decode.d5.loss_dice: 1.1603, decode.d6.loss_cls: 0.3373, decode.d6.loss_mask: 0.6533, decode.d6.loss_dice: 1.1404, decode.d7.loss_cls: 0.2963, decode.d7.loss_mask: 0.6563, decode.d7.loss_dice: 1.1474, decode.d8.loss_cls: 0.2924, decode.d8.loss_mask: 0.6550, decode.d8.loss_dice: 1.1550, loss: 23.9018
2023-02-22 02:52:36,593 - mmseg - INFO - Iter [4200/80000]	lr: 1.360e-06, eta: 2 days, 11:04:14, time: 2.749, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3860, decode.loss_mask: 0.6741, decode.loss_dice: 1.1813, decode.d0.loss_cls: 2.8113, decode.d0.loss_mask: 0.7249, decode.d0.loss_dice: 1.2096, decode.d1.loss_cls: 0.4387, decode.d1.loss_mask: 0.6709, decode.d1.loss_dice: 1.1941, decode.d2.loss_cls: 0.3979, decode.d2.loss_mask: 0.6876, decode.d2.loss_dice: 1.1623, decode.d3.loss_cls: 0.4049, decode.d3.loss_mask: 0.6624, decode.d3.loss_dice: 1.1553, decode.d4.loss_cls: 0.3962, decode.d4.loss_mask: 0.6786, decode.d4.loss_dice: 1.1656, decode.d5.loss_cls: 0.3918, decode.d5.loss_mask: 0.6720, decode.d5.loss_dice: 1.1718, decode.d6.loss_cls: 0.3815, decode.d6.loss_mask: 0.6829, decode.d6.loss_dice: 1.1672, decode.d7.loss_cls: 0.4001, decode.d7.loss_mask: 0.6687, decode.d7.loss_dice: 1.1734, decode.d8.loss_cls: 0.3870, decode.d8.loss_mask: 0.6753, decode.d8.loss_dice: 1.1760, loss: 24.9497
2023-02-22 02:54:53,900 - mmseg - INFO - Iter [4250/80000]	lr: 1.360e-06, eta: 2 days, 11:01:01, time: 2.746, data_time: 0.024, memory: 31493, decode.loss_cls: 0.3774, decode.loss_mask: 0.6837, decode.loss_dice: 1.0751, decode.d0.loss_cls: 2.8040, decode.d0.loss_mask: 0.7097, decode.d0.loss_dice: 1.1762, decode.d1.loss_cls: 0.3939, decode.d1.loss_mask: 0.7056, decode.d1.loss_dice: 1.1323, decode.d2.loss_cls: 0.3836, decode.d2.loss_mask: 0.6931, decode.d2.loss_dice: 1.0860, decode.d3.loss_cls: 0.4211, decode.d3.loss_mask: 0.6874, decode.d3.loss_dice: 1.0725, decode.d4.loss_cls: 0.4194, decode.d4.loss_mask: 0.6805, decode.d4.loss_dice: 1.0711, decode.d5.loss_cls: 0.4074, decode.d5.loss_mask: 0.6807, decode.d5.loss_dice: 1.0810, decode.d6.loss_cls: 0.3893, decode.d6.loss_mask: 0.6713, decode.d6.loss_dice: 1.0744, decode.d7.loss_cls: 0.3977, decode.d7.loss_mask: 0.6741, decode.d7.loss_dice: 1.0810, decode.d8.loss_cls: 0.3986, decode.d8.loss_mask: 0.6837, decode.d8.loss_dice: 1.0729, loss: 24.1847
2023-02-22 02:57:13,728 - mmseg - INFO - Iter [4300/80000]	lr: 1.359e-06, eta: 2 days, 10:58:34, time: 2.797, data_time: 0.074, memory: 31493, decode.loss_cls: 0.4301, decode.loss_mask: 0.6444, decode.loss_dice: 1.1560, decode.d0.loss_cls: 2.8051, decode.d0.loss_mask: 0.6748, decode.d0.loss_dice: 1.2793, decode.d1.loss_cls: 0.4597, decode.d1.loss_mask: 0.6599, decode.d1.loss_dice: 1.2259, decode.d2.loss_cls: 0.4445, decode.d2.loss_mask: 0.6542, decode.d2.loss_dice: 1.1713, decode.d3.loss_cls: 0.4344, decode.d3.loss_mask: 0.6571, decode.d3.loss_dice: 1.1640, decode.d4.loss_cls: 0.4181, decode.d4.loss_mask: 0.6535, decode.d4.loss_dice: 1.1655, decode.d5.loss_cls: 0.4161, decode.d5.loss_mask: 0.6563, decode.d5.loss_dice: 1.1893, decode.d6.loss_cls: 0.4200, decode.d6.loss_mask: 0.6516, decode.d6.loss_dice: 1.1713, decode.d7.loss_cls: 0.4170, decode.d7.loss_mask: 0.6486, decode.d7.loss_dice: 1.1673, decode.d8.loss_cls: 0.3959, decode.d8.loss_mask: 0.6537, decode.d8.loss_dice: 1.1888, loss: 25.0736
2023-02-22 02:59:31,203 - mmseg - INFO - Iter [4350/80000]	lr: 1.358e-06, eta: 2 days, 10:55:25, time: 2.749, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3717, decode.loss_mask: 0.5675, decode.loss_dice: 1.0836, decode.d0.loss_cls: 2.7310, decode.d0.loss_mask: 0.5909, decode.d0.loss_dice: 1.1636, decode.d1.loss_cls: 0.3966, decode.d1.loss_mask: 0.5854, decode.d1.loss_dice: 1.1414, decode.d2.loss_cls: 0.3891, decode.d2.loss_mask: 0.5738, decode.d2.loss_dice: 1.1073, decode.d3.loss_cls: 0.3964, decode.d3.loss_mask: 0.5691, decode.d3.loss_dice: 1.0825, decode.d4.loss_cls: 0.3942, decode.d4.loss_mask: 0.5719, decode.d4.loss_dice: 1.0937, decode.d5.loss_cls: 0.3931, decode.d5.loss_mask: 0.5671, decode.d5.loss_dice: 1.1046, decode.d6.loss_cls: 0.4038, decode.d6.loss_mask: 0.5677, decode.d6.loss_dice: 1.0722, decode.d7.loss_cls: 0.3759, decode.d7.loss_mask: 0.5702, decode.d7.loss_dice: 1.0840, decode.d8.loss_cls: 0.3761, decode.d8.loss_mask: 0.5680, decode.d8.loss_dice: 1.0901, loss: 22.9823
2023-02-22 03:01:48,729 - mmseg - INFO - Iter [4400/80000]	lr: 1.357e-06, eta: 2 days, 10:52:19, time: 2.750, data_time: 0.026, memory: 31493, decode.loss_cls: 0.3422, decode.loss_mask: 0.6748, decode.loss_dice: 1.1803, decode.d0.loss_cls: 2.7196, decode.d0.loss_mask: 0.6901, decode.d0.loss_dice: 1.2527, decode.d1.loss_cls: 0.3348, decode.d1.loss_mask: 0.6865, decode.d1.loss_dice: 1.2420, decode.d2.loss_cls: 0.3285, decode.d2.loss_mask: 0.6752, decode.d2.loss_dice: 1.2087, decode.d3.loss_cls: 0.3514, decode.d3.loss_mask: 0.6758, decode.d3.loss_dice: 1.1876, decode.d4.loss_cls: 0.3552, decode.d4.loss_mask: 0.6724, decode.d4.loss_dice: 1.1800, decode.d5.loss_cls: 0.3428, decode.d5.loss_mask: 0.6690, decode.d5.loss_dice: 1.1977, decode.d6.loss_cls: 0.3289, decode.d6.loss_mask: 0.6681, decode.d6.loss_dice: 1.1769, decode.d7.loss_cls: 0.3267, decode.d7.loss_mask: 0.6782, decode.d7.loss_dice: 1.1917, decode.d8.loss_cls: 0.3327, decode.d8.loss_mask: 0.6790, decode.d8.loss_dice: 1.1993, loss: 24.5487
2023-02-22 03:04:06,063 - mmseg - INFO - Iter [4450/80000]	lr: 1.356e-06, eta: 2 days, 10:49:11, time: 2.747, data_time: 0.024, memory: 31493, decode.loss_cls: 0.4019, decode.loss_mask: 0.7018, decode.loss_dice: 1.1693, decode.d0.loss_cls: 2.7230, decode.d0.loss_mask: 0.7263, decode.d0.loss_dice: 1.2378, decode.d1.loss_cls: 0.4337, decode.d1.loss_mask: 0.6934, decode.d1.loss_dice: 1.2011, decode.d2.loss_cls: 0.4047, decode.d2.loss_mask: 0.6979, decode.d2.loss_dice: 1.1860, decode.d3.loss_cls: 0.4212, decode.d3.loss_mask: 0.6896, decode.d3.loss_dice: 1.1439, decode.d4.loss_cls: 0.4067, decode.d4.loss_mask: 0.6915, decode.d4.loss_dice: 1.1572, decode.d5.loss_cls: 0.4016, decode.d5.loss_mask: 0.6961, decode.d5.loss_dice: 1.1663, decode.d6.loss_cls: 0.4358, decode.d6.loss_mask: 0.6803, decode.d6.loss_dice: 1.1440, decode.d7.loss_cls: 0.4266, decode.d7.loss_mask: 0.6831, decode.d7.loss_dice: 1.1615, decode.d8.loss_cls: 0.4016, decode.d8.loss_mask: 0.6899, decode.d8.loss_dice: 1.1673, loss: 25.1411
2023-02-22 03:06:23,534 - mmseg - INFO - Iter [4500/80000]	lr: 1.355e-06, eta: 2 days, 10:46:06, time: 2.749, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3878, decode.loss_mask: 0.6778, decode.loss_dice: 1.1817, decode.d0.loss_cls: 2.6310, decode.d0.loss_mask: 0.7116, decode.d0.loss_dice: 1.2719, decode.d1.loss_cls: 0.4126, decode.d1.loss_mask: 0.6968, decode.d1.loss_dice: 1.2213, decode.d2.loss_cls: 0.3792, decode.d2.loss_mask: 0.6994, decode.d2.loss_dice: 1.1929, decode.d3.loss_cls: 0.3927, decode.d3.loss_mask: 0.6911, decode.d3.loss_dice: 1.1933, decode.d4.loss_cls: 0.3744, decode.d4.loss_mask: 0.6966, decode.d4.loss_dice: 1.1772, decode.d5.loss_cls: 0.3908, decode.d5.loss_mask: 0.6861, decode.d5.loss_dice: 1.1807, decode.d6.loss_cls: 0.3778, decode.d6.loss_mask: 0.6785, decode.d6.loss_dice: 1.1715, decode.d7.loss_cls: 0.3992, decode.d7.loss_mask: 0.6784, decode.d7.loss_dice: 1.1767, decode.d8.loss_cls: 0.3679, decode.d8.loss_mask: 0.6838, decode.d8.loss_dice: 1.1879, loss: 24.9690
2023-02-22 03:08:41,103 - mmseg - INFO - Iter [4550/80000]	lr: 1.354e-06, eta: 2 days, 10:43:04, time: 2.751, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4116, decode.loss_mask: 0.6155, decode.loss_dice: 1.0361, decode.d0.loss_cls: 2.6729, decode.d0.loss_mask: 0.6433, decode.d0.loss_dice: 1.1412, decode.d1.loss_cls: 0.4118, decode.d1.loss_mask: 0.6297, decode.d1.loss_dice: 1.1088, decode.d2.loss_cls: 0.4310, decode.d2.loss_mask: 0.6098, decode.d2.loss_dice: 1.0692, decode.d3.loss_cls: 0.4341, decode.d3.loss_mask: 0.6278, decode.d3.loss_dice: 1.0280, decode.d4.loss_cls: 0.4118, decode.d4.loss_mask: 0.6177, decode.d4.loss_dice: 1.0462, decode.d5.loss_cls: 0.4104, decode.d5.loss_mask: 0.6198, decode.d5.loss_dice: 1.0567, decode.d6.loss_cls: 0.4295, decode.d6.loss_mask: 0.6170, decode.d6.loss_dice: 1.0353, decode.d7.loss_cls: 0.3873, decode.d7.loss_mask: 0.6194, decode.d7.loss_dice: 1.0419, decode.d8.loss_cls: 0.4111, decode.d8.loss_mask: 0.6140, decode.d8.loss_dice: 1.0499, loss: 23.2389
2023-02-22 03:10:58,627 - mmseg - INFO - Iter [4600/80000]	lr: 1.353e-06, eta: 2 days, 10:40:01, time: 2.750, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3829, decode.loss_mask: 0.6694, decode.loss_dice: 1.0461, decode.d0.loss_cls: 2.6193, decode.d0.loss_mask: 0.7196, decode.d0.loss_dice: 1.1542, decode.d1.loss_cls: 0.4328, decode.d1.loss_mask: 0.6849, decode.d1.loss_dice: 1.1201, decode.d2.loss_cls: 0.4429, decode.d2.loss_mask: 0.6682, decode.d2.loss_dice: 1.0498, decode.d3.loss_cls: 0.4284, decode.d3.loss_mask: 0.6529, decode.d3.loss_dice: 1.0196, decode.d4.loss_cls: 0.4165, decode.d4.loss_mask: 0.6536, decode.d4.loss_dice: 1.0457, decode.d5.loss_cls: 0.4291, decode.d5.loss_mask: 0.6574, decode.d5.loss_dice: 1.0379, decode.d6.loss_cls: 0.4090, decode.d6.loss_mask: 0.6597, decode.d6.loss_dice: 1.0311, decode.d7.loss_cls: 0.4087, decode.d7.loss_mask: 0.6546, decode.d7.loss_dice: 1.0441, decode.d8.loss_cls: 0.4019, decode.d8.loss_mask: 0.6560, decode.d8.loss_dice: 1.0540, loss: 23.6503
2023-02-22 03:13:15,924 - mmseg - INFO - Iter [4650/80000]	lr: 1.352e-06, eta: 2 days, 10:36:57, time: 2.746, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3874, decode.loss_mask: 0.6613, decode.loss_dice: 1.0870, decode.d0.loss_cls: 2.6063, decode.d0.loss_mask: 0.6796, decode.d0.loss_dice: 1.1721, decode.d1.loss_cls: 0.3894, decode.d1.loss_mask: 0.6784, decode.d1.loss_dice: 1.1496, decode.d2.loss_cls: 0.4110, decode.d2.loss_mask: 0.6684, decode.d2.loss_dice: 1.1139, decode.d3.loss_cls: 0.3980, decode.d3.loss_mask: 0.6717, decode.d3.loss_dice: 1.1006, decode.d4.loss_cls: 0.3914, decode.d4.loss_mask: 0.6623, decode.d4.loss_dice: 1.1164, decode.d5.loss_cls: 0.3810, decode.d5.loss_mask: 0.6668, decode.d5.loss_dice: 1.1083, decode.d6.loss_cls: 0.3718, decode.d6.loss_mask: 0.6596, decode.d6.loss_dice: 1.0915, decode.d7.loss_cls: 0.3781, decode.d7.loss_mask: 0.6698, decode.d7.loss_dice: 1.0864, decode.d8.loss_cls: 0.3717, decode.d8.loss_mask: 0.6651, decode.d8.loss_dice: 1.0954, loss: 23.8900
2023-02-22 03:15:33,223 - mmseg - INFO - Iter [4700/80000]	lr: 1.351e-06, eta: 2 days, 10:33:53, time: 2.746, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4493, decode.loss_mask: 0.7111, decode.loss_dice: 1.2031, decode.d0.loss_cls: 2.6086, decode.d0.loss_mask: 0.7255, decode.d0.loss_dice: 1.2970, decode.d1.loss_cls: 0.5154, decode.d1.loss_mask: 0.7228, decode.d1.loss_dice: 1.2628, decode.d2.loss_cls: 0.4505, decode.d2.loss_mask: 0.7279, decode.d2.loss_dice: 1.2343, decode.d3.loss_cls: 0.4789, decode.d3.loss_mask: 0.7181, decode.d3.loss_dice: 1.2259, decode.d4.loss_cls: 0.4756, decode.d4.loss_mask: 0.7173, decode.d4.loss_dice: 1.2359, decode.d5.loss_cls: 0.4692, decode.d5.loss_mask: 0.7223, decode.d5.loss_dice: 1.2260, decode.d6.loss_cls: 0.4638, decode.d6.loss_mask: 0.7137, decode.d6.loss_dice: 1.2276, decode.d7.loss_cls: 0.4759, decode.d7.loss_mask: 0.7115, decode.d7.loss_dice: 1.2100, decode.d8.loss_cls: 0.4705, decode.d8.loss_mask: 0.7105, decode.d8.loss_dice: 1.2174, loss: 26.3785
2023-02-22 03:17:50,658 - mmseg - INFO - Iter [4750/80000]	lr: 1.351e-06, eta: 2 days, 10:30:52, time: 2.749, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4603, decode.loss_mask: 0.6904, decode.loss_dice: 1.1878, decode.d0.loss_cls: 2.6090, decode.d0.loss_mask: 0.6926, decode.d0.loss_dice: 1.2790, decode.d1.loss_cls: 0.4716, decode.d1.loss_mask: 0.6906, decode.d1.loss_dice: 1.2263, decode.d2.loss_cls: 0.4709, decode.d2.loss_mask: 0.6981, decode.d2.loss_dice: 1.2071, decode.d3.loss_cls: 0.4799, decode.d3.loss_mask: 0.7071, decode.d3.loss_dice: 1.1945, decode.d4.loss_cls: 0.4657, decode.d4.loss_mask: 0.7073, decode.d4.loss_dice: 1.2085, decode.d5.loss_cls: 0.4893, decode.d5.loss_mask: 0.6871, decode.d5.loss_dice: 1.1790, decode.d6.loss_cls: 0.4758, decode.d6.loss_mask: 0.6848, decode.d6.loss_dice: 1.2019, decode.d7.loss_cls: 0.4713, decode.d7.loss_mask: 0.6981, decode.d7.loss_dice: 1.1987, decode.d8.loss_cls: 0.4657, decode.d8.loss_mask: 0.6975, decode.d8.loss_dice: 1.1904, loss: 25.8863
2023-02-22 03:20:08,018 - mmseg - INFO - Iter [4800/80000]	lr: 1.350e-06, eta: 2 days, 10:27:52, time: 2.747, data_time: 0.024, memory: 31493, decode.loss_cls: 0.2994, decode.loss_mask: 0.6549, decode.loss_dice: 1.0765, decode.d0.loss_cls: 2.4836, decode.d0.loss_mask: 0.6811, decode.d0.loss_dice: 1.1442, decode.d1.loss_cls: 0.3408, decode.d1.loss_mask: 0.6587, decode.d1.loss_dice: 1.1300, decode.d2.loss_cls: 0.2752, decode.d2.loss_mask: 0.6542, decode.d2.loss_dice: 1.1041, decode.d3.loss_cls: 0.2991, decode.d3.loss_mask: 0.6483, decode.d3.loss_dice: 1.0982, decode.d4.loss_cls: 0.3059, decode.d4.loss_mask: 0.6630, decode.d4.loss_dice: 1.0939, decode.d5.loss_cls: 0.2991, decode.d5.loss_mask: 0.6623, decode.d5.loss_dice: 1.0899, decode.d6.loss_cls: 0.3012, decode.d6.loss_mask: 0.6474, decode.d6.loss_dice: 1.0950, decode.d7.loss_cls: 0.2956, decode.d7.loss_mask: 0.6444, decode.d7.loss_dice: 1.0971, decode.d8.loss_cls: 0.3015, decode.d8.loss_mask: 0.6584, decode.d8.loss_dice: 1.0883, loss: 22.7913
2023-02-22 03:22:25,168 - mmseg - INFO - Iter [4850/80000]	lr: 1.349e-06, eta: 2 days, 10:24:48, time: 2.743, data_time: 0.024, memory: 31493, decode.loss_cls: 0.2864, decode.loss_mask: 0.6366, decode.loss_dice: 1.0243, decode.d0.loss_cls: 2.4960, decode.d0.loss_mask: 0.6603, decode.d0.loss_dice: 1.1126, decode.d1.loss_cls: 0.3064, decode.d1.loss_mask: 0.6330, decode.d1.loss_dice: 1.0811, decode.d2.loss_cls: 0.3244, decode.d2.loss_mask: 0.6311, decode.d2.loss_dice: 1.0473, decode.d3.loss_cls: 0.3045, decode.d3.loss_mask: 0.6299, decode.d3.loss_dice: 1.0544, decode.d4.loss_cls: 0.2916, decode.d4.loss_mask: 0.6299, decode.d4.loss_dice: 1.0555, decode.d5.loss_cls: 0.3068, decode.d5.loss_mask: 0.6367, decode.d5.loss_dice: 1.0565, decode.d6.loss_cls: 0.3052, decode.d6.loss_mask: 0.6295, decode.d6.loss_dice: 1.0497, decode.d7.loss_cls: 0.2928, decode.d7.loss_mask: 0.6315, decode.d7.loss_dice: 1.0301, decode.d8.loss_cls: 0.3077, decode.d8.loss_mask: 0.6326, decode.d8.loss_dice: 1.0312, loss: 22.1155
2023-02-22 03:24:42,462 - mmseg - INFO - Iter [4900/80000]	lr: 1.348e-06, eta: 2 days, 10:21:48, time: 2.746, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3907, decode.loss_mask: 0.6249, decode.loss_dice: 1.0555, decode.d0.loss_cls: 2.4723, decode.d0.loss_mask: 0.6487, decode.d0.loss_dice: 1.1514, decode.d1.loss_cls: 0.4096, decode.d1.loss_mask: 0.6364, decode.d1.loss_dice: 1.1270, decode.d2.loss_cls: 0.3881, decode.d2.loss_mask: 0.6317, decode.d2.loss_dice: 1.0969, decode.d3.loss_cls: 0.4001, decode.d3.loss_mask: 0.6256, decode.d3.loss_dice: 1.0833, decode.d4.loss_cls: 0.3791, decode.d4.loss_mask: 0.6309, decode.d4.loss_dice: 1.0607, decode.d5.loss_cls: 0.3917, decode.d5.loss_mask: 0.6237, decode.d5.loss_dice: 1.0564, decode.d6.loss_cls: 0.3997, decode.d6.loss_mask: 0.6236, decode.d6.loss_dice: 1.0554, decode.d7.loss_cls: 0.4062, decode.d7.loss_mask: 0.6224, decode.d7.loss_dice: 1.0493, decode.d8.loss_cls: 0.4040, decode.d8.loss_mask: 0.6276, decode.d8.loss_dice: 1.0716, loss: 23.1444
2023-02-22 03:27:00,350 - mmseg - INFO - Iter [4950/80000]	lr: 1.347e-06, eta: 2 days, 10:18:58, time: 2.758, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4364, decode.loss_mask: 0.7025, decode.loss_dice: 1.1653, decode.d0.loss_cls: 2.5823, decode.d0.loss_mask: 0.7012, decode.d0.loss_dice: 1.1965, decode.d1.loss_cls: 0.3730, decode.d1.loss_mask: 0.7236, decode.d1.loss_dice: 1.2227, decode.d2.loss_cls: 0.4090, decode.d2.loss_mask: 0.7188, decode.d2.loss_dice: 1.2066, decode.d3.loss_cls: 0.4101, decode.d3.loss_mask: 0.7226, decode.d3.loss_dice: 1.1886, decode.d4.loss_cls: 0.4173, decode.d4.loss_mask: 0.7168, decode.d4.loss_dice: 1.1801, decode.d5.loss_cls: 0.4073, decode.d5.loss_mask: 0.7141, decode.d5.loss_dice: 1.1818, decode.d6.loss_cls: 0.3977, decode.d6.loss_mask: 0.6992, decode.d6.loss_dice: 1.1840, decode.d7.loss_cls: 0.4306, decode.d7.loss_mask: 0.6996, decode.d7.loss_dice: 1.1634, decode.d8.loss_cls: 0.4158, decode.d8.loss_mask: 0.6955, decode.d8.loss_dice: 1.1657, loss: 25.2281
2023-02-22 03:29:20,111 - mmseg - INFO - Saving checkpoint at 5000 iterations
2023-02-22 03:29:42,817 - mmseg - INFO - Exp name: my_city.py
2023-02-22 03:29:42,818 - mmseg - INFO - Iter [5000/80000]	lr: 1.346e-06, eta: 2 days, 10:22:17, time: 3.249, data_time: 0.074, memory: 31493, decode.loss_cls: 0.4057, decode.loss_mask: 0.6636, decode.loss_dice: 1.1799, decode.d0.loss_cls: 2.4693, decode.d0.loss_mask: 0.6731, decode.d0.loss_dice: 1.2768, decode.d1.loss_cls: 0.3666, decode.d1.loss_mask: 0.6727, decode.d1.loss_dice: 1.2418, decode.d2.loss_cls: 0.4299, decode.d2.loss_mask: 0.6621, decode.d2.loss_dice: 1.1973, decode.d3.loss_cls: 0.4224, decode.d3.loss_mask: 0.6606, decode.d3.loss_dice: 1.1857, decode.d4.loss_cls: 0.4060, decode.d4.loss_mask: 0.6603, decode.d4.loss_dice: 1.1992, decode.d5.loss_cls: 0.4030, decode.d5.loss_mask: 0.6643, decode.d5.loss_dice: 1.1752, decode.d6.loss_cls: 0.4280, decode.d6.loss_mask: 0.6544, decode.d6.loss_dice: 1.1624, decode.d7.loss_cls: 0.4061, decode.d7.loss_mask: 0.6557, decode.d7.loss_dice: 1.1591, decode.d8.loss_cls: 0.4293, decode.d8.loss_mask: 0.6663, decode.d8.loss_dice: 1.1732, loss: 24.7499
2023-02-22 03:32:00,534 - mmseg - INFO - Iter [5050/80000]	lr: 1.345e-06, eta: 2 days, 10:19:22, time: 2.754, data_time: 0.026, memory: 31493, decode.loss_cls: 0.3360, decode.loss_mask: 0.6398, decode.loss_dice: 1.1436, decode.d0.loss_cls: 2.4314, decode.d0.loss_mask: 0.6503, decode.d0.loss_dice: 1.2182, decode.d1.loss_cls: 0.3661, decode.d1.loss_mask: 0.6546, decode.d1.loss_dice: 1.1870, decode.d2.loss_cls: 0.3263, decode.d2.loss_mask: 0.6475, decode.d2.loss_dice: 1.1744, decode.d3.loss_cls: 0.3392, decode.d3.loss_mask: 0.6573, decode.d3.loss_dice: 1.1713, decode.d4.loss_cls: 0.3501, decode.d4.loss_mask: 0.6542, decode.d4.loss_dice: 1.1605, decode.d5.loss_cls: 0.3563, decode.d5.loss_mask: 0.6451, decode.d5.loss_dice: 1.1404, decode.d6.loss_cls: 0.3419, decode.d6.loss_mask: 0.6424, decode.d6.loss_dice: 1.1386, decode.d7.loss_cls: 0.3419, decode.d7.loss_mask: 0.6440, decode.d7.loss_dice: 1.1566, decode.d8.loss_cls: 0.3533, decode.d8.loss_mask: 0.6444, decode.d8.loss_dice: 1.1529, loss: 23.6655
2023-02-22 03:34:18,090 - mmseg - INFO - Iter [5100/80000]	lr: 1.344e-06, eta: 2 days, 10:16:25, time: 2.751, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3352, decode.loss_mask: 0.6414, decode.loss_dice: 1.1186, decode.d0.loss_cls: 2.4082, decode.d0.loss_mask: 0.6644, decode.d0.loss_dice: 1.2201, decode.d1.loss_cls: 0.3358, decode.d1.loss_mask: 0.6438, decode.d1.loss_dice: 1.1858, decode.d2.loss_cls: 0.3634, decode.d2.loss_mask: 0.6439, decode.d2.loss_dice: 1.1685, decode.d3.loss_cls: 0.3452, decode.d3.loss_mask: 0.6360, decode.d3.loss_dice: 1.1522, decode.d4.loss_cls: 0.3714, decode.d4.loss_mask: 0.6392, decode.d4.loss_dice: 1.1456, decode.d5.loss_cls: 0.3299, decode.d5.loss_mask: 0.6409, decode.d5.loss_dice: 1.1419, decode.d6.loss_cls: 0.3496, decode.d6.loss_mask: 0.6440, decode.d6.loss_dice: 1.1506, decode.d7.loss_cls: 0.3481, decode.d7.loss_mask: 0.6465, decode.d7.loss_dice: 1.1429, decode.d8.loss_cls: 0.3408, decode.d8.loss_mask: 0.6413, decode.d8.loss_dice: 1.1232, loss: 23.5184
2023-02-22 03:36:35,764 - mmseg - INFO - Iter [5150/80000]	lr: 1.343e-06, eta: 2 days, 10:13:30, time: 2.753, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3886, decode.loss_mask: 0.6755, decode.loss_dice: 1.1830, decode.d0.loss_cls: 2.4208, decode.d0.loss_mask: 0.7016, decode.d0.loss_dice: 1.2780, decode.d1.loss_cls: 0.4042, decode.d1.loss_mask: 0.6985, decode.d1.loss_dice: 1.2253, decode.d2.loss_cls: 0.4338, decode.d2.loss_mask: 0.6662, decode.d2.loss_dice: 1.1773, decode.d3.loss_cls: 0.3937, decode.d3.loss_mask: 0.6679, decode.d3.loss_dice: 1.1648, decode.d4.loss_cls: 0.3705, decode.d4.loss_mask: 0.6735, decode.d4.loss_dice: 1.1814, decode.d5.loss_cls: 0.3966, decode.d5.loss_mask: 0.6728, decode.d5.loss_dice: 1.1711, decode.d6.loss_cls: 0.4012, decode.d6.loss_mask: 0.6640, decode.d6.loss_dice: 1.1667, decode.d7.loss_cls: 0.4093, decode.d7.loss_mask: 0.6663, decode.d7.loss_dice: 1.1607, decode.d8.loss_cls: 0.3972, decode.d8.loss_mask: 0.6739, decode.d8.loss_dice: 1.1719, loss: 24.6563
2023-02-22 03:39:03,619 - mmseg - INFO - Iter [5200/80000]	lr: 1.342e-06, eta: 2 days, 10:13:03, time: 2.957, data_time: 0.033, memory: 31493, decode.loss_cls: 0.3833, decode.loss_mask: 0.6257, decode.loss_dice: 1.1116, decode.d0.loss_cls: 2.4175, decode.d0.loss_mask: 0.6643, decode.d0.loss_dice: 1.2122, decode.d1.loss_cls: 0.3737, decode.d1.loss_mask: 0.6357, decode.d1.loss_dice: 1.1649, decode.d2.loss_cls: 0.4027, decode.d2.loss_mask: 0.6189, decode.d2.loss_dice: 1.1389, decode.d3.loss_cls: 0.3877, decode.d3.loss_mask: 0.6188, decode.d3.loss_dice: 1.1347, decode.d4.loss_cls: 0.3899, decode.d4.loss_mask: 0.6181, decode.d4.loss_dice: 1.1156, decode.d5.loss_cls: 0.3788, decode.d5.loss_mask: 0.6247, decode.d5.loss_dice: 1.1259, decode.d6.loss_cls: 0.3866, decode.d6.loss_mask: 0.6267, decode.d6.loss_dice: 1.1257, decode.d7.loss_cls: 0.3737, decode.d7.loss_mask: 0.6242, decode.d7.loss_dice: 1.1282, decode.d8.loss_cls: 0.3679, decode.d8.loss_mask: 0.6275, decode.d8.loss_dice: 1.1267, loss: 23.5309
2023-02-22 03:41:21,741 - mmseg - INFO - Iter [5250/80000]	lr: 1.342e-06, eta: 2 days, 10:10:15, time: 2.762, data_time: 0.024, memory: 31493, decode.loss_cls: 0.4099, decode.loss_mask: 0.6315, decode.loss_dice: 1.1044, decode.d0.loss_cls: 2.3630, decode.d0.loss_mask: 0.6451, decode.d0.loss_dice: 1.1816, decode.d1.loss_cls: 0.4155, decode.d1.loss_mask: 0.6389, decode.d1.loss_dice: 1.1266, decode.d2.loss_cls: 0.3743, decode.d2.loss_mask: 0.6325, decode.d2.loss_dice: 1.0963, decode.d3.loss_cls: 0.3967, decode.d3.loss_mask: 0.6288, decode.d3.loss_dice: 1.1095, decode.d4.loss_cls: 0.3775, decode.d4.loss_mask: 0.6384, decode.d4.loss_dice: 1.1205, decode.d5.loss_cls: 0.3930, decode.d5.loss_mask: 0.6320, decode.d5.loss_dice: 1.1116, decode.d6.loss_cls: 0.3640, decode.d6.loss_mask: 0.6304, decode.d6.loss_dice: 1.0929, decode.d7.loss_cls: 0.3888, decode.d7.loss_mask: 0.6294, decode.d7.loss_dice: 1.1064, decode.d8.loss_cls: 0.3664, decode.d8.loss_mask: 0.6358, decode.d8.loss_dice: 1.1026, loss: 23.3445
2023-02-22 03:43:40,251 - mmseg - INFO - Iter [5300/80000]	lr: 1.341e-06, eta: 2 days, 10:07:33, time: 2.770, data_time: 0.026, memory: 31493, decode.loss_cls: 0.3549, decode.loss_mask: 0.6429, decode.loss_dice: 1.1715, decode.d0.loss_cls: 2.3782, decode.d0.loss_mask: 0.6703, decode.d0.loss_dice: 1.2766, decode.d1.loss_cls: 0.3699, decode.d1.loss_mask: 0.6664, decode.d1.loss_dice: 1.2254, decode.d2.loss_cls: 0.3624, decode.d2.loss_mask: 0.6511, decode.d2.loss_dice: 1.1841, decode.d3.loss_cls: 0.3795, decode.d3.loss_mask: 0.6481, decode.d3.loss_dice: 1.1756, decode.d4.loss_cls: 0.3452, decode.d4.loss_mask: 0.6555, decode.d4.loss_dice: 1.2023, decode.d5.loss_cls: 0.3683, decode.d5.loss_mask: 0.6416, decode.d5.loss_dice: 1.1899, decode.d6.loss_cls: 0.3854, decode.d6.loss_mask: 0.6488, decode.d6.loss_dice: 1.1690, decode.d7.loss_cls: 0.3608, decode.d7.loss_mask: 0.6445, decode.d7.loss_dice: 1.1720, decode.d8.loss_cls: 0.3750, decode.d8.loss_mask: 0.6464, decode.d8.loss_dice: 1.1663, loss: 24.1280
2023-02-22 03:46:03,756 - mmseg - INFO - Iter [5350/80000]	lr: 1.340e-06, eta: 2 days, 10:06:01, time: 2.870, data_time: 0.027, memory: 31493, decode.loss_cls: 0.3730, decode.loss_mask: 0.6469, decode.loss_dice: 1.0779, decode.d0.loss_cls: 2.3937, decode.d0.loss_mask: 0.6677, decode.d0.loss_dice: 1.1696, decode.d1.loss_cls: 0.4421, decode.d1.loss_mask: 0.6489, decode.d1.loss_dice: 1.1007, decode.d2.loss_cls: 0.4008, decode.d2.loss_mask: 0.6501, decode.d2.loss_dice: 1.0940, decode.d3.loss_cls: 0.3628, decode.d3.loss_mask: 0.6506, decode.d3.loss_dice: 1.0912, decode.d4.loss_cls: 0.3801, decode.d4.loss_mask: 0.6545, decode.d4.loss_dice: 1.0893, decode.d5.loss_cls: 0.3921, decode.d5.loss_mask: 0.6547, decode.d5.loss_dice: 1.0683, decode.d6.loss_cls: 0.3789, decode.d6.loss_mask: 0.6517, decode.d6.loss_dice: 1.0861, decode.d7.loss_cls: 0.3780, decode.d7.loss_mask: 0.6482, decode.d7.loss_dice: 1.0778, decode.d8.loss_cls: 0.3815, decode.d8.loss_mask: 0.6482, decode.d8.loss_dice: 1.0921, loss: 23.3514
2023-02-22 03:48:22,739 - mmseg - INFO - Iter [5400/80000]	lr: 1.339e-06, eta: 2 days, 10:03:25, time: 2.780, data_time: 0.026, memory: 31493, decode.loss_cls: 0.3339, decode.loss_mask: 0.6114, decode.loss_dice: 1.0896, decode.d0.loss_cls: 2.3398, decode.d0.loss_mask: 0.6279, decode.d0.loss_dice: 1.1783, decode.d1.loss_cls: 0.3700, decode.d1.loss_mask: 0.6226, decode.d1.loss_dice: 1.1347, decode.d2.loss_cls: 0.3827, decode.d2.loss_mask: 0.6175, decode.d2.loss_dice: 1.1165, decode.d3.loss_cls: 0.3845, decode.d3.loss_mask: 0.6120, decode.d3.loss_dice: 1.0942, decode.d4.loss_cls: 0.3499, decode.d4.loss_mask: 0.6087, decode.d4.loss_dice: 1.1011, decode.d5.loss_cls: 0.3540, decode.d5.loss_mask: 0.6119, decode.d5.loss_dice: 1.0906, decode.d6.loss_cls: 0.3580, decode.d6.loss_mask: 0.6151, decode.d6.loss_dice: 1.1038, decode.d7.loss_cls: 0.3370, decode.d7.loss_mask: 0.6167, decode.d7.loss_dice: 1.0932, decode.d8.loss_cls: 0.3398, decode.d8.loss_mask: 0.6123, decode.d8.loss_dice: 1.0971, loss: 22.8048
2023-02-22 03:50:41,068 - mmseg - INFO - Iter [5450/80000]	lr: 1.338e-06, eta: 2 days, 10:00:41, time: 2.767, data_time: 0.026, memory: 31493, decode.loss_cls: 0.3037, decode.loss_mask: 0.6038, decode.loss_dice: 1.0756, decode.d0.loss_cls: 2.2443, decode.d0.loss_mask: 0.6128, decode.d0.loss_dice: 1.1351, decode.d1.loss_cls: 0.3100, decode.d1.loss_mask: 0.5962, decode.d1.loss_dice: 1.1113, decode.d2.loss_cls: 0.2966, decode.d2.loss_mask: 0.5975, decode.d2.loss_dice: 1.0928, decode.d3.loss_cls: 0.3059, decode.d3.loss_mask: 0.5952, decode.d3.loss_dice: 1.0801, decode.d4.loss_cls: 0.3200, decode.d4.loss_mask: 0.5927, decode.d4.loss_dice: 1.0766, decode.d5.loss_cls: 0.2891, decode.d5.loss_mask: 0.5975, decode.d5.loss_dice: 1.0796, decode.d6.loss_cls: 0.3098, decode.d6.loss_mask: 0.5925, decode.d6.loss_dice: 1.0737, decode.d7.loss_cls: 0.2978, decode.d7.loss_mask: 0.6045, decode.d7.loss_dice: 1.0884, decode.d8.loss_cls: 0.2898, decode.d8.loss_mask: 0.6045, decode.d8.loss_dice: 1.0772, loss: 21.8544
2023-02-22 03:52:58,557 - mmseg - INFO - Iter [5500/80000]	lr: 1.337e-06, eta: 2 days, 9:57:46, time: 2.750, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3762, decode.loss_mask: 0.6552, decode.loss_dice: 1.1324, decode.d0.loss_cls: 2.2643, decode.d0.loss_mask: 0.6911, decode.d0.loss_dice: 1.2114, decode.d1.loss_cls: 0.4144, decode.d1.loss_mask: 0.6611, decode.d1.loss_dice: 1.1604, decode.d2.loss_cls: 0.3798, decode.d2.loss_mask: 0.6538, decode.d2.loss_dice: 1.1412, decode.d3.loss_cls: 0.3846, decode.d3.loss_mask: 0.6594, decode.d3.loss_dice: 1.1330, decode.d4.loss_cls: 0.3507, decode.d4.loss_mask: 0.6685, decode.d4.loss_dice: 1.1262, decode.d5.loss_cls: 0.3875, decode.d5.loss_mask: 0.6596, decode.d5.loss_dice: 1.1416, decode.d6.loss_cls: 0.3925, decode.d6.loss_mask: 0.6508, decode.d6.loss_dice: 1.1290, decode.d7.loss_cls: 0.4082, decode.d7.loss_mask: 0.6514, decode.d7.loss_dice: 1.1140, decode.d8.loss_cls: 0.3907, decode.d8.loss_mask: 0.6505, decode.d8.loss_dice: 1.1203, loss: 23.7596
2023-02-22 03:55:15,887 - mmseg - INFO - Iter [5550/80000]	lr: 1.336e-06, eta: 2 days, 9:54:50, time: 2.747, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3726, decode.loss_mask: 0.6350, decode.loss_dice: 1.1225, decode.d0.loss_cls: 2.2892, decode.d0.loss_mask: 0.6513, decode.d0.loss_dice: 1.2019, decode.d1.loss_cls: 0.4272, decode.d1.loss_mask: 0.6405, decode.d1.loss_dice: 1.1586, decode.d2.loss_cls: 0.4017, decode.d2.loss_mask: 0.6380, decode.d2.loss_dice: 1.1311, decode.d3.loss_cls: 0.3689, decode.d3.loss_mask: 0.6355, decode.d3.loss_dice: 1.1180, decode.d4.loss_cls: 0.3971, decode.d4.loss_mask: 0.6344, decode.d4.loss_dice: 1.1173, decode.d5.loss_cls: 0.4288, decode.d5.loss_mask: 0.6348, decode.d5.loss_dice: 1.1129, decode.d6.loss_cls: 0.3948, decode.d6.loss_mask: 0.6357, decode.d6.loss_dice: 1.1127, decode.d7.loss_cls: 0.3888, decode.d7.loss_mask: 0.6389, decode.d7.loss_dice: 1.1176, decode.d8.loss_cls: 0.3803, decode.d8.loss_mask: 0.6336, decode.d8.loss_dice: 1.1202, loss: 23.5402
2023-02-22 03:57:33,207 - mmseg - INFO - Iter [5600/80000]	lr: 1.335e-06, eta: 2 days, 9:51:54, time: 2.746, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3529, decode.loss_mask: 0.6370, decode.loss_dice: 1.1043, decode.d0.loss_cls: 2.2318, decode.d0.loss_mask: 0.6590, decode.d0.loss_dice: 1.1766, decode.d1.loss_cls: 0.3613, decode.d1.loss_mask: 0.6533, decode.d1.loss_dice: 1.1489, decode.d2.loss_cls: 0.3136, decode.d2.loss_mask: 0.6521, decode.d2.loss_dice: 1.1300, decode.d3.loss_cls: 0.3548, decode.d3.loss_mask: 0.6346, decode.d3.loss_dice: 1.1090, decode.d4.loss_cls: 0.3721, decode.d4.loss_mask: 0.6323, decode.d4.loss_dice: 1.1125, decode.d5.loss_cls: 0.3403, decode.d5.loss_mask: 0.6304, decode.d5.loss_dice: 1.0992, decode.d6.loss_cls: 0.3517, decode.d6.loss_mask: 0.6209, decode.d6.loss_dice: 1.0967, decode.d7.loss_cls: 0.3650, decode.d7.loss_mask: 0.6271, decode.d7.loss_dice: 1.0952, decode.d8.loss_cls: 0.3451, decode.d8.loss_mask: 0.6391, decode.d8.loss_dice: 1.1177, loss: 22.9646
2023-02-22 03:59:50,553 - mmseg - INFO - Iter [5650/80000]	lr: 1.334e-06, eta: 2 days, 9:48:59, time: 2.747, data_time: 0.024, memory: 31493, decode.loss_cls: 0.3281, decode.loss_mask: 0.6486, decode.loss_dice: 1.0523, decode.d0.loss_cls: 2.2371, decode.d0.loss_mask: 0.6820, decode.d0.loss_dice: 1.1376, decode.d1.loss_cls: 0.3642, decode.d1.loss_mask: 0.6500, decode.d1.loss_dice: 1.0975, decode.d2.loss_cls: 0.3602, decode.d2.loss_mask: 0.6529, decode.d2.loss_dice: 1.0825, decode.d3.loss_cls: 0.3628, decode.d3.loss_mask: 0.6418, decode.d3.loss_dice: 1.0596, decode.d4.loss_cls: 0.3566, decode.d4.loss_mask: 0.6491, decode.d4.loss_dice: 1.0732, decode.d5.loss_cls: 0.3438, decode.d5.loss_mask: 0.6506, decode.d5.loss_dice: 1.0628, decode.d6.loss_cls: 0.3432, decode.d6.loss_mask: 0.6492, decode.d6.loss_dice: 1.0633, decode.d7.loss_cls: 0.3681, decode.d7.loss_mask: 0.6501, decode.d7.loss_dice: 1.0552, decode.d8.loss_cls: 0.3356, decode.d8.loss_mask: 0.6470, decode.d8.loss_dice: 1.0690, loss: 22.6742
2023-02-22 04:02:07,675 - mmseg - INFO - Iter [5700/80000]	lr: 1.334e-06, eta: 2 days, 9:46:02, time: 2.742, data_time: 0.024, memory: 31493, decode.loss_cls: 0.4537, decode.loss_mask: 0.7152, decode.loss_dice: 1.1143, decode.d0.loss_cls: 2.2408, decode.d0.loss_mask: 0.7351, decode.d0.loss_dice: 1.1944, decode.d1.loss_cls: 0.4515, decode.d1.loss_mask: 0.7171, decode.d1.loss_dice: 1.1658, decode.d2.loss_cls: 0.4105, decode.d2.loss_mask: 0.7127, decode.d2.loss_dice: 1.1538, decode.d3.loss_cls: 0.4155, decode.d3.loss_mask: 0.7069, decode.d3.loss_dice: 1.1376, decode.d4.loss_cls: 0.4478, decode.d4.loss_mask: 0.7063, decode.d4.loss_dice: 1.1245, decode.d5.loss_cls: 0.4476, decode.d5.loss_mask: 0.6978, decode.d5.loss_dice: 1.1366, decode.d6.loss_cls: 0.4200, decode.d6.loss_mask: 0.7160, decode.d6.loss_dice: 1.1232, decode.d7.loss_cls: 0.4350, decode.d7.loss_mask: 0.7228, decode.d7.loss_dice: 1.1307, decode.d8.loss_cls: 0.4475, decode.d8.loss_mask: 0.7243, decode.d8.loss_dice: 1.1128, loss: 24.7179
2023-02-22 04:04:27,830 - mmseg - INFO - Iter [5750/80000]	lr: 1.333e-06, eta: 2 days, 9:43:44, time: 2.803, data_time: 0.074, memory: 31493, decode.loss_cls: 0.3515, decode.loss_mask: 0.6821, decode.loss_dice: 1.1729, decode.d0.loss_cls: 2.1975, decode.d0.loss_mask: 0.7138, decode.d0.loss_dice: 1.2308, decode.d1.loss_cls: 0.4431, decode.d1.loss_mask: 0.6992, decode.d1.loss_dice: 1.2102, decode.d2.loss_cls: 0.3833, decode.d2.loss_mask: 0.6997, decode.d2.loss_dice: 1.1845, decode.d3.loss_cls: 0.3982, decode.d3.loss_mask: 0.6792, decode.d3.loss_dice: 1.1646, decode.d4.loss_cls: 0.3824, decode.d4.loss_mask: 0.6832, decode.d4.loss_dice: 1.1562, decode.d5.loss_cls: 0.3534, decode.d5.loss_mask: 0.6709, decode.d5.loss_dice: 1.1682, decode.d6.loss_cls: 0.3436, decode.d6.loss_mask: 0.6844, decode.d6.loss_dice: 1.1595, decode.d7.loss_cls: 0.3699, decode.d7.loss_mask: 0.6799, decode.d7.loss_dice: 1.1530, decode.d8.loss_cls: 0.3620, decode.d8.loss_mask: 0.6770, decode.d8.loss_dice: 1.1469, loss: 24.2010
2023-02-22 04:06:45,315 - mmseg - INFO - Iter [5800/80000]	lr: 1.332e-06, eta: 2 days, 9:40:53, time: 2.750, data_time: 0.025, memory: 31493, decode.loss_cls: 0.2993, decode.loss_mask: 0.6286, decode.loss_dice: 1.0579, decode.d0.loss_cls: 2.1716, decode.d0.loss_mask: 0.6556, decode.d0.loss_dice: 1.1705, decode.d1.loss_cls: 0.3566, decode.d1.loss_mask: 0.6317, decode.d1.loss_dice: 1.1051, decode.d2.loss_cls: 0.3320, decode.d2.loss_mask: 0.6330, decode.d2.loss_dice: 1.0738, decode.d3.loss_cls: 0.3198, decode.d3.loss_mask: 0.6228, decode.d3.loss_dice: 1.0623, decode.d4.loss_cls: 0.3371, decode.d4.loss_mask: 0.6228, decode.d4.loss_dice: 1.0718, decode.d5.loss_cls: 0.3246, decode.d5.loss_mask: 0.6168, decode.d5.loss_dice: 1.0629, decode.d6.loss_cls: 0.3208, decode.d6.loss_mask: 0.6220, decode.d6.loss_dice: 1.0628, decode.d7.loss_cls: 0.3071, decode.d7.loss_mask: 0.6196, decode.d7.loss_dice: 1.0601, decode.d8.loss_cls: 0.3032, decode.d8.loss_mask: 0.6188, decode.d8.loss_dice: 1.0635, loss: 22.1347
2023-02-22 04:09:02,899 - mmseg - INFO - Iter [5850/80000]	lr: 1.331e-06, eta: 2 days, 9:38:03, time: 2.752, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3211, decode.loss_mask: 0.6461, decode.loss_dice: 1.1854, decode.d0.loss_cls: 2.1502, decode.d0.loss_mask: 0.6723, decode.d0.loss_dice: 1.2293, decode.d1.loss_cls: 0.3720, decode.d1.loss_mask: 0.6799, decode.d1.loss_dice: 1.2302, decode.d2.loss_cls: 0.3477, decode.d2.loss_mask: 0.6692, decode.d2.loss_dice: 1.1975, decode.d3.loss_cls: 0.3562, decode.d3.loss_mask: 0.6377, decode.d3.loss_dice: 1.1883, decode.d4.loss_cls: 0.3337, decode.d4.loss_mask: 0.6461, decode.d4.loss_dice: 1.1947, decode.d5.loss_cls: 0.3251, decode.d5.loss_mask: 0.6493, decode.d5.loss_dice: 1.2066, decode.d6.loss_cls: 0.3075, decode.d6.loss_mask: 0.6518, decode.d6.loss_dice: 1.2093, decode.d7.loss_cls: 0.3375, decode.d7.loss_mask: 0.6503, decode.d7.loss_dice: 1.1813, decode.d8.loss_cls: 0.3218, decode.d8.loss_mask: 0.6489, decode.d8.loss_dice: 1.1923, loss: 23.7394
2023-02-22 04:11:20,429 - mmseg - INFO - Iter [5900/80000]	lr: 1.330e-06, eta: 2 days, 9:35:13, time: 2.751, data_time: 0.026, memory: 31493, decode.loss_cls: 0.3847, decode.loss_mask: 0.6096, decode.loss_dice: 1.1039, decode.d0.loss_cls: 2.1799, decode.d0.loss_mask: 0.6157, decode.d0.loss_dice: 1.1729, decode.d1.loss_cls: 0.4266, decode.d1.loss_mask: 0.6052, decode.d1.loss_dice: 1.1464, decode.d2.loss_cls: 0.4200, decode.d2.loss_mask: 0.6047, decode.d2.loss_dice: 1.1101, decode.d3.loss_cls: 0.3820, decode.d3.loss_mask: 0.5949, decode.d3.loss_dice: 1.1007, decode.d4.loss_cls: 0.3790, decode.d4.loss_mask: 0.5877, decode.d4.loss_dice: 1.0867, decode.d5.loss_cls: 0.3904, decode.d5.loss_mask: 0.6016, decode.d5.loss_dice: 1.0948, decode.d6.loss_cls: 0.4003, decode.d6.loss_mask: 0.5965, decode.d6.loss_dice: 1.0990, decode.d7.loss_cls: 0.4013, decode.d7.loss_mask: 0.5934, decode.d7.loss_dice: 1.0817, decode.d8.loss_cls: 0.3896, decode.d8.loss_mask: 0.6205, decode.d8.loss_dice: 1.0851, loss: 22.8652
2023-02-22 04:13:38,254 - mmseg - INFO - Iter [5950/80000]	lr: 1.329e-06, eta: 2 days, 9:32:28, time: 2.756, data_time: 0.026, memory: 31493, decode.loss_cls: 0.3393, decode.loss_mask: 0.6486, decode.loss_dice: 1.1581, decode.d0.loss_cls: 2.1132, decode.d0.loss_mask: 0.6985, decode.d0.loss_dice: 1.2357, decode.d1.loss_cls: 0.3986, decode.d1.loss_mask: 0.6635, decode.d1.loss_dice: 1.1850, decode.d2.loss_cls: 0.3779, decode.d2.loss_mask: 0.6548, decode.d2.loss_dice: 1.1657, decode.d3.loss_cls: 0.3344, decode.d3.loss_mask: 0.6543, decode.d3.loss_dice: 1.1427, decode.d4.loss_cls: 0.3371, decode.d4.loss_mask: 0.6575, decode.d4.loss_dice: 1.1565, decode.d5.loss_cls: 0.3574, decode.d5.loss_mask: 0.6623, decode.d5.loss_dice: 1.1448, decode.d6.loss_cls: 0.3434, decode.d6.loss_mask: 0.6521, decode.d6.loss_dice: 1.1471, decode.d7.loss_cls: 0.3278, decode.d7.loss_mask: 0.6583, decode.d7.loss_dice: 1.1702, decode.d8.loss_cls: 0.3538, decode.d8.loss_mask: 0.6563, decode.d8.loss_dice: 1.1566, loss: 23.5514
2023-02-22 04:15:55,672 - mmseg - INFO - Saving checkpoint at 6000 iterations
2023-02-22 04:16:18,196 - mmseg - INFO - Exp name: my_city.py
2023-02-22 04:16:18,197 - mmseg - INFO - Iter [6000/80000]	lr: 1.328e-06, eta: 2 days, 9:34:16, time: 3.199, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4108, decode.loss_mask: 0.6930, decode.loss_dice: 1.1281, decode.d0.loss_cls: 2.1108, decode.d0.loss_mask: 0.7408, decode.d0.loss_dice: 1.1892, decode.d1.loss_cls: 0.4066, decode.d1.loss_mask: 0.6897, decode.d1.loss_dice: 1.1560, decode.d2.loss_cls: 0.3848, decode.d2.loss_mask: 0.6855, decode.d2.loss_dice: 1.1245, decode.d3.loss_cls: 0.4034, decode.d3.loss_mask: 0.6743, decode.d3.loss_dice: 1.1201, decode.d4.loss_cls: 0.4054, decode.d4.loss_mask: 0.6777, decode.d4.loss_dice: 1.1281, decode.d5.loss_cls: 0.4000, decode.d5.loss_mask: 0.6814, decode.d5.loss_dice: 1.1266, decode.d6.loss_cls: 0.3956, decode.d6.loss_mask: 0.6849, decode.d6.loss_dice: 1.1326, decode.d7.loss_cls: 0.3824, decode.d7.loss_mask: 0.6863, decode.d7.loss_dice: 1.1251, decode.d8.loss_cls: 0.4004, decode.d8.loss_mask: 0.6798, decode.d8.loss_dice: 1.1338, loss: 23.9577
2023-02-22 04:18:35,613 - mmseg - INFO - Iter [6050/80000]	lr: 1.327e-06, eta: 2 days, 9:31:23, time: 2.748, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3779, decode.loss_mask: 0.6143, decode.loss_dice: 1.1194, decode.d0.loss_cls: 2.0275, decode.d0.loss_mask: 0.6380, decode.d0.loss_dice: 1.1921, decode.d1.loss_cls: 0.3954, decode.d1.loss_mask: 0.6308, decode.d1.loss_dice: 1.1384, decode.d2.loss_cls: 0.3837, decode.d2.loss_mask: 0.6206, decode.d2.loss_dice: 1.1153, decode.d3.loss_cls: 0.3843, decode.d3.loss_mask: 0.6207, decode.d3.loss_dice: 1.1048, decode.d4.loss_cls: 0.4005, decode.d4.loss_mask: 0.6157, decode.d4.loss_dice: 1.1161, decode.d5.loss_cls: 0.3785, decode.d5.loss_mask: 0.6167, decode.d5.loss_dice: 1.1084, decode.d6.loss_cls: 0.3794, decode.d6.loss_mask: 0.6142, decode.d6.loss_dice: 1.1013, decode.d7.loss_cls: 0.3916, decode.d7.loss_mask: 0.6114, decode.d7.loss_dice: 1.0949, decode.d8.loss_cls: 0.3897, decode.d8.loss_mask: 0.6132, decode.d8.loss_dice: 1.1113, loss: 22.9059
2023-02-22 04:20:53,200 - mmseg - INFO - Iter [6100/80000]	lr: 1.326e-06, eta: 2 days, 9:28:34, time: 2.752, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3480, decode.loss_mask: 0.6106, decode.loss_dice: 0.9957, decode.d0.loss_cls: 2.0608, decode.d0.loss_mask: 0.6614, decode.d0.loss_dice: 1.1017, decode.d1.loss_cls: 0.3552, decode.d1.loss_mask: 0.6258, decode.d1.loss_dice: 1.0617, decode.d2.loss_cls: 0.3552, decode.d2.loss_mask: 0.6196, decode.d2.loss_dice: 1.0030, decode.d3.loss_cls: 0.3508, decode.d3.loss_mask: 0.6122, decode.d3.loss_dice: 1.0050, decode.d4.loss_cls: 0.3436, decode.d4.loss_mask: 0.6177, decode.d4.loss_dice: 1.0085, decode.d5.loss_cls: 0.3628, decode.d5.loss_mask: 0.6060, decode.d5.loss_dice: 1.0021, decode.d6.loss_cls: 0.3507, decode.d6.loss_mask: 0.6086, decode.d6.loss_dice: 0.9902, decode.d7.loss_cls: 0.3609, decode.d7.loss_mask: 0.6116, decode.d7.loss_dice: 0.9987, decode.d8.loss_cls: 0.3404, decode.d8.loss_mask: 0.6085, decode.d8.loss_dice: 0.9946, loss: 21.5715
2023-02-22 04:23:10,761 - mmseg - INFO - Iter [6150/80000]	lr: 1.325e-06, eta: 2 days, 9:25:45, time: 2.751, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3442, decode.loss_mask: 0.6475, decode.loss_dice: 1.1399, decode.d0.loss_cls: 2.0334, decode.d0.loss_mask: 0.6846, decode.d0.loss_dice: 1.2344, decode.d1.loss_cls: 0.3536, decode.d1.loss_mask: 0.6712, decode.d1.loss_dice: 1.1752, decode.d2.loss_cls: 0.3414, decode.d2.loss_mask: 0.6535, decode.d2.loss_dice: 1.1511, decode.d3.loss_cls: 0.3147, decode.d3.loss_mask: 0.6534, decode.d3.loss_dice: 1.1388, decode.d4.loss_cls: 0.3390, decode.d4.loss_mask: 0.6490, decode.d4.loss_dice: 1.1411, decode.d5.loss_cls: 0.3592, decode.d5.loss_mask: 0.6481, decode.d5.loss_dice: 1.1391, decode.d6.loss_cls: 0.3582, decode.d6.loss_mask: 0.6438, decode.d6.loss_dice: 1.1213, decode.d7.loss_cls: 0.3600, decode.d7.loss_mask: 0.6442, decode.d7.loss_dice: 1.1330, decode.d8.loss_cls: 0.3680, decode.d8.loss_mask: 0.6401, decode.d8.loss_dice: 1.1359, loss: 23.2169
2023-02-22 04:25:28,348 - mmseg - INFO - Iter [6200/80000]	lr: 1.325e-06, eta: 2 days, 9:22:56, time: 2.752, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3097, decode.loss_mask: 0.6170, decode.loss_dice: 1.0301, decode.d0.loss_cls: 2.0198, decode.d0.loss_mask: 0.6392, decode.d0.loss_dice: 1.0967, decode.d1.loss_cls: 0.3464, decode.d1.loss_mask: 0.6345, decode.d1.loss_dice: 1.0567, decode.d2.loss_cls: 0.3429, decode.d2.loss_mask: 0.6337, decode.d2.loss_dice: 1.0387, decode.d3.loss_cls: 0.3178, decode.d3.loss_mask: 0.6243, decode.d3.loss_dice: 1.0345, decode.d4.loss_cls: 0.3097, decode.d4.loss_mask: 0.6262, decode.d4.loss_dice: 1.0409, decode.d5.loss_cls: 0.3062, decode.d5.loss_mask: 0.6217, decode.d5.loss_dice: 1.0388, decode.d6.loss_cls: 0.3241, decode.d6.loss_mask: 0.6105, decode.d6.loss_dice: 1.0155, decode.d7.loss_cls: 0.3186, decode.d7.loss_mask: 0.6097, decode.d7.loss_dice: 1.0248, decode.d8.loss_cls: 0.3177, decode.d8.loss_mask: 0.6110, decode.d8.loss_dice: 1.0060, loss: 21.5237
2023-02-22 04:27:45,803 - mmseg - INFO - Iter [6250/80000]	lr: 1.324e-06, eta: 2 days, 9:20:07, time: 2.749, data_time: 0.024, memory: 31493, decode.loss_cls: 0.4385, decode.loss_mask: 0.5765, decode.loss_dice: 1.0563, decode.d0.loss_cls: 2.0330, decode.d0.loss_mask: 0.6280, decode.d0.loss_dice: 1.1382, decode.d1.loss_cls: 0.3997, decode.d1.loss_mask: 0.5984, decode.d1.loss_dice: 1.1093, decode.d2.loss_cls: 0.3632, decode.d2.loss_mask: 0.5906, decode.d2.loss_dice: 1.0873, decode.d3.loss_cls: 0.4016, decode.d3.loss_mask: 0.5819, decode.d3.loss_dice: 1.0527, decode.d4.loss_cls: 0.3898, decode.d4.loss_mask: 0.5856, decode.d4.loss_dice: 1.0584, decode.d5.loss_cls: 0.3843, decode.d5.loss_mask: 0.5858, decode.d5.loss_dice: 1.0581, decode.d6.loss_cls: 0.3771, decode.d6.loss_mask: 0.5799, decode.d6.loss_dice: 1.0707, decode.d7.loss_cls: 0.3969, decode.d7.loss_mask: 0.5794, decode.d7.loss_dice: 1.0509, decode.d8.loss_cls: 0.4060, decode.d8.loss_mask: 0.5784, decode.d8.loss_dice: 1.0686, loss: 22.2250
2023-02-22 04:30:03,266 - mmseg - INFO - Iter [6300/80000]	lr: 1.323e-06, eta: 2 days, 9:17:18, time: 2.749, data_time: 0.024, memory: 31493, decode.loss_cls: 0.3934, decode.loss_mask: 0.6532, decode.loss_dice: 1.1507, decode.d0.loss_cls: 2.0442, decode.d0.loss_mask: 0.6770, decode.d0.loss_dice: 1.2115, decode.d1.loss_cls: 0.3919, decode.d1.loss_mask: 0.6588, decode.d1.loss_dice: 1.1572, decode.d2.loss_cls: 0.3898, decode.d2.loss_mask: 0.6486, decode.d2.loss_dice: 1.1336, decode.d3.loss_cls: 0.4081, decode.d3.loss_mask: 0.6431, decode.d3.loss_dice: 1.1143, decode.d4.loss_cls: 0.4028, decode.d4.loss_mask: 0.6446, decode.d4.loss_dice: 1.1024, decode.d5.loss_cls: 0.4081, decode.d5.loss_mask: 0.6500, decode.d5.loss_dice: 1.1115, decode.d6.loss_cls: 0.3776, decode.d6.loss_mask: 0.6549, decode.d6.loss_dice: 1.1194, decode.d7.loss_cls: 0.3911, decode.d7.loss_mask: 0.6519, decode.d7.loss_dice: 1.1311, decode.d8.loss_cls: 0.3816, decode.d8.loss_mask: 0.6546, decode.d8.loss_dice: 1.1362, loss: 23.4931
2023-02-22 04:32:20,839 - mmseg - INFO - Iter [6350/80000]	lr: 1.322e-06, eta: 2 days, 9:14:31, time: 2.751, data_time: 0.025, memory: 31493, decode.loss_cls: 0.4077, decode.loss_mask: 0.6805, decode.loss_dice: 1.1645, decode.d0.loss_cls: 2.0333, decode.d0.loss_mask: 0.7156, decode.d0.loss_dice: 1.2672, decode.d1.loss_cls: 0.3763, decode.d1.loss_mask: 0.6968, decode.d1.loss_dice: 1.2255, decode.d2.loss_cls: 0.3859, decode.d2.loss_mask: 0.6803, decode.d2.loss_dice: 1.1792, decode.d3.loss_cls: 0.4064, decode.d3.loss_mask: 0.6777, decode.d3.loss_dice: 1.1780, decode.d4.loss_cls: 0.3853, decode.d4.loss_mask: 0.6825, decode.d4.loss_dice: 1.1746, decode.d5.loss_cls: 0.4089, decode.d5.loss_mask: 0.6742, decode.d5.loss_dice: 1.1616, decode.d6.loss_cls: 0.4161, decode.d6.loss_mask: 0.6772, decode.d6.loss_dice: 1.1662, decode.d7.loss_cls: 0.4092, decode.d7.loss_mask: 0.6784, decode.d7.loss_dice: 1.1743, decode.d8.loss_cls: 0.4178, decode.d8.loss_mask: 0.6776, decode.d8.loss_dice: 1.1801, loss: 24.3589
2023-02-22 04:34:38,102 - mmseg - INFO - Iter [6400/80000]	lr: 1.321e-06, eta: 2 days, 9:11:41, time: 2.745, data_time: 0.024, memory: 31493, decode.loss_cls: 0.4152, decode.loss_mask: 0.6626, decode.loss_dice: 1.1189, decode.d0.loss_cls: 2.0884, decode.d0.loss_mask: 0.6928, decode.d0.loss_dice: 1.2164, decode.d1.loss_cls: 0.4060, decode.d1.loss_mask: 0.6657, decode.d1.loss_dice: 1.1675, decode.d2.loss_cls: 0.3921, decode.d2.loss_mask: 0.6624, decode.d2.loss_dice: 1.1346, decode.d3.loss_cls: 0.4060, decode.d3.loss_mask: 0.6664, decode.d3.loss_dice: 1.1210, decode.d4.loss_cls: 0.4216, decode.d4.loss_mask: 0.6639, decode.d4.loss_dice: 1.1173, decode.d5.loss_cls: 0.4116, decode.d5.loss_mask: 0.6624, decode.d5.loss_dice: 1.1258, decode.d6.loss_cls: 0.4355, decode.d6.loss_mask: 0.6566, decode.d6.loss_dice: 1.1061, decode.d7.loss_cls: 0.4424, decode.d7.loss_mask: 0.6629, decode.d7.loss_dice: 1.1017, decode.d8.loss_cls: 0.4227, decode.d8.loss_mask: 0.6596, decode.d8.loss_dice: 1.0965, loss: 23.8027
2023-02-22 04:36:59,564 - mmseg - INFO - Iter [6450/80000]	lr: 1.320e-06, eta: 2 days, 9:09:39, time: 2.829, data_time: 0.075, memory: 31493, decode.loss_cls: 0.3712, decode.loss_mask: 0.6046, decode.loss_dice: 1.0631, decode.d0.loss_cls: 2.0559, decode.d0.loss_mask: 0.6370, decode.d0.loss_dice: 1.1883, decode.d1.loss_cls: 0.3656, decode.d1.loss_mask: 0.6199, decode.d1.loss_dice: 1.0950, decode.d2.loss_cls: 0.3761, decode.d2.loss_mask: 0.6050, decode.d2.loss_dice: 1.0620, decode.d3.loss_cls: 0.3704, decode.d3.loss_mask: 0.6036, decode.d3.loss_dice: 1.0586, decode.d4.loss_cls: 0.3522, decode.d4.loss_mask: 0.6132, decode.d4.loss_dice: 1.0577, decode.d5.loss_cls: 0.3825, decode.d5.loss_mask: 0.6011, decode.d5.loss_dice: 1.0508, decode.d6.loss_cls: 0.3917, decode.d6.loss_mask: 0.6008, decode.d6.loss_dice: 1.0619, decode.d7.loss_cls: 0.3926, decode.d7.loss_mask: 0.6043, decode.d7.loss_dice: 1.0583, decode.d8.loss_cls: 0.3850, decode.d8.loss_mask: 0.5987, decode.d8.loss_dice: 1.0600, loss: 22.2868
2023-02-22 04:39:27,193 - mmseg - INFO - Iter [6500/80000]	lr: 1.319e-06, eta: 2 days, 9:08:46, time: 2.953, data_time: 0.038, memory: 31493, decode.loss_cls: 0.3854, decode.loss_mask: 0.6073, decode.loss_dice: 1.0901, decode.d0.loss_cls: 2.0051, decode.d0.loss_mask: 0.6227, decode.d0.loss_dice: 1.1576, decode.d1.loss_cls: 0.3869, decode.d1.loss_mask: 0.6328, decode.d1.loss_dice: 1.1584, decode.d2.loss_cls: 0.3599, decode.d2.loss_mask: 0.6236, decode.d2.loss_dice: 1.1327, decode.d3.loss_cls: 0.3968, decode.d3.loss_mask: 0.6087, decode.d3.loss_dice: 1.1099, decode.d4.loss_cls: 0.4042, decode.d4.loss_mask: 0.6001, decode.d4.loss_dice: 1.0941, decode.d5.loss_cls: 0.4023, decode.d5.loss_mask: 0.6039, decode.d5.loss_dice: 1.0990, decode.d6.loss_cls: 0.3996, decode.d6.loss_mask: 0.6095, decode.d6.loss_dice: 1.0830, decode.d7.loss_cls: 0.3883, decode.d7.loss_mask: 0.6161, decode.d7.loss_dice: 1.0950, decode.d8.loss_cls: 0.3693, decode.d8.loss_mask: 0.6110, decode.d8.loss_dice: 1.1152, loss: 22.7688
2023-02-22 04:41:46,037 - mmseg - INFO - Iter [6550/80000]	lr: 1.318e-06, eta: 2 days, 9:06:14, time: 2.777, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3081, decode.loss_mask: 0.6290, decode.loss_dice: 1.0770, decode.d0.loss_cls: 1.9124, decode.d0.loss_mask: 0.6499, decode.d0.loss_dice: 1.1336, decode.d1.loss_cls: 0.2922, decode.d1.loss_mask: 0.6297, decode.d1.loss_dice: 1.1153, decode.d2.loss_cls: 0.3028, decode.d2.loss_mask: 0.6303, decode.d2.loss_dice: 1.0916, decode.d3.loss_cls: 0.3142, decode.d3.loss_mask: 0.6236, decode.d3.loss_dice: 1.0989, decode.d4.loss_cls: 0.3087, decode.d4.loss_mask: 0.6315, decode.d4.loss_dice: 1.1011, decode.d5.loss_cls: 0.3180, decode.d5.loss_mask: 0.6320, decode.d5.loss_dice: 1.0984, decode.d6.loss_cls: 0.3224, decode.d6.loss_mask: 0.6268, decode.d6.loss_dice: 1.0811, decode.d7.loss_cls: 0.3107, decode.d7.loss_mask: 0.6236, decode.d7.loss_dice: 1.0862, decode.d8.loss_cls: 0.3015, decode.d8.loss_mask: 0.6276, decode.d8.loss_dice: 1.0902, loss: 21.9684
2023-02-22 04:44:05,471 - mmseg - INFO - Iter [6600/80000]	lr: 1.317e-06, eta: 2 days, 9:03:48, time: 2.789, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3772, decode.loss_mask: 0.6723, decode.loss_dice: 1.1494, decode.d0.loss_cls: 1.9522, decode.d0.loss_mask: 0.7037, decode.d0.loss_dice: 1.2695, decode.d1.loss_cls: 0.3993, decode.d1.loss_mask: 0.6823, decode.d1.loss_dice: 1.2124, decode.d2.loss_cls: 0.4010, decode.d2.loss_mask: 0.6680, decode.d2.loss_dice: 1.1691, decode.d3.loss_cls: 0.3714, decode.d3.loss_mask: 0.6695, decode.d3.loss_dice: 1.1520, decode.d4.loss_cls: 0.3935, decode.d4.loss_mask: 0.6651, decode.d4.loss_dice: 1.1509, decode.d5.loss_cls: 0.3968, decode.d5.loss_mask: 0.6620, decode.d5.loss_dice: 1.1667, decode.d6.loss_cls: 0.3564, decode.d6.loss_mask: 0.6729, decode.d6.loss_dice: 1.1742, decode.d7.loss_cls: 0.3543, decode.d7.loss_mask: 0.6717, decode.d7.loss_dice: 1.1685, decode.d8.loss_cls: 0.3835, decode.d8.loss_mask: 0.6742, decode.d8.loss_dice: 1.1509, loss: 23.8908
2023-02-22 04:46:27,029 - mmseg - INFO - Iter [6650/80000]	lr: 1.316e-06, eta: 2 days, 9:01:46, time: 2.831, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3260, decode.loss_mask: 0.6086, decode.loss_dice: 1.0496, decode.d0.loss_cls: 1.9192, decode.d0.loss_mask: 0.6341, decode.d0.loss_dice: 1.1119, decode.d1.loss_cls: 0.3697, decode.d1.loss_mask: 0.6135, decode.d1.loss_dice: 1.1024, decode.d2.loss_cls: 0.3568, decode.d2.loss_mask: 0.6064, decode.d2.loss_dice: 1.0623, decode.d3.loss_cls: 0.3374, decode.d3.loss_mask: 0.6054, decode.d3.loss_dice: 1.0506, decode.d4.loss_cls: 0.3387, decode.d4.loss_mask: 0.6102, decode.d4.loss_dice: 1.0606, decode.d5.loss_cls: 0.3274, decode.d5.loss_mask: 0.6062, decode.d5.loss_dice: 1.0577, decode.d6.loss_cls: 0.3181, decode.d6.loss_mask: 0.6055, decode.d6.loss_dice: 1.0680, decode.d7.loss_cls: 0.3243, decode.d7.loss_mask: 0.6089, decode.d7.loss_dice: 1.0561, decode.d8.loss_cls: 0.3341, decode.d8.loss_mask: 0.6078, decode.d8.loss_dice: 1.0596, loss: 21.7369
2023-02-22 04:48:46,621 - mmseg - INFO - Iter [6700/80000]	lr: 1.316e-06, eta: 2 days, 8:59:22, time: 2.792, data_time: 0.027, memory: 31493, decode.loss_cls: 0.3147, decode.loss_mask: 0.6226, decode.loss_dice: 1.1105, decode.d0.loss_cls: 1.8967, decode.d0.loss_mask: 0.6581, decode.d0.loss_dice: 1.1637, decode.d1.loss_cls: 0.3548, decode.d1.loss_mask: 0.6365, decode.d1.loss_dice: 1.1622, decode.d2.loss_cls: 0.3462, decode.d2.loss_mask: 0.6303, decode.d2.loss_dice: 1.1292, decode.d3.loss_cls: 0.3378, decode.d3.loss_mask: 0.6321, decode.d3.loss_dice: 1.1261, decode.d4.loss_cls: 0.3558, decode.d4.loss_mask: 0.6172, decode.d4.loss_dice: 1.1054, decode.d5.loss_cls: 0.3282, decode.d5.loss_mask: 0.6146, decode.d5.loss_dice: 1.1089, decode.d6.loss_cls: 0.3305, decode.d6.loss_mask: 0.6092, decode.d6.loss_dice: 1.0913, decode.d7.loss_cls: 0.3216, decode.d7.loss_mask: 0.6172, decode.d7.loss_dice: 1.1076, decode.d8.loss_cls: 0.3590, decode.d8.loss_mask: 0.6196, decode.d8.loss_dice: 1.1001, loss: 22.4076
2023-02-22 04:51:04,255 - mmseg - INFO - Iter [6750/80000]	lr: 1.315e-06, eta: 2 days, 8:56:37, time: 2.753, data_time: 0.024, memory: 31493, decode.loss_cls: 0.3077, decode.loss_mask: 0.6452, decode.loss_dice: 1.0581, decode.d0.loss_cls: 1.8792, decode.d0.loss_mask: 0.6774, decode.d0.loss_dice: 1.0931, decode.d1.loss_cls: 0.3437, decode.d1.loss_mask: 0.6559, decode.d1.loss_dice: 1.0968, decode.d2.loss_cls: 0.3176, decode.d2.loss_mask: 0.6547, decode.d2.loss_dice: 1.0724, decode.d3.loss_cls: 0.3091, decode.d3.loss_mask: 0.6547, decode.d3.loss_dice: 1.0476, decode.d4.loss_cls: 0.3343, decode.d4.loss_mask: 0.6536, decode.d4.loss_dice: 1.0738, decode.d5.loss_cls: 0.3146, decode.d5.loss_mask: 0.6413, decode.d5.loss_dice: 1.0521, decode.d6.loss_cls: 0.3239, decode.d6.loss_mask: 0.6407, decode.d6.loss_dice: 1.0670, decode.d7.loss_cls: 0.3343, decode.d7.loss_mask: 0.6522, decode.d7.loss_dice: 1.0692, decode.d8.loss_cls: 0.3334, decode.d8.loss_mask: 0.6457, decode.d8.loss_dice: 1.0654, loss: 22.0146
2023-02-22 04:53:21,720 - mmseg - INFO - Iter [6800/80000]	lr: 1.314e-06, eta: 2 days, 8:53:51, time: 2.749, data_time: 0.025, memory: 31493, decode.loss_cls: 0.2535, decode.loss_mask: 0.6105, decode.loss_dice: 1.1173, decode.d0.loss_cls: 1.8271, decode.d0.loss_mask: 0.6223, decode.d0.loss_dice: 1.1670, decode.d1.loss_cls: 0.3204, decode.d1.loss_mask: 0.6095, decode.d1.loss_dice: 1.1622, decode.d2.loss_cls: 0.2657, decode.d2.loss_mask: 0.6107, decode.d2.loss_dice: 1.1401, decode.d3.loss_cls: 0.2716, decode.d3.loss_mask: 0.6052, decode.d3.loss_dice: 1.1168, decode.d4.loss_cls: 0.2938, decode.d4.loss_mask: 0.6025, decode.d4.loss_dice: 1.1178, decode.d5.loss_cls: 0.2781, decode.d5.loss_mask: 0.6092, decode.d5.loss_dice: 1.1200, decode.d6.loss_cls: 0.2613, decode.d6.loss_mask: 0.6177, decode.d6.loss_dice: 1.1159, decode.d7.loss_cls: 0.2714, decode.d7.loss_mask: 0.6114, decode.d7.loss_dice: 1.1124, decode.d8.loss_cls: 0.2567, decode.d8.loss_mask: 0.6143, decode.d8.loss_dice: 1.1208, loss: 21.7030
2023-02-22 04:55:38,935 - mmseg - INFO - Iter [6850/80000]	lr: 1.313e-06, eta: 2 days, 8:51:02, time: 2.744, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3845, decode.loss_mask: 0.6904, decode.loss_dice: 1.1265, decode.d0.loss_cls: 1.8862, decode.d0.loss_mask: 0.7138, decode.d0.loss_dice: 1.1686, decode.d1.loss_cls: 0.3784, decode.d1.loss_mask: 0.7065, decode.d1.loss_dice: 1.1869, decode.d2.loss_cls: 0.3780, decode.d2.loss_mask: 0.6881, decode.d2.loss_dice: 1.1567, decode.d3.loss_cls: 0.3954, decode.d3.loss_mask: 0.6816, decode.d3.loss_dice: 1.1288, decode.d4.loss_cls: 0.3893, decode.d4.loss_mask: 0.6762, decode.d4.loss_dice: 1.1376, decode.d5.loss_cls: 0.3952, decode.d5.loss_mask: 0.6735, decode.d5.loss_dice: 1.1324, decode.d6.loss_cls: 0.3950, decode.d6.loss_mask: 0.6806, decode.d6.loss_dice: 1.1240, decode.d7.loss_cls: 0.3597, decode.d7.loss_mask: 0.6907, decode.d7.loss_dice: 1.1318, decode.d8.loss_cls: 0.3506, decode.d8.loss_mask: 0.6965, decode.d8.loss_dice: 1.1282, loss: 23.6316
2023-02-22 04:57:56,520 - mmseg - INFO - Iter [6900/80000]	lr: 1.312e-06, eta: 2 days, 8:48:18, time: 2.752, data_time: 0.026, memory: 31493, decode.loss_cls: 0.3936, decode.loss_mask: 0.6113, decode.loss_dice: 1.0983, decode.d0.loss_cls: 1.8976, decode.d0.loss_mask: 0.6603, decode.d0.loss_dice: 1.1232, decode.d1.loss_cls: 0.3951, decode.d1.loss_mask: 0.6359, decode.d1.loss_dice: 1.1435, decode.d2.loss_cls: 0.3792, decode.d2.loss_mask: 0.6273, decode.d2.loss_dice: 1.1004, decode.d3.loss_cls: 0.3876, decode.d3.loss_mask: 0.6152, decode.d3.loss_dice: 1.1117, decode.d4.loss_cls: 0.4129, decode.d4.loss_mask: 0.6108, decode.d4.loss_dice: 1.1083, decode.d5.loss_cls: 0.3879, decode.d5.loss_mask: 0.6146, decode.d5.loss_dice: 1.1095, decode.d6.loss_cls: 0.3885, decode.d6.loss_mask: 0.6184, decode.d6.loss_dice: 1.1009, decode.d7.loss_cls: 0.3776, decode.d7.loss_mask: 0.6161, decode.d7.loss_dice: 1.0973, decode.d8.loss_cls: 0.3922, decode.d8.loss_mask: 0.6142, decode.d8.loss_dice: 1.1087, loss: 22.7382
2023-02-22 05:00:13,939 - mmseg - INFO - Iter [6950/80000]	lr: 1.311e-06, eta: 2 days, 8:45:32, time: 2.748, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3444, decode.loss_mask: 0.6653, decode.loss_dice: 1.1215, decode.d0.loss_cls: 1.8475, decode.d0.loss_mask: 0.6742, decode.d0.loss_dice: 1.1349, decode.d1.loss_cls: 0.3601, decode.d1.loss_mask: 0.6740, decode.d1.loss_dice: 1.1351, decode.d2.loss_cls: 0.3393, decode.d2.loss_mask: 0.6673, decode.d2.loss_dice: 1.1415, decode.d3.loss_cls: 0.3631, decode.d3.loss_mask: 0.6711, decode.d3.loss_dice: 1.1197, decode.d4.loss_cls: 0.3401, decode.d4.loss_mask: 0.6748, decode.d4.loss_dice: 1.1141, decode.d5.loss_cls: 0.3404, decode.d5.loss_mask: 0.6682, decode.d5.loss_dice: 1.1179, decode.d6.loss_cls: 0.3558, decode.d6.loss_mask: 0.6627, decode.d6.loss_dice: 1.1213, decode.d7.loss_cls: 0.3553, decode.d7.loss_mask: 0.6560, decode.d7.loss_dice: 1.1250, decode.d8.loss_cls: 0.3427, decode.d8.loss_mask: 0.6557, decode.d8.loss_dice: 1.1399, loss: 22.9288
2023-02-22 05:02:31,333 - mmseg - INFO - Saving checkpoint at 7000 iterations
2023-02-22 05:02:53,959 - mmseg - INFO - Exp name: my_city.py
2023-02-22 05:02:53,959 - mmseg - INFO - Iter [7000/80000]	lr: 1.310e-06, eta: 2 days, 8:46:42, time: 3.200, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3878, decode.loss_mask: 0.6503, decode.loss_dice: 1.0501, decode.d0.loss_cls: 1.8335, decode.d0.loss_mask: 0.6824, decode.d0.loss_dice: 1.1247, decode.d1.loss_cls: 0.4113, decode.d1.loss_mask: 0.6680, decode.d1.loss_dice: 1.1107, decode.d2.loss_cls: 0.3880, decode.d2.loss_mask: 0.6596, decode.d2.loss_dice: 1.0926, decode.d3.loss_cls: 0.3789, decode.d3.loss_mask: 0.6591, decode.d3.loss_dice: 1.0822, decode.d4.loss_cls: 0.3832, decode.d4.loss_mask: 0.6597, decode.d4.loss_dice: 1.0922, decode.d5.loss_cls: 0.3912, decode.d5.loss_mask: 0.6442, decode.d5.loss_dice: 1.0924, decode.d6.loss_cls: 0.3679, decode.d6.loss_mask: 0.6462, decode.d6.loss_dice: 1.0651, decode.d7.loss_cls: 0.3790, decode.d7.loss_mask: 0.6515, decode.d7.loss_dice: 1.0802, decode.d8.loss_cls: 0.3961, decode.d8.loss_mask: 0.6520, decode.d8.loss_dice: 1.0772, loss: 22.7573
2023-02-22 05:05:11,234 - mmseg - INFO - Iter [7050/80000]	lr: 1.309e-06, eta: 2 days, 8:43:54, time: 2.745, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3353, decode.loss_mask: 0.5849, decode.loss_dice: 1.0433, decode.d0.loss_cls: 1.7773, decode.d0.loss_mask: 0.5959, decode.d0.loss_dice: 1.1063, decode.d1.loss_cls: 0.3314, decode.d1.loss_mask: 0.5887, decode.d1.loss_dice: 1.1021, decode.d2.loss_cls: 0.3580, decode.d2.loss_mask: 0.5850, decode.d2.loss_dice: 1.0473, decode.d3.loss_cls: 0.3432, decode.d3.loss_mask: 0.5808, decode.d3.loss_dice: 1.0400, decode.d4.loss_cls: 0.3642, decode.d4.loss_mask: 0.5819, decode.d4.loss_dice: 1.0346, decode.d5.loss_cls: 0.3374, decode.d5.loss_mask: 0.5835, decode.d5.loss_dice: 1.0351, decode.d6.loss_cls: 0.3344, decode.d6.loss_mask: 0.5810, decode.d6.loss_dice: 1.0286, decode.d7.loss_cls: 0.3402, decode.d7.loss_mask: 0.5771, decode.d7.loss_dice: 1.0449, decode.d8.loss_cls: 0.3410, decode.d8.loss_mask: 0.5759, decode.d8.loss_dice: 1.0329, loss: 21.2120
2023-02-22 05:07:29,047 - mmseg - INFO - Iter [7100/80000]	lr: 1.308e-06, eta: 2 days, 8:41:12, time: 2.756, data_time: 0.026, memory: 31493, decode.loss_cls: 0.3233, decode.loss_mask: 0.6100, decode.loss_dice: 1.1630, decode.d0.loss_cls: 1.7971, decode.d0.loss_mask: 0.6288, decode.d0.loss_dice: 1.1885, decode.d1.loss_cls: 0.3277, decode.d1.loss_mask: 0.6318, decode.d1.loss_dice: 1.1854, decode.d2.loss_cls: 0.3361, decode.d2.loss_mask: 0.6308, decode.d2.loss_dice: 1.1498, decode.d3.loss_cls: 0.3374, decode.d3.loss_mask: 0.6255, decode.d3.loss_dice: 1.1475, decode.d4.loss_cls: 0.3420, decode.d4.loss_mask: 0.6255, decode.d4.loss_dice: 1.1390, decode.d5.loss_cls: 0.3597, decode.d5.loss_mask: 0.6237, decode.d5.loss_dice: 1.1390, decode.d6.loss_cls: 0.3264, decode.d6.loss_mask: 0.6301, decode.d6.loss_dice: 1.1585, decode.d7.loss_cls: 0.3432, decode.d7.loss_mask: 0.6250, decode.d7.loss_dice: 1.1434, decode.d8.loss_cls: 0.3540, decode.d8.loss_mask: 0.6257, decode.d8.loss_dice: 1.1514, loss: 22.6692
2023-02-22 05:09:48,875 - mmseg - INFO - Iter [7150/80000]	lr: 1.307e-06, eta: 2 days, 8:38:51, time: 2.797, data_time: 0.074, memory: 31493, decode.loss_cls: 0.3911, decode.loss_mask: 0.5659, decode.loss_dice: 1.0346, decode.d0.loss_cls: 1.8476, decode.d0.loss_mask: 0.5871, decode.d0.loss_dice: 1.0968, decode.d1.loss_cls: 0.3916, decode.d1.loss_mask: 0.5730, decode.d1.loss_dice: 1.0920, decode.d2.loss_cls: 0.3878, decode.d2.loss_mask: 0.5648, decode.d2.loss_dice: 1.0482, decode.d3.loss_cls: 0.4156, decode.d3.loss_mask: 0.5537, decode.d3.loss_dice: 1.0423, decode.d4.loss_cls: 0.3971, decode.d4.loss_mask: 0.5554, decode.d4.loss_dice: 1.0434, decode.d5.loss_cls: 0.3971, decode.d5.loss_mask: 0.5595, decode.d5.loss_dice: 1.0541, decode.d6.loss_cls: 0.3998, decode.d6.loss_mask: 0.5581, decode.d6.loss_dice: 1.0400, decode.d7.loss_cls: 0.3718, decode.d7.loss_mask: 0.5623, decode.d7.loss_dice: 1.0403, decode.d8.loss_cls: 0.3767, decode.d8.loss_mask: 0.5648, decode.d8.loss_dice: 1.0277, loss: 21.5402
2023-02-22 05:12:06,369 - mmseg - INFO - Iter [7200/80000]	lr: 1.307e-06, eta: 2 days, 8:36:06, time: 2.750, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3662, decode.loss_mask: 0.5713, decode.loss_dice: 1.0314, decode.d0.loss_cls: 1.8366, decode.d0.loss_mask: 0.6039, decode.d0.loss_dice: 1.1179, decode.d1.loss_cls: 0.4045, decode.d1.loss_mask: 0.5796, decode.d1.loss_dice: 1.0866, decode.d2.loss_cls: 0.3988, decode.d2.loss_mask: 0.5713, decode.d2.loss_dice: 1.0330, decode.d3.loss_cls: 0.3873, decode.d3.loss_mask: 0.5750, decode.d3.loss_dice: 1.0149, decode.d4.loss_cls: 0.3955, decode.d4.loss_mask: 0.5670, decode.d4.loss_dice: 1.0380, decode.d5.loss_cls: 0.3925, decode.d5.loss_mask: 0.5715, decode.d5.loss_dice: 1.0394, decode.d6.loss_cls: 0.3997, decode.d6.loss_mask: 0.5755, decode.d6.loss_dice: 1.0337, decode.d7.loss_cls: 0.3799, decode.d7.loss_mask: 0.5671, decode.d7.loss_dice: 1.0273, decode.d8.loss_cls: 0.3773, decode.d8.loss_mask: 0.5586, decode.d8.loss_dice: 1.0303, loss: 21.5314
2023-02-22 05:14:23,661 - mmseg - INFO - Iter [7250/80000]	lr: 1.306e-06, eta: 2 days, 8:33:19, time: 2.746, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3608, decode.loss_mask: 0.6682, decode.loss_dice: 1.1181, decode.d0.loss_cls: 1.8333, decode.d0.loss_mask: 0.6859, decode.d0.loss_dice: 1.1906, decode.d1.loss_cls: 0.3837, decode.d1.loss_mask: 0.6775, decode.d1.loss_dice: 1.1527, decode.d2.loss_cls: 0.3905, decode.d2.loss_mask: 0.6650, decode.d2.loss_dice: 1.1108, decode.d3.loss_cls: 0.3697, decode.d3.loss_mask: 0.6696, decode.d3.loss_dice: 1.1028, decode.d4.loss_cls: 0.3764, decode.d4.loss_mask: 0.6662, decode.d4.loss_dice: 1.1229, decode.d5.loss_cls: 0.3623, decode.d5.loss_mask: 0.6692, decode.d5.loss_dice: 1.1125, decode.d6.loss_cls: 0.3712, decode.d6.loss_mask: 0.6602, decode.d6.loss_dice: 1.0972, decode.d7.loss_cls: 0.3595, decode.d7.loss_mask: 0.6658, decode.d7.loss_dice: 1.1202, decode.d8.loss_cls: 0.3629, decode.d8.loss_mask: 0.6655, decode.d8.loss_dice: 1.1072, loss: 23.0986
2023-02-22 05:16:41,778 - mmseg - INFO - Iter [7300/80000]	lr: 1.305e-06, eta: 2 days, 8:30:41, time: 2.762, data_time: 0.026, memory: 31493, decode.loss_cls: 0.3146, decode.loss_mask: 0.5921, decode.loss_dice: 1.0716, decode.d0.loss_cls: 1.7841, decode.d0.loss_mask: 0.5970, decode.d0.loss_dice: 1.1152, decode.d1.loss_cls: 0.3089, decode.d1.loss_mask: 0.6140, decode.d1.loss_dice: 1.1262, decode.d2.loss_cls: 0.2893, decode.d2.loss_mask: 0.6109, decode.d2.loss_dice: 1.0877, decode.d3.loss_cls: 0.2932, decode.d3.loss_mask: 0.5963, decode.d3.loss_dice: 1.0970, decode.d4.loss_cls: 0.2901, decode.d4.loss_mask: 0.5992, decode.d4.loss_dice: 1.0915, decode.d5.loss_cls: 0.3013, decode.d5.loss_mask: 0.5921, decode.d5.loss_dice: 1.0690, decode.d6.loss_cls: 0.3076, decode.d6.loss_mask: 0.5946, decode.d6.loss_dice: 1.0814, decode.d7.loss_cls: 0.3004, decode.d7.loss_mask: 0.5952, decode.d7.loss_dice: 1.0732, decode.d8.loss_cls: 0.2906, decode.d8.loss_mask: 0.5970, decode.d8.loss_dice: 1.0777, loss: 21.3589
2023-02-22 05:18:59,163 - mmseg - INFO - Iter [7350/80000]	lr: 1.304e-06, eta: 2 days, 8:27:56, time: 2.748, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3642, decode.loss_mask: 0.6046, decode.loss_dice: 1.0643, decode.d0.loss_cls: 1.7889, decode.d0.loss_mask: 0.6499, decode.d0.loss_dice: 1.1258, decode.d1.loss_cls: 0.3626, decode.d1.loss_mask: 0.6107, decode.d1.loss_dice: 1.1104, decode.d2.loss_cls: 0.3582, decode.d2.loss_mask: 0.6136, decode.d2.loss_dice: 1.0831, decode.d3.loss_cls: 0.3702, decode.d3.loss_mask: 0.6040, decode.d3.loss_dice: 1.0637, decode.d4.loss_cls: 0.3750, decode.d4.loss_mask: 0.6010, decode.d4.loss_dice: 1.0572, decode.d5.loss_cls: 0.3820, decode.d5.loss_mask: 0.5978, decode.d5.loss_dice: 1.0608, decode.d6.loss_cls: 0.3855, decode.d6.loss_mask: 0.5980, decode.d6.loss_dice: 1.0587, decode.d7.loss_cls: 0.3978, decode.d7.loss_mask: 0.6016, decode.d7.loss_dice: 1.0735, decode.d8.loss_cls: 0.3837, decode.d8.loss_mask: 0.6022, decode.d8.loss_dice: 1.0596, loss: 22.0085
2023-02-22 05:21:16,904 - mmseg - INFO - Iter [7400/80000]	lr: 1.303e-06, eta: 2 days, 8:25:15, time: 2.755, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3043, decode.loss_mask: 0.5639, decode.loss_dice: 1.0961, decode.d0.loss_cls: 1.7659, decode.d0.loss_mask: 0.5752, decode.d0.loss_dice: 1.1659, decode.d1.loss_cls: 0.3357, decode.d1.loss_mask: 0.5741, decode.d1.loss_dice: 1.1166, decode.d2.loss_cls: 0.3547, decode.d2.loss_mask: 0.5642, decode.d2.loss_dice: 1.1087, decode.d3.loss_cls: 0.3624, decode.d3.loss_mask: 0.5685, decode.d3.loss_dice: 1.0946, decode.d4.loss_cls: 0.3287, decode.d4.loss_mask: 0.5701, decode.d4.loss_dice: 1.1014, decode.d5.loss_cls: 0.3121, decode.d5.loss_mask: 0.5685, decode.d5.loss_dice: 1.1028, decode.d6.loss_cls: 0.3107, decode.d6.loss_mask: 0.5680, decode.d6.loss_dice: 1.0915, decode.d7.loss_cls: 0.2911, decode.d7.loss_mask: 0.5679, decode.d7.loss_dice: 1.0988, decode.d8.loss_cls: 0.2892, decode.d8.loss_mask: 0.5718, decode.d8.loss_dice: 1.0884, loss: 21.4118
2023-02-22 05:23:34,314 - mmseg - INFO - Iter [7450/80000]	lr: 1.302e-06, eta: 2 days, 8:22:31, time: 2.748, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3448, decode.loss_mask: 0.6446, decode.loss_dice: 1.0638, decode.d0.loss_cls: 1.7538, decode.d0.loss_mask: 0.6787, decode.d0.loss_dice: 1.1353, decode.d1.loss_cls: 0.3918, decode.d1.loss_mask: 0.6638, decode.d1.loss_dice: 1.0956, decode.d2.loss_cls: 0.3459, decode.d2.loss_mask: 0.6537, decode.d2.loss_dice: 1.0636, decode.d3.loss_cls: 0.3414, decode.d3.loss_mask: 0.6493, decode.d3.loss_dice: 1.0541, decode.d4.loss_cls: 0.3239, decode.d4.loss_mask: 0.6578, decode.d4.loss_dice: 1.0658, decode.d5.loss_cls: 0.3183, decode.d5.loss_mask: 0.6523, decode.d5.loss_dice: 1.0654, decode.d6.loss_cls: 0.3523, decode.d6.loss_mask: 0.6444, decode.d6.loss_dice: 1.0659, decode.d7.loss_cls: 0.3600, decode.d7.loss_mask: 0.6534, decode.d7.loss_dice: 1.0596, decode.d8.loss_cls: 0.3436, decode.d8.loss_mask: 0.6413, decode.d8.loss_dice: 1.0657, loss: 22.1502
2023-02-22 05:25:51,906 - mmseg - INFO - Iter [7500/80000]	lr: 1.301e-06, eta: 2 days, 8:19:49, time: 2.752, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3424, decode.loss_mask: 0.5906, decode.loss_dice: 1.0638, decode.d0.loss_cls: 1.7457, decode.d0.loss_mask: 0.6179, decode.d0.loss_dice: 1.1299, decode.d1.loss_cls: 0.3422, decode.d1.loss_mask: 0.6043, decode.d1.loss_dice: 1.0805, decode.d2.loss_cls: 0.3341, decode.d2.loss_mask: 0.6002, decode.d2.loss_dice: 1.0793, decode.d3.loss_cls: 0.3466, decode.d3.loss_mask: 0.5941, decode.d3.loss_dice: 1.0595, decode.d4.loss_cls: 0.3432, decode.d4.loss_mask: 0.5942, decode.d4.loss_dice: 1.0777, decode.d5.loss_cls: 0.3508, decode.d5.loss_mask: 0.5909, decode.d5.loss_dice: 1.0813, decode.d6.loss_cls: 0.3415, decode.d6.loss_mask: 0.5889, decode.d6.loss_dice: 1.0737, decode.d7.loss_cls: 0.3372, decode.d7.loss_mask: 0.5882, decode.d7.loss_dice: 1.0584, decode.d8.loss_cls: 0.3504, decode.d8.loss_mask: 0.5901, decode.d8.loss_dice: 1.0828, loss: 21.5804
2023-02-22 05:28:09,715 - mmseg - INFO - Iter [7550/80000]	lr: 1.300e-06, eta: 2 days, 8:17:09, time: 2.756, data_time: 0.024, memory: 31493, decode.loss_cls: 0.4449, decode.loss_mask: 0.6422, decode.loss_dice: 1.0575, decode.d0.loss_cls: 1.7453, decode.d0.loss_mask: 0.6631, decode.d0.loss_dice: 1.1088, decode.d1.loss_cls: 0.4208, decode.d1.loss_mask: 0.6310, decode.d1.loss_dice: 1.0514, decode.d2.loss_cls: 0.4343, decode.d2.loss_mask: 0.6101, decode.d2.loss_dice: 1.0330, decode.d3.loss_cls: 0.4662, decode.d3.loss_mask: 0.6133, decode.d3.loss_dice: 1.0316, decode.d4.loss_cls: 0.4418, decode.d4.loss_mask: 0.6171, decode.d4.loss_dice: 1.0523, decode.d5.loss_cls: 0.4496, decode.d5.loss_mask: 0.6314, decode.d5.loss_dice: 1.0415, decode.d6.loss_cls: 0.4352, decode.d6.loss_mask: 0.6188, decode.d6.loss_dice: 1.0372, decode.d7.loss_cls: 0.4254, decode.d7.loss_mask: 0.6196, decode.d7.loss_dice: 1.0435, decode.d8.loss_cls: 0.4311, decode.d8.loss_mask: 0.6387, decode.d8.loss_dice: 1.0583, loss: 22.4951
2023-02-22 05:30:26,710 - mmseg - INFO - Iter [7600/80000]	lr: 1.299e-06, eta: 2 days, 8:14:22, time: 2.740, data_time: 0.023, memory: 31493, decode.loss_cls: 0.3393, decode.loss_mask: 0.7089, decode.loss_dice: 1.0891, decode.d0.loss_cls: 1.6915, decode.d0.loss_mask: 0.7570, decode.d0.loss_dice: 1.1203, decode.d1.loss_cls: 0.3936, decode.d1.loss_mask: 0.7105, decode.d1.loss_dice: 1.0976, decode.d2.loss_cls: 0.3296, decode.d2.loss_mask: 0.7087, decode.d2.loss_dice: 1.0813, decode.d3.loss_cls: 0.3660, decode.d3.loss_mask: 0.7142, decode.d3.loss_dice: 1.0544, decode.d4.loss_cls: 0.3652, decode.d4.loss_mask: 0.7001, decode.d4.loss_dice: 1.0697, decode.d5.loss_cls: 0.3719, decode.d5.loss_mask: 0.7028, decode.d5.loss_dice: 1.0736, decode.d6.loss_cls: 0.3706, decode.d6.loss_mask: 0.7048, decode.d6.loss_dice: 1.0580, decode.d7.loss_cls: 0.3530, decode.d7.loss_mask: 0.7084, decode.d7.loss_dice: 1.0678, decode.d8.loss_cls: 0.3322, decode.d8.loss_mask: 0.7108, decode.d8.loss_dice: 1.0767, loss: 22.8275
2023-02-22 05:32:43,757 - mmseg - INFO - Iter [7650/80000]	lr: 1.299e-06, eta: 2 days, 8:11:36, time: 2.741, data_time: 0.022, memory: 31493, decode.loss_cls: 0.3091, decode.loss_mask: 0.5884, decode.loss_dice: 0.9748, decode.d0.loss_cls: 1.6326, decode.d0.loss_mask: 0.6149, decode.d0.loss_dice: 1.0685, decode.d1.loss_cls: 0.3197, decode.d1.loss_mask: 0.6007, decode.d1.loss_dice: 1.0282, decode.d2.loss_cls: 0.3215, decode.d2.loss_mask: 0.5972, decode.d2.loss_dice: 1.0182, decode.d3.loss_cls: 0.2767, decode.d3.loss_mask: 0.6055, decode.d3.loss_dice: 0.9941, decode.d4.loss_cls: 0.2943, decode.d4.loss_mask: 0.6081, decode.d4.loss_dice: 0.9850, decode.d5.loss_cls: 0.2894, decode.d5.loss_mask: 0.6024, decode.d5.loss_dice: 0.9979, decode.d6.loss_cls: 0.2764, decode.d6.loss_mask: 0.5907, decode.d6.loss_dice: 0.9870, decode.d7.loss_cls: 0.2816, decode.d7.loss_mask: 0.5812, decode.d7.loss_dice: 0.9905, decode.d8.loss_cls: 0.2840, decode.d8.loss_mask: 0.5864, decode.d8.loss_dice: 0.9879, loss: 20.2930
2023-02-22 05:35:01,032 - mmseg - INFO - Iter [7700/80000]	lr: 1.298e-06, eta: 2 days, 8:08:53, time: 2.745, data_time: 0.022, memory: 31493, decode.loss_cls: 0.3424, decode.loss_mask: 0.6837, decode.loss_dice: 1.0994, decode.d0.loss_cls: 1.6394, decode.d0.loss_mask: 0.7216, decode.d0.loss_dice: 1.1654, decode.d1.loss_cls: 0.3997, decode.d1.loss_mask: 0.6896, decode.d1.loss_dice: 1.1149, decode.d2.loss_cls: 0.3639, decode.d2.loss_mask: 0.6756, decode.d2.loss_dice: 1.0835, decode.d3.loss_cls: 0.3443, decode.d3.loss_mask: 0.6899, decode.d3.loss_dice: 1.0952, decode.d4.loss_cls: 0.3730, decode.d4.loss_mask: 0.6852, decode.d4.loss_dice: 1.0903, decode.d5.loss_cls: 0.3761, decode.d5.loss_mask: 0.6814, decode.d5.loss_dice: 1.0841, decode.d6.loss_cls: 0.3742, decode.d6.loss_mask: 0.6698, decode.d6.loss_dice: 1.0632, decode.d7.loss_cls: 0.3741, decode.d7.loss_mask: 0.6785, decode.d7.loss_dice: 1.0684, decode.d8.loss_cls: 0.3605, decode.d8.loss_mask: 0.6786, decode.d8.loss_dice: 1.0949, loss: 22.7609
2023-02-22 05:37:20,280 - mmseg - INFO - Iter [7750/80000]	lr: 1.297e-06, eta: 2 days, 8:06:28, time: 2.785, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3754, decode.loss_mask: 0.6822, decode.loss_dice: 1.2303, decode.d0.loss_cls: 1.7088, decode.d0.loss_mask: 0.6979, decode.d0.loss_dice: 1.2754, decode.d1.loss_cls: 0.3559, decode.d1.loss_mask: 0.6912, decode.d1.loss_dice: 1.2451, decode.d2.loss_cls: 0.3750, decode.d2.loss_mask: 0.6836, decode.d2.loss_dice: 1.2156, decode.d3.loss_cls: 0.3909, decode.d3.loss_mask: 0.6781, decode.d3.loss_dice: 1.2151, decode.d4.loss_cls: 0.3616, decode.d4.loss_mask: 0.6842, decode.d4.loss_dice: 1.2364, decode.d5.loss_cls: 0.3584, decode.d5.loss_mask: 0.6835, decode.d5.loss_dice: 1.2159, decode.d6.loss_cls: 0.3809, decode.d6.loss_mask: 0.6780, decode.d6.loss_dice: 1.1943, decode.d7.loss_cls: 0.3690, decode.d7.loss_mask: 0.6830, decode.d7.loss_dice: 1.2213, decode.d8.loss_cls: 0.3889, decode.d8.loss_mask: 0.6796, decode.d8.loss_dice: 1.2329, loss: 24.1883
2023-02-22 05:39:47,315 - mmseg - INFO - Iter [7800/80000]	lr: 1.296e-06, eta: 2 days, 8:05:15, time: 2.941, data_time: 0.029, memory: 31493, decode.loss_cls: 0.3028, decode.loss_mask: 0.5882, decode.loss_dice: 1.0370, decode.d0.loss_cls: 1.6468, decode.d0.loss_mask: 0.6069, decode.d0.loss_dice: 1.1000, decode.d1.loss_cls: 0.3600, decode.d1.loss_mask: 0.5930, decode.d1.loss_dice: 1.0677, decode.d2.loss_cls: 0.3293, decode.d2.loss_mask: 0.5955, decode.d2.loss_dice: 1.0546, decode.d3.loss_cls: 0.2829, decode.d3.loss_mask: 0.5951, decode.d3.loss_dice: 1.0511, decode.d4.loss_cls: 0.2995, decode.d4.loss_mask: 0.5970, decode.d4.loss_dice: 1.0669, decode.d5.loss_cls: 0.3182, decode.d5.loss_mask: 0.5968, decode.d5.loss_dice: 1.0443, decode.d6.loss_cls: 0.3033, decode.d6.loss_mask: 0.6001, decode.d6.loss_dice: 1.0477, decode.d7.loss_cls: 0.3113, decode.d7.loss_mask: 0.5981, decode.d7.loss_dice: 1.0454, decode.d8.loss_cls: 0.3253, decode.d8.loss_mask: 0.5875, decode.d8.loss_dice: 1.0610, loss: 21.0134
2023-02-22 05:42:05,756 - mmseg - INFO - Iter [7850/80000]	lr: 1.295e-06, eta: 2 days, 8:02:43, time: 2.769, data_time: 0.023, memory: 31493, decode.loss_cls: 0.3164, decode.loss_mask: 0.6163, decode.loss_dice: 1.0375, decode.d0.loss_cls: 1.6286, decode.d0.loss_mask: 0.6306, decode.d0.loss_dice: 1.0759, decode.d1.loss_cls: 0.3420, decode.d1.loss_mask: 0.6363, decode.d1.loss_dice: 1.0833, decode.d2.loss_cls: 0.3375, decode.d2.loss_mask: 0.6221, decode.d2.loss_dice: 1.0597, decode.d3.loss_cls: 0.3237, decode.d3.loss_mask: 0.6163, decode.d3.loss_dice: 1.0240, decode.d4.loss_cls: 0.3390, decode.d4.loss_mask: 0.6126, decode.d4.loss_dice: 1.0143, decode.d5.loss_cls: 0.3450, decode.d5.loss_mask: 0.6097, decode.d5.loss_dice: 1.0099, decode.d6.loss_cls: 0.3446, decode.d6.loss_mask: 0.6155, decode.d6.loss_dice: 1.0283, decode.d7.loss_cls: 0.2965, decode.d7.loss_mask: 0.6126, decode.d7.loss_dice: 1.0395, decode.d8.loss_cls: 0.3363, decode.d8.loss_mask: 0.6134, decode.d8.loss_dice: 1.0277, loss: 21.1949
2023-02-22 05:44:26,675 - mmseg - INFO - Iter [7900/80000]	lr: 1.294e-06, eta: 2 days, 8:00:33, time: 2.818, data_time: 0.071, memory: 31493, decode.loss_cls: 0.3805, decode.loss_mask: 0.6251, decode.loss_dice: 1.0437, decode.d0.loss_cls: 1.6542, decode.d0.loss_mask: 0.6707, decode.d0.loss_dice: 1.1184, decode.d1.loss_cls: 0.4136, decode.d1.loss_mask: 0.6239, decode.d1.loss_dice: 1.0482, decode.d2.loss_cls: 0.4248, decode.d2.loss_mask: 0.6151, decode.d2.loss_dice: 1.0083, decode.d3.loss_cls: 0.3795, decode.d3.loss_mask: 0.6196, decode.d3.loss_dice: 0.9930, decode.d4.loss_cls: 0.3839, decode.d4.loss_mask: 0.6181, decode.d4.loss_dice: 1.0020, decode.d5.loss_cls: 0.3498, decode.d5.loss_mask: 0.6275, decode.d5.loss_dice: 1.0036, decode.d6.loss_cls: 0.3573, decode.d6.loss_mask: 0.6260, decode.d6.loss_dice: 1.0160, decode.d7.loss_cls: 0.3758, decode.d7.loss_mask: 0.6254, decode.d7.loss_dice: 1.0117, decode.d8.loss_cls: 0.3981, decode.d8.loss_mask: 0.6172, decode.d8.loss_dice: 1.0043, loss: 21.6356
2023-02-22 05:46:48,096 - mmseg - INFO - Iter [7950/80000]	lr: 1.293e-06, eta: 2 days, 7:58:27, time: 2.828, data_time: 0.024, memory: 31493, decode.loss_cls: 0.3241, decode.loss_mask: 0.6567, decode.loss_dice: 1.0590, decode.d0.loss_cls: 1.6342, decode.d0.loss_mask: 0.6785, decode.d0.loss_dice: 1.1246, decode.d1.loss_cls: 0.3591, decode.d1.loss_mask: 0.6531, decode.d1.loss_dice: 1.0875, decode.d2.loss_cls: 0.3157, decode.d2.loss_mask: 0.6550, decode.d2.loss_dice: 1.0577, decode.d3.loss_cls: 0.3232, decode.d3.loss_mask: 0.6535, decode.d3.loss_dice: 1.0553, decode.d4.loss_cls: 0.3336, decode.d4.loss_mask: 0.6554, decode.d4.loss_dice: 1.0530, decode.d5.loss_cls: 0.3377, decode.d5.loss_mask: 0.6520, decode.d5.loss_dice: 1.0441, decode.d6.loss_cls: 0.3129, decode.d6.loss_mask: 0.6470, decode.d6.loss_dice: 1.0554, decode.d7.loss_cls: 0.3320, decode.d7.loss_mask: 0.6470, decode.d7.loss_dice: 1.0645, decode.d8.loss_cls: 0.3291, decode.d8.loss_mask: 0.6513, decode.d8.loss_dice: 1.0548, loss: 21.8072
2023-02-22 05:49:08,026 - mmseg - INFO - Saving checkpoint at 8000 iterations
2023-02-22 05:49:31,537 - mmseg - INFO - Exp name: my_city.py
2023-02-22 05:49:31,538 - mmseg - INFO - Iter [8000/80000]	lr: 1.292e-06, eta: 2 days, 7:59:40, time: 3.269, data_time: 0.024, memory: 31493, decode.loss_cls: 0.3779, decode.loss_mask: 0.5729, decode.loss_dice: 1.0776, decode.d0.loss_cls: 1.6371, decode.d0.loss_mask: 0.6023, decode.d0.loss_dice: 1.1488, decode.d1.loss_cls: 0.3638, decode.d1.loss_mask: 0.5905, decode.d1.loss_dice: 1.1212, decode.d2.loss_cls: 0.3490, decode.d2.loss_mask: 0.5751, decode.d2.loss_dice: 1.0906, decode.d3.loss_cls: 0.3752, decode.d3.loss_mask: 0.5757, decode.d3.loss_dice: 1.0950, decode.d4.loss_cls: 0.3498, decode.d4.loss_mask: 0.5746, decode.d4.loss_dice: 1.0891, decode.d5.loss_cls: 0.3631, decode.d5.loss_mask: 0.5755, decode.d5.loss_dice: 1.0960, decode.d6.loss_cls: 0.3787, decode.d6.loss_mask: 0.5695, decode.d6.loss_dice: 1.0744, decode.d7.loss_cls: 0.3806, decode.d7.loss_mask: 0.5769, decode.d7.loss_dice: 1.0904, decode.d8.loss_cls: 0.3829, decode.d8.loss_mask: 0.5742, decode.d8.loss_dice: 1.0813, loss: 21.7095
2023-02-22 05:51:48,601 - mmseg - INFO - Iter [8050/80000]	lr: 1.291e-06, eta: 2 days, 7:56:54, time: 2.741, data_time: 0.022, memory: 31493, decode.loss_cls: 0.3514, decode.loss_mask: 0.5742, decode.loss_dice: 0.9881, decode.d0.loss_cls: 1.5733, decode.d0.loss_mask: 0.6090, decode.d0.loss_dice: 1.0660, decode.d1.loss_cls: 0.3824, decode.d1.loss_mask: 0.5890, decode.d1.loss_dice: 1.0097, decode.d2.loss_cls: 0.3601, decode.d2.loss_mask: 0.5801, decode.d2.loss_dice: 0.9984, decode.d3.loss_cls: 0.3569, decode.d3.loss_mask: 0.5730, decode.d3.loss_dice: 0.9735, decode.d4.loss_cls: 0.3668, decode.d4.loss_mask: 0.5660, decode.d4.loss_dice: 0.9626, decode.d5.loss_cls: 0.3393, decode.d5.loss_mask: 0.5688, decode.d5.loss_dice: 0.9801, decode.d6.loss_cls: 0.3433, decode.d6.loss_mask: 0.5691, decode.d6.loss_dice: 0.9794, decode.d7.loss_cls: 0.3638, decode.d7.loss_mask: 0.5737, decode.d7.loss_dice: 0.9720, decode.d8.loss_cls: 0.3622, decode.d8.loss_mask: 0.5753, decode.d8.loss_dice: 0.9859, loss: 20.4935
2023-02-22 05:54:05,683 - mmseg - INFO - Iter [8100/80000]	lr: 1.290e-06, eta: 2 days, 7:54:08, time: 2.742, data_time: 0.022, memory: 31493, decode.loss_cls: 0.3666, decode.loss_mask: 0.6645, decode.loss_dice: 1.1075, decode.d0.loss_cls: 1.6318, decode.d0.loss_mask: 0.6954, decode.d0.loss_dice: 1.1863, decode.d1.loss_cls: 0.3400, decode.d1.loss_mask: 0.6734, decode.d1.loss_dice: 1.1498, decode.d2.loss_cls: 0.3441, decode.d2.loss_mask: 0.6601, decode.d2.loss_dice: 1.1268, decode.d3.loss_cls: 0.3333, decode.d3.loss_mask: 0.6505, decode.d3.loss_dice: 1.1160, decode.d4.loss_cls: 0.3363, decode.d4.loss_mask: 0.6510, decode.d4.loss_dice: 1.0939, decode.d5.loss_cls: 0.3371, decode.d5.loss_mask: 0.6539, decode.d5.loss_dice: 1.0974, decode.d6.loss_cls: 0.3262, decode.d6.loss_mask: 0.6504, decode.d6.loss_dice: 1.1010, decode.d7.loss_cls: 0.3293, decode.d7.loss_mask: 0.6576, decode.d7.loss_dice: 1.1183, decode.d8.loss_cls: 0.3412, decode.d8.loss_mask: 0.6537, decode.d8.loss_dice: 1.1228, loss: 22.5161
2023-02-22 05:56:22,593 - mmseg - INFO - Iter [8150/80000]	lr: 1.290e-06, eta: 2 days, 7:51:22, time: 2.738, data_time: 0.023, memory: 31493, decode.loss_cls: 0.3402, decode.loss_mask: 0.6163, decode.loss_dice: 1.0973, decode.d0.loss_cls: 1.6071, decode.d0.loss_mask: 0.6371, decode.d0.loss_dice: 1.1371, decode.d1.loss_cls: 0.3473, decode.d1.loss_mask: 0.6312, decode.d1.loss_dice: 1.1322, decode.d2.loss_cls: 0.3288, decode.d2.loss_mask: 0.6222, decode.d2.loss_dice: 1.1193, decode.d3.loss_cls: 0.3597, decode.d3.loss_mask: 0.6167, decode.d3.loss_dice: 1.0896, decode.d4.loss_cls: 0.3364, decode.d4.loss_mask: 0.6174, decode.d4.loss_dice: 1.0811, decode.d5.loss_cls: 0.3406, decode.d5.loss_mask: 0.6197, decode.d5.loss_dice: 1.0966, decode.d6.loss_cls: 0.3518, decode.d6.loss_mask: 0.6102, decode.d6.loss_dice: 1.0695, decode.d7.loss_cls: 0.3552, decode.d7.loss_mask: 0.6152, decode.d7.loss_dice: 1.0924, decode.d8.loss_cls: 0.3196, decode.d8.loss_mask: 0.6214, decode.d8.loss_dice: 1.0838, loss: 21.8934
2023-02-22 05:58:39,529 - mmseg - INFO - Iter [8200/80000]	lr: 1.289e-06, eta: 2 days, 7:48:35, time: 2.739, data_time: 0.022, memory: 31493, decode.loss_cls: 0.3289, decode.loss_mask: 0.6699, decode.loss_dice: 1.0931, decode.d0.loss_cls: 1.5424, decode.d0.loss_mask: 0.7169, decode.d0.loss_dice: 1.1376, decode.d1.loss_cls: 0.3378, decode.d1.loss_mask: 0.6804, decode.d1.loss_dice: 1.1127, decode.d2.loss_cls: 0.3215, decode.d2.loss_mask: 0.6714, decode.d2.loss_dice: 1.1154, decode.d3.loss_cls: 0.3138, decode.d3.loss_mask: 0.6664, decode.d3.loss_dice: 1.1122, decode.d4.loss_cls: 0.2913, decode.d4.loss_mask: 0.6661, decode.d4.loss_dice: 1.1036, decode.d5.loss_cls: 0.3363, decode.d5.loss_mask: 0.6668, decode.d5.loss_dice: 1.0922, decode.d6.loss_cls: 0.3422, decode.d6.loss_mask: 0.6697, decode.d6.loss_dice: 1.0835, decode.d7.loss_cls: 0.3281, decode.d7.loss_mask: 0.6729, decode.d7.loss_dice: 1.0945, decode.d8.loss_cls: 0.3319, decode.d8.loss_mask: 0.6601, decode.d8.loss_dice: 1.0771, loss: 22.2368
2023-02-22 06:00:56,907 - mmseg - INFO - Iter [8250/80000]	lr: 1.288e-06, eta: 2 days, 7:45:53, time: 2.748, data_time: 0.024, memory: 31493, decode.loss_cls: 0.3532, decode.loss_mask: 0.6495, decode.loss_dice: 1.1181, decode.d0.loss_cls: 1.5841, decode.d0.loss_mask: 0.6924, decode.d0.loss_dice: 1.1708, decode.d1.loss_cls: 0.4279, decode.d1.loss_mask: 0.6627, decode.d1.loss_dice: 1.1414, decode.d2.loss_cls: 0.3844, decode.d2.loss_mask: 0.6523, decode.d2.loss_dice: 1.1182, decode.d3.loss_cls: 0.3722, decode.d3.loss_mask: 0.6477, decode.d3.loss_dice: 1.1138, decode.d4.loss_cls: 0.3496, decode.d4.loss_mask: 0.6508, decode.d4.loss_dice: 1.1040, decode.d5.loss_cls: 0.3848, decode.d5.loss_mask: 0.6409, decode.d5.loss_dice: 1.1040, decode.d6.loss_cls: 0.3795, decode.d6.loss_mask: 0.6395, decode.d6.loss_dice: 1.1063, decode.d7.loss_cls: 0.3716, decode.d7.loss_mask: 0.6467, decode.d7.loss_dice: 1.1127, decode.d8.loss_cls: 0.3769, decode.d8.loss_mask: 0.6450, decode.d8.loss_dice: 1.1210, loss: 22.7219
2023-02-22 06:03:14,210 - mmseg - INFO - Iter [8300/80000]	lr: 1.287e-06, eta: 2 days, 7:43:11, time: 2.746, data_time: 0.023, memory: 31493, decode.loss_cls: 0.3339, decode.loss_mask: 0.6164, decode.loss_dice: 1.0319, decode.d0.loss_cls: 1.5281, decode.d0.loss_mask: 0.6534, decode.d0.loss_dice: 1.0896, decode.d1.loss_cls: 0.2855, decode.d1.loss_mask: 0.6244, decode.d1.loss_dice: 1.0671, decode.d2.loss_cls: 0.3082, decode.d2.loss_mask: 0.6271, decode.d2.loss_dice: 1.0437, decode.d3.loss_cls: 0.3279, decode.d3.loss_mask: 0.6153, decode.d3.loss_dice: 1.0279, decode.d4.loss_cls: 0.3359, decode.d4.loss_mask: 0.6150, decode.d4.loss_dice: 1.0262, decode.d5.loss_cls: 0.3089, decode.d5.loss_mask: 0.6206, decode.d5.loss_dice: 1.0145, decode.d6.loss_cls: 0.3099, decode.d6.loss_mask: 0.6216, decode.d6.loss_dice: 1.0212, decode.d7.loss_cls: 0.3080, decode.d7.loss_mask: 0.6161, decode.d7.loss_dice: 1.0248, decode.d8.loss_cls: 0.3235, decode.d8.loss_mask: 0.6181, decode.d8.loss_dice: 1.0315, loss: 20.9761
2023-02-22 06:05:31,546 - mmseg - INFO - Iter [8350/80000]	lr: 1.286e-06, eta: 2 days, 7:40:29, time: 2.747, data_time: 0.023, memory: 31493, decode.loss_cls: 0.2868, decode.loss_mask: 0.6227, decode.loss_dice: 1.0753, decode.d0.loss_cls: 1.5973, decode.d0.loss_mask: 0.6591, decode.d0.loss_dice: 1.1544, decode.d1.loss_cls: 0.3387, decode.d1.loss_mask: 0.6293, decode.d1.loss_dice: 1.1221, decode.d2.loss_cls: 0.3293, decode.d2.loss_mask: 0.6205, decode.d2.loss_dice: 1.0831, decode.d3.loss_cls: 0.3067, decode.d3.loss_mask: 0.6172, decode.d3.loss_dice: 1.0695, decode.d4.loss_cls: 0.3019, decode.d4.loss_mask: 0.6194, decode.d4.loss_dice: 1.0743, decode.d5.loss_cls: 0.2897, decode.d5.loss_mask: 0.6185, decode.d5.loss_dice: 1.0846, decode.d6.loss_cls: 0.2837, decode.d6.loss_mask: 0.6173, decode.d6.loss_dice: 1.0839, decode.d7.loss_cls: 0.2646, decode.d7.loss_mask: 0.6154, decode.d7.loss_dice: 1.0980, decode.d8.loss_cls: 0.2707, decode.d8.loss_mask: 0.6225, decode.d8.loss_dice: 1.0961, loss: 21.4526
2023-02-22 06:07:48,680 - mmseg - INFO - Iter [8400/80000]	lr: 1.285e-06, eta: 2 days, 7:37:46, time: 2.743, data_time: 0.022, memory: 31493, decode.loss_cls: 0.2739, decode.loss_mask: 0.6452, decode.loss_dice: 1.0689, decode.d0.loss_cls: 1.5227, decode.d0.loss_mask: 0.6557, decode.d0.loss_dice: 1.1309, decode.d1.loss_cls: 0.2871, decode.d1.loss_mask: 0.6614, decode.d1.loss_dice: 1.1102, decode.d2.loss_cls: 0.2899, decode.d2.loss_mask: 0.6535, decode.d2.loss_dice: 1.0844, decode.d3.loss_cls: 0.3084, decode.d3.loss_mask: 0.6511, decode.d3.loss_dice: 1.0819, decode.d4.loss_cls: 0.3222, decode.d4.loss_mask: 0.6488, decode.d4.loss_dice: 1.0660, decode.d5.loss_cls: 0.3050, decode.d5.loss_mask: 0.6465, decode.d5.loss_dice: 1.0672, decode.d6.loss_cls: 0.2958, decode.d6.loss_mask: 0.6511, decode.d6.loss_dice: 1.0592, decode.d7.loss_cls: 0.2883, decode.d7.loss_mask: 0.6442, decode.d7.loss_dice: 1.0691, decode.d8.loss_cls: 0.2983, decode.d8.loss_mask: 0.6460, decode.d8.loss_dice: 1.0772, loss: 21.5099
2023-02-22 06:10:05,747 - mmseg - INFO - Iter [8450/80000]	lr: 1.284e-06, eta: 2 days, 7:35:03, time: 2.741, data_time: 0.022, memory: 31493, decode.loss_cls: 0.3009, decode.loss_mask: 0.5869, decode.loss_dice: 1.0248, decode.d0.loss_cls: 1.5760, decode.d0.loss_mask: 0.6186, decode.d0.loss_dice: 1.0831, decode.d1.loss_cls: 0.3377, decode.d1.loss_mask: 0.6015, decode.d1.loss_dice: 1.0400, decode.d2.loss_cls: 0.3283, decode.d2.loss_mask: 0.5874, decode.d2.loss_dice: 1.0126, decode.d3.loss_cls: 0.3391, decode.d3.loss_mask: 0.5969, decode.d3.loss_dice: 1.0150, decode.d4.loss_cls: 0.3197, decode.d4.loss_mask: 0.5883, decode.d4.loss_dice: 1.0188, decode.d5.loss_cls: 0.3376, decode.d5.loss_mask: 0.5881, decode.d5.loss_dice: 1.0044, decode.d6.loss_cls: 0.3234, decode.d6.loss_mask: 0.5903, decode.d6.loss_dice: 0.9939, decode.d7.loss_cls: 0.3223, decode.d7.loss_mask: 0.5965, decode.d7.loss_dice: 0.9949, decode.d8.loss_cls: 0.3073, decode.d8.loss_mask: 0.5890, decode.d8.loss_dice: 1.0108, loss: 20.6339
2023-02-22 06:12:22,700 - mmseg - INFO - Iter [8500/80000]	lr: 1.283e-06, eta: 2 days, 7:32:19, time: 2.739, data_time: 0.021, memory: 31493, decode.loss_cls: 0.3661, decode.loss_mask: 0.6268, decode.loss_dice: 1.1127, decode.d0.loss_cls: 1.5892, decode.d0.loss_mask: 0.6741, decode.d0.loss_dice: 1.1954, decode.d1.loss_cls: 0.3659, decode.d1.loss_mask: 0.6392, decode.d1.loss_dice: 1.1640, decode.d2.loss_cls: 0.3811, decode.d2.loss_mask: 0.6289, decode.d2.loss_dice: 1.1152, decode.d3.loss_cls: 0.3359, decode.d3.loss_mask: 0.6352, decode.d3.loss_dice: 1.1073, decode.d4.loss_cls: 0.3381, decode.d4.loss_mask: 0.6343, decode.d4.loss_dice: 1.1024, decode.d5.loss_cls: 0.3495, decode.d5.loss_mask: 0.6302, decode.d5.loss_dice: 1.1204, decode.d6.loss_cls: 0.3449, decode.d6.loss_mask: 0.6314, decode.d6.loss_dice: 1.1052, decode.d7.loss_cls: 0.3605, decode.d7.loss_mask: 0.6153, decode.d7.loss_dice: 1.1106, decode.d8.loss_cls: 0.3857, decode.d8.loss_mask: 0.6181, decode.d8.loss_dice: 1.0991, loss: 22.3829
2023-02-22 06:14:39,874 - mmseg - INFO - Iter [8550/80000]	lr: 1.282e-06, eta: 2 days, 7:29:37, time: 2.743, data_time: 0.023, memory: 31493, decode.loss_cls: 0.3299, decode.loss_mask: 0.6317, decode.loss_dice: 1.1049, decode.d0.loss_cls: 1.5168, decode.d0.loss_mask: 0.6585, decode.d0.loss_dice: 1.1694, decode.d1.loss_cls: 0.3342, decode.d1.loss_mask: 0.6449, decode.d1.loss_dice: 1.1409, decode.d2.loss_cls: 0.3225, decode.d2.loss_mask: 0.6322, decode.d2.loss_dice: 1.1223, decode.d3.loss_cls: 0.3218, decode.d3.loss_mask: 0.6254, decode.d3.loss_dice: 1.0911, decode.d4.loss_cls: 0.3095, decode.d4.loss_mask: 0.6326, decode.d4.loss_dice: 1.0994, decode.d5.loss_cls: 0.3196, decode.d5.loss_mask: 0.6287, decode.d5.loss_dice: 1.1022, decode.d6.loss_cls: 0.3002, decode.d6.loss_mask: 0.6278, decode.d6.loss_dice: 1.1094, decode.d7.loss_cls: 0.3260, decode.d7.loss_mask: 0.6287, decode.d7.loss_dice: 1.0826, decode.d8.loss_cls: 0.3108, decode.d8.loss_mask: 0.6271, decode.d8.loss_dice: 1.0893, loss: 21.8405
2023-02-22 06:16:59,536 - mmseg - INFO - Iter [8600/80000]	lr: 1.281e-06, eta: 2 days, 7:27:16, time: 2.793, data_time: 0.071, memory: 31493, decode.loss_cls: 0.3017, decode.loss_mask: 0.5787, decode.loss_dice: 1.0809, decode.d0.loss_cls: 1.5035, decode.d0.loss_mask: 0.6101, decode.d0.loss_dice: 1.1262, decode.d1.loss_cls: 0.3030, decode.d1.loss_mask: 0.5996, decode.d1.loss_dice: 1.1097, decode.d2.loss_cls: 0.2828, decode.d2.loss_mask: 0.5840, decode.d2.loss_dice: 1.0960, decode.d3.loss_cls: 0.2915, decode.d3.loss_mask: 0.5789, decode.d3.loss_dice: 1.0795, decode.d4.loss_cls: 0.2861, decode.d4.loss_mask: 0.5849, decode.d4.loss_dice: 1.0925, decode.d5.loss_cls: 0.3016, decode.d5.loss_mask: 0.5799, decode.d5.loss_dice: 1.0928, decode.d6.loss_cls: 0.3086, decode.d6.loss_mask: 0.5751, decode.d6.loss_dice: 1.0796, decode.d7.loss_cls: 0.3124, decode.d7.loss_mask: 0.5790, decode.d7.loss_dice: 1.0755, decode.d8.loss_cls: 0.2991, decode.d8.loss_mask: 0.5765, decode.d8.loss_dice: 1.0713, loss: 20.9410
2023-02-22 06:19:16,402 - mmseg - INFO - Iter [8650/80000]	lr: 1.281e-06, eta: 2 days, 7:24:32, time: 2.737, data_time: 0.022, memory: 31493, decode.loss_cls: 0.3111, decode.loss_mask: 0.6200, decode.loss_dice: 1.1043, decode.d0.loss_cls: 1.5066, decode.d0.loss_mask: 0.6431, decode.d0.loss_dice: 1.1564, decode.d1.loss_cls: 0.3643, decode.d1.loss_mask: 0.6248, decode.d1.loss_dice: 1.1353, decode.d2.loss_cls: 0.3381, decode.d2.loss_mask: 0.6255, decode.d2.loss_dice: 1.0974, decode.d3.loss_cls: 0.3319, decode.d3.loss_mask: 0.6208, decode.d3.loss_dice: 1.0957, decode.d4.loss_cls: 0.2960, decode.d4.loss_mask: 0.6286, decode.d4.loss_dice: 1.1067, decode.d5.loss_cls: 0.2938, decode.d5.loss_mask: 0.6206, decode.d5.loss_dice: 1.1192, decode.d6.loss_cls: 0.3448, decode.d6.loss_mask: 0.6214, decode.d6.loss_dice: 1.0735, decode.d7.loss_cls: 0.3166, decode.d7.loss_mask: 0.6237, decode.d7.loss_dice: 1.0828, decode.d8.loss_cls: 0.3169, decode.d8.loss_mask: 0.6161, decode.d8.loss_dice: 1.1017, loss: 21.7379
2023-02-22 06:21:33,475 - mmseg - INFO - Iter [8700/80000]	lr: 1.280e-06, eta: 2 days, 7:21:50, time: 2.741, data_time: 0.021, memory: 31493, decode.loss_cls: 0.4121, decode.loss_mask: 0.6931, decode.loss_dice: 1.1677, decode.d0.loss_cls: 1.6009, decode.d0.loss_mask: 0.7262, decode.d0.loss_dice: 1.2333, decode.d1.loss_cls: 0.3872, decode.d1.loss_mask: 0.7047, decode.d1.loss_dice: 1.2223, decode.d2.loss_cls: 0.4091, decode.d2.loss_mask: 0.6973, decode.d2.loss_dice: 1.1824, decode.d3.loss_cls: 0.4125, decode.d3.loss_mask: 0.6866, decode.d3.loss_dice: 1.1357, decode.d4.loss_cls: 0.4192, decode.d4.loss_mask: 0.6870, decode.d4.loss_dice: 1.1479, decode.d5.loss_cls: 0.4144, decode.d5.loss_mask: 0.6902, decode.d5.loss_dice: 1.1711, decode.d6.loss_cls: 0.4026, decode.d6.loss_mask: 0.6946, decode.d6.loss_dice: 1.1384, decode.d7.loss_cls: 0.4038, decode.d7.loss_mask: 0.6926, decode.d7.loss_dice: 1.1402, decode.d8.loss_cls: 0.4135, decode.d8.loss_mask: 0.6990, decode.d8.loss_dice: 1.1549, loss: 23.9404
2023-02-22 06:23:50,190 - mmseg - INFO - Iter [8750/80000]	lr: 1.279e-06, eta: 2 days, 7:19:05, time: 2.734, data_time: 0.022, memory: 31493, decode.loss_cls: 0.2914, decode.loss_mask: 0.6171, decode.loss_dice: 1.0956, decode.d0.loss_cls: 1.4772, decode.d0.loss_mask: 0.6260, decode.d0.loss_dice: 1.1924, decode.d1.loss_cls: 0.3138, decode.d1.loss_mask: 0.6131, decode.d1.loss_dice: 1.1502, decode.d2.loss_cls: 0.3070, decode.d2.loss_mask: 0.6093, decode.d2.loss_dice: 1.1074, decode.d3.loss_cls: 0.3292, decode.d3.loss_mask: 0.6044, decode.d3.loss_dice: 1.0948, decode.d4.loss_cls: 0.3062, decode.d4.loss_mask: 0.6014, decode.d4.loss_dice: 1.0881, decode.d5.loss_cls: 0.2838, decode.d5.loss_mask: 0.6057, decode.d5.loss_dice: 1.0896, decode.d6.loss_cls: 0.2823, decode.d6.loss_mask: 0.6050, decode.d6.loss_dice: 1.0745, decode.d7.loss_cls: 0.3041, decode.d7.loss_mask: 0.6043, decode.d7.loss_dice: 1.0879, decode.d8.loss_cls: 0.2879, decode.d8.loss_mask: 0.6066, decode.d8.loss_dice: 1.0876, loss: 21.3441
2023-02-22 06:26:07,417 - mmseg - INFO - Iter [8800/80000]	lr: 1.278e-06, eta: 2 days, 7:16:25, time: 2.745, data_time: 0.022, memory: 31493, decode.loss_cls: 0.3280, decode.loss_mask: 0.6123, decode.loss_dice: 1.1039, decode.d0.loss_cls: 1.5184, decode.d0.loss_mask: 0.6286, decode.d0.loss_dice: 1.1852, decode.d1.loss_cls: 0.3258, decode.d1.loss_mask: 0.6231, decode.d1.loss_dice: 1.1614, decode.d2.loss_cls: 0.3725, decode.d2.loss_mask: 0.6163, decode.d2.loss_dice: 1.1266, decode.d3.loss_cls: 0.3572, decode.d3.loss_mask: 0.6188, decode.d3.loss_dice: 1.1179, decode.d4.loss_cls: 0.3506, decode.d4.loss_mask: 0.6164, decode.d4.loss_dice: 1.1252, decode.d5.loss_cls: 0.3444, decode.d5.loss_mask: 0.6150, decode.d5.loss_dice: 1.1218, decode.d6.loss_cls: 0.3474, decode.d6.loss_mask: 0.6141, decode.d6.loss_dice: 1.1233, decode.d7.loss_cls: 0.3393, decode.d7.loss_mask: 0.6120, decode.d7.loss_dice: 1.1074, decode.d8.loss_cls: 0.3233, decode.d8.loss_mask: 0.6125, decode.d8.loss_dice: 1.1205, loss: 22.0693
2023-02-22 06:28:24,409 - mmseg - INFO - Iter [8850/80000]	lr: 1.277e-06, eta: 2 days, 7:13:43, time: 2.740, data_time: 0.022, memory: 31493, decode.loss_cls: 0.3397, decode.loss_mask: 0.5709, decode.loss_dice: 1.0073, decode.d0.loss_cls: 1.5430, decode.d0.loss_mask: 0.5988, decode.d0.loss_dice: 1.1044, decode.d1.loss_cls: 0.3274, decode.d1.loss_mask: 0.5910, decode.d1.loss_dice: 1.0666, decode.d2.loss_cls: 0.3280, decode.d2.loss_mask: 0.5846, decode.d2.loss_dice: 1.0380, decode.d3.loss_cls: 0.3333, decode.d3.loss_mask: 0.5685, decode.d3.loss_dice: 1.0019, decode.d4.loss_cls: 0.3314, decode.d4.loss_mask: 0.5668, decode.d4.loss_dice: 1.0179, decode.d5.loss_cls: 0.3093, decode.d5.loss_mask: 0.5680, decode.d5.loss_dice: 1.0145, decode.d6.loss_cls: 0.3270, decode.d6.loss_mask: 0.5658, decode.d6.loss_dice: 0.9926, decode.d7.loss_cls: 0.3261, decode.d7.loss_mask: 0.5681, decode.d7.loss_dice: 1.0015, decode.d8.loss_cls: 0.3532, decode.d8.loss_mask: 0.5677, decode.d8.loss_dice: 1.0145, loss: 20.5275
2023-02-22 06:30:41,554 - mmseg - INFO - Iter [8900/80000]	lr: 1.276e-06, eta: 2 days, 7:11:03, time: 2.743, data_time: 0.021, memory: 31493, decode.loss_cls: 0.3290, decode.loss_mask: 0.6001, decode.loss_dice: 1.0700, decode.d0.loss_cls: 1.4621, decode.d0.loss_mask: 0.6455, decode.d0.loss_dice: 1.1737, decode.d1.loss_cls: 0.3484, decode.d1.loss_mask: 0.6350, decode.d1.loss_dice: 1.1138, decode.d2.loss_cls: 0.3417, decode.d2.loss_mask: 0.6124, decode.d2.loss_dice: 1.0890, decode.d3.loss_cls: 0.3335, decode.d3.loss_mask: 0.6062, decode.d3.loss_dice: 1.0713, decode.d4.loss_cls: 0.3532, decode.d4.loss_mask: 0.6003, decode.d4.loss_dice: 1.0823, decode.d5.loss_cls: 0.3342, decode.d5.loss_mask: 0.6018, decode.d5.loss_dice: 1.0867, decode.d6.loss_cls: 0.3582, decode.d6.loss_mask: 0.5940, decode.d6.loss_dice: 1.0526, decode.d7.loss_cls: 0.3310, decode.d7.loss_mask: 0.5977, decode.d7.loss_dice: 1.0624, decode.d8.loss_cls: 0.3252, decode.d8.loss_mask: 0.6016, decode.d8.loss_dice: 1.0634, loss: 21.4764
2023-02-22 06:32:58,743 - mmseg - INFO - Iter [8950/80000]	lr: 1.275e-06, eta: 2 days, 7:08:23, time: 2.744, data_time: 0.023, memory: 31493, decode.loss_cls: 0.3295, decode.loss_mask: 0.6128, decode.loss_dice: 1.0802, decode.d0.loss_cls: 1.4879, decode.d0.loss_mask: 0.6487, decode.d0.loss_dice: 1.1610, decode.d1.loss_cls: 0.3774, decode.d1.loss_mask: 0.6220, decode.d1.loss_dice: 1.1142, decode.d2.loss_cls: 0.3466, decode.d2.loss_mask: 0.6092, decode.d2.loss_dice: 1.0815, decode.d3.loss_cls: 0.3382, decode.d3.loss_mask: 0.6203, decode.d3.loss_dice: 1.0887, decode.d4.loss_cls: 0.3410, decode.d4.loss_mask: 0.6166, decode.d4.loss_dice: 1.0927, decode.d5.loss_cls: 0.3424, decode.d5.loss_mask: 0.6217, decode.d5.loss_dice: 1.0867, decode.d6.loss_cls: 0.3184, decode.d6.loss_mask: 0.6146, decode.d6.loss_dice: 1.0558, decode.d7.loss_cls: 0.3119, decode.d7.loss_mask: 0.6139, decode.d7.loss_dice: 1.0778, decode.d8.loss_cls: 0.3303, decode.d8.loss_mask: 0.6188, decode.d8.loss_dice: 1.0666, loss: 21.6274
2023-02-22 06:35:19,199 - mmseg - INFO - Saving checkpoint at 9000 iterations
2023-02-22 06:35:40,296 - mmseg - INFO - Exp name: my_city.py
2023-02-22 06:35:40,296 - mmseg - INFO - Iter [9000/80000]	lr: 1.274e-06, eta: 2 days, 7:08:56, time: 3.231, data_time: 0.026, memory: 31493, decode.loss_cls: 0.3553, decode.loss_mask: 0.6615, decode.loss_dice: 1.0320, decode.d0.loss_cls: 1.5048, decode.d0.loss_mask: 0.6784, decode.d0.loss_dice: 1.1042, decode.d1.loss_cls: 0.3718, decode.d1.loss_mask: 0.6794, decode.d1.loss_dice: 1.0815, decode.d2.loss_cls: 0.3693, decode.d2.loss_mask: 0.6551, decode.d2.loss_dice: 1.0536, decode.d3.loss_cls: 0.3477, decode.d3.loss_mask: 0.6609, decode.d3.loss_dice: 1.0363, decode.d4.loss_cls: 0.3697, decode.d4.loss_mask: 0.6451, decode.d4.loss_dice: 1.0315, decode.d5.loss_cls: 0.3570, decode.d5.loss_mask: 0.6570, decode.d5.loss_dice: 1.0334, decode.d6.loss_cls: 0.3744, decode.d6.loss_mask: 0.6590, decode.d6.loss_dice: 1.0272, decode.d7.loss_cls: 0.3766, decode.d7.loss_mask: 0.6588, decode.d7.loss_dice: 1.0215, decode.d8.loss_cls: 0.3573, decode.d8.loss_mask: 0.6584, decode.d8.loss_dice: 1.0227, loss: 21.8414
2023-02-22 06:38:06,996 - mmseg - INFO - Iter [9050/80000]	lr: 1.273e-06, eta: 2 days, 7:07:30, time: 2.934, data_time: 0.030, memory: 31493, decode.loss_cls: 0.3382, decode.loss_mask: 0.6144, decode.loss_dice: 1.1049, decode.d0.loss_cls: 1.5160, decode.d0.loss_mask: 0.6521, decode.d0.loss_dice: 1.1536, decode.d1.loss_cls: 0.3502, decode.d1.loss_mask: 0.6350, decode.d1.loss_dice: 1.1465, decode.d2.loss_cls: 0.3199, decode.d2.loss_mask: 0.6303, decode.d2.loss_dice: 1.1025, decode.d3.loss_cls: 0.3451, decode.d3.loss_mask: 0.6177, decode.d3.loss_dice: 1.1022, decode.d4.loss_cls: 0.3513, decode.d4.loss_mask: 0.6177, decode.d4.loss_dice: 1.0834, decode.d5.loss_cls: 0.3195, decode.d5.loss_mask: 0.6266, decode.d5.loss_dice: 1.1059, decode.d6.loss_cls: 0.3287, decode.d6.loss_mask: 0.6165, decode.d6.loss_dice: 1.0950, decode.d7.loss_cls: 0.3243, decode.d7.loss_mask: 0.6198, decode.d7.loss_dice: 1.1027, decode.d8.loss_cls: 0.3242, decode.d8.loss_mask: 0.6189, decode.d8.loss_dice: 1.1060, loss: 21.8693
2023-02-22 06:40:25,272 - mmseg - INFO - Iter [9100/80000]	lr: 1.272e-06, eta: 2 days, 7:04:58, time: 2.766, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3410, decode.loss_mask: 0.5527, decode.loss_dice: 1.0132, decode.d0.loss_cls: 1.4701, decode.d0.loss_mask: 0.5722, decode.d0.loss_dice: 1.0818, decode.d1.loss_cls: 0.3675, decode.d1.loss_mask: 0.5661, decode.d1.loss_dice: 1.0534, decode.d2.loss_cls: 0.3460, decode.d2.loss_mask: 0.5554, decode.d2.loss_dice: 1.0221, decode.d3.loss_cls: 0.3375, decode.d3.loss_mask: 0.5514, decode.d3.loss_dice: 1.0075, decode.d4.loss_cls: 0.3210, decode.d4.loss_mask: 0.5532, decode.d4.loss_dice: 1.0149, decode.d5.loss_cls: 0.3259, decode.d5.loss_mask: 0.5527, decode.d5.loss_dice: 1.0191, decode.d6.loss_cls: 0.3359, decode.d6.loss_mask: 0.5561, decode.d6.loss_dice: 0.9999, decode.d7.loss_cls: 0.3317, decode.d7.loss_mask: 0.5573, decode.d7.loss_dice: 0.9965, decode.d8.loss_cls: 0.3443, decode.d8.loss_mask: 0.5523, decode.d8.loss_dice: 1.0117, loss: 20.3106
2023-02-22 06:42:45,146 - mmseg - INFO - Iter [9150/80000]	lr: 1.272e-06, eta: 2 days, 7:02:38, time: 2.797, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3753, decode.loss_mask: 0.6341, decode.loss_dice: 1.0321, decode.d0.loss_cls: 1.4444, decode.d0.loss_mask: 0.6730, decode.d0.loss_dice: 1.1214, decode.d1.loss_cls: 0.4056, decode.d1.loss_mask: 0.6525, decode.d1.loss_dice: 1.0749, decode.d2.loss_cls: 0.3764, decode.d2.loss_mask: 0.6411, decode.d2.loss_dice: 1.0405, decode.d3.loss_cls: 0.3519, decode.d3.loss_mask: 0.6476, decode.d3.loss_dice: 1.0408, decode.d4.loss_cls: 0.3419, decode.d4.loss_mask: 0.6474, decode.d4.loss_dice: 1.0341, decode.d5.loss_cls: 0.3678, decode.d5.loss_mask: 0.6524, decode.d5.loss_dice: 1.0364, decode.d6.loss_cls: 0.3592, decode.d6.loss_mask: 0.6483, decode.d6.loss_dice: 1.0392, decode.d7.loss_cls: 0.3672, decode.d7.loss_mask: 0.6504, decode.d7.loss_dice: 1.0247, decode.d8.loss_cls: 0.3534, decode.d8.loss_mask: 0.6492, decode.d8.loss_dice: 1.0425, loss: 21.7258
2023-02-22 06:45:07,137 - mmseg - INFO - Iter [9200/80000]	lr: 1.271e-06, eta: 2 days, 7:00:35, time: 2.840, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3056, decode.loss_mask: 0.6211, decode.loss_dice: 1.0400, decode.d0.loss_cls: 1.3935, decode.d0.loss_mask: 0.6649, decode.d0.loss_dice: 1.0632, decode.d1.loss_cls: 0.3005, decode.d1.loss_mask: 0.6508, decode.d1.loss_dice: 1.0578, decode.d2.loss_cls: 0.3088, decode.d2.loss_mask: 0.6290, decode.d2.loss_dice: 1.0436, decode.d3.loss_cls: 0.3219, decode.d3.loss_mask: 0.6337, decode.d3.loss_dice: 1.0315, decode.d4.loss_cls: 0.3106, decode.d4.loss_mask: 0.6450, decode.d4.loss_dice: 1.0496, decode.d5.loss_cls: 0.3142, decode.d5.loss_mask: 0.6337, decode.d5.loss_dice: 1.0423, decode.d6.loss_cls: 0.3025, decode.d6.loss_mask: 0.6303, decode.d6.loss_dice: 1.0230, decode.d7.loss_cls: 0.3377, decode.d7.loss_mask: 0.6254, decode.d7.loss_dice: 1.0303, decode.d8.loss_cls: 0.3259, decode.d8.loss_mask: 0.6246, decode.d8.loss_dice: 1.0241, loss: 20.9848
2023-02-22 06:47:24,623 - mmseg - INFO - Iter [9250/80000]	lr: 1.270e-06, eta: 2 days, 6:57:57, time: 2.750, data_time: 0.023, memory: 31493, decode.loss_cls: 0.3013, decode.loss_mask: 0.6308, decode.loss_dice: 1.0375, decode.d0.loss_cls: 1.4556, decode.d0.loss_mask: 0.6356, decode.d0.loss_dice: 1.1196, decode.d1.loss_cls: 0.3632, decode.d1.loss_mask: 0.6170, decode.d1.loss_dice: 1.0895, decode.d2.loss_cls: 0.3189, decode.d2.loss_mask: 0.6179, decode.d2.loss_dice: 1.0632, decode.d3.loss_cls: 0.3606, decode.d3.loss_mask: 0.5966, decode.d3.loss_dice: 1.0369, decode.d4.loss_cls: 0.3174, decode.d4.loss_mask: 0.6111, decode.d4.loss_dice: 1.0320, decode.d5.loss_cls: 0.3323, decode.d5.loss_mask: 0.6020, decode.d5.loss_dice: 1.0296, decode.d6.loss_cls: 0.3203, decode.d6.loss_mask: 0.6046, decode.d6.loss_dice: 1.0300, decode.d7.loss_cls: 0.3151, decode.d7.loss_mask: 0.6168, decode.d7.loss_dice: 1.0426, decode.d8.loss_cls: 0.3075, decode.d8.loss_mask: 0.6284, decode.d8.loss_dice: 1.0298, loss: 21.0636
2023-02-22 06:49:44,273 - mmseg - INFO - Iter [9300/80000]	lr: 1.269e-06, eta: 2 days, 6:55:36, time: 2.793, data_time: 0.073, memory: 31493, decode.loss_cls: 0.3618, decode.loss_mask: 0.6271, decode.loss_dice: 1.0999, decode.d0.loss_cls: 1.4316, decode.d0.loss_mask: 0.6670, decode.d0.loss_dice: 1.1891, decode.d1.loss_cls: 0.3765, decode.d1.loss_mask: 0.6260, decode.d1.loss_dice: 1.1324, decode.d2.loss_cls: 0.3710, decode.d2.loss_mask: 0.6130, decode.d2.loss_dice: 1.0926, decode.d3.loss_cls: 0.3400, decode.d3.loss_mask: 0.6471, decode.d3.loss_dice: 1.0892, decode.d4.loss_cls: 0.3481, decode.d4.loss_mask: 0.6505, decode.d4.loss_dice: 1.0864, decode.d5.loss_cls: 0.3544, decode.d5.loss_mask: 0.6490, decode.d5.loss_dice: 1.0926, decode.d6.loss_cls: 0.3406, decode.d6.loss_mask: 0.6277, decode.d6.loss_dice: 1.0906, decode.d7.loss_cls: 0.3545, decode.d7.loss_mask: 0.6260, decode.d7.loss_dice: 1.0876, decode.d8.loss_cls: 0.3514, decode.d8.loss_mask: 0.6280, decode.d8.loss_dice: 1.1042, loss: 22.0559
2023-02-22 06:52:01,710 - mmseg - INFO - Iter [9350/80000]	lr: 1.268e-06, eta: 2 days, 6:52:57, time: 2.749, data_time: 0.024, memory: 31493, decode.loss_cls: 0.3336, decode.loss_mask: 0.5871, decode.loss_dice: 1.0586, decode.d0.loss_cls: 1.4153, decode.d0.loss_mask: 0.6191, decode.d0.loss_dice: 1.1188, decode.d1.loss_cls: 0.3414, decode.d1.loss_mask: 0.6074, decode.d1.loss_dice: 1.0915, decode.d2.loss_cls: 0.3413, decode.d2.loss_mask: 0.6056, decode.d2.loss_dice: 1.0618, decode.d3.loss_cls: 0.3318, decode.d3.loss_mask: 0.5973, decode.d3.loss_dice: 1.0477, decode.d4.loss_cls: 0.3178, decode.d4.loss_mask: 0.5942, decode.d4.loss_dice: 1.0456, decode.d5.loss_cls: 0.3384, decode.d5.loss_mask: 0.5916, decode.d5.loss_dice: 1.0614, decode.d6.loss_cls: 0.3307, decode.d6.loss_mask: 0.5907, decode.d6.loss_dice: 1.0446, decode.d7.loss_cls: 0.3278, decode.d7.loss_mask: 0.5935, decode.d7.loss_dice: 1.0631, decode.d8.loss_cls: 0.3516, decode.d8.loss_mask: 0.5938, decode.d8.loss_dice: 1.0550, loss: 21.0579
2023-02-22 06:54:19,086 - mmseg - INFO - Iter [9400/80000]	lr: 1.267e-06, eta: 2 days, 6:50:19, time: 2.748, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3022, decode.loss_mask: 0.5803, decode.loss_dice: 1.0952, decode.d0.loss_cls: 1.4341, decode.d0.loss_mask: 0.6201, decode.d0.loss_dice: 1.1830, decode.d1.loss_cls: 0.3721, decode.d1.loss_mask: 0.5914, decode.d1.loss_dice: 1.1381, decode.d2.loss_cls: 0.3564, decode.d2.loss_mask: 0.5743, decode.d2.loss_dice: 1.0874, decode.d3.loss_cls: 0.3229, decode.d3.loss_mask: 0.5787, decode.d3.loss_dice: 1.0841, decode.d4.loss_cls: 0.3065, decode.d4.loss_mask: 0.5771, decode.d4.loss_dice: 1.0996, decode.d5.loss_cls: 0.3051, decode.d5.loss_mask: 0.5751, decode.d5.loss_dice: 1.0921, decode.d6.loss_cls: 0.2986, decode.d6.loss_mask: 0.5704, decode.d6.loss_dice: 1.1003, decode.d7.loss_cls: 0.2959, decode.d7.loss_mask: 0.5824, decode.d7.loss_dice: 1.0908, decode.d8.loss_cls: 0.3074, decode.d8.loss_mask: 0.5783, decode.d8.loss_dice: 1.0903, loss: 21.1903
2023-02-22 06:56:36,761 - mmseg - INFO - Iter [9450/80000]	lr: 1.266e-06, eta: 2 days, 6:47:43, time: 2.753, data_time: 0.024, memory: 31493, decode.loss_cls: 0.3106, decode.loss_mask: 0.5764, decode.loss_dice: 1.0577, decode.d0.loss_cls: 1.3855, decode.d0.loss_mask: 0.6134, decode.d0.loss_dice: 1.1552, decode.d1.loss_cls: 0.3544, decode.d1.loss_mask: 0.6005, decode.d1.loss_dice: 1.0835, decode.d2.loss_cls: 0.3277, decode.d2.loss_mask: 0.5942, decode.d2.loss_dice: 1.0520, decode.d3.loss_cls: 0.2956, decode.d3.loss_mask: 0.5855, decode.d3.loss_dice: 1.0573, decode.d4.loss_cls: 0.3098, decode.d4.loss_mask: 0.5843, decode.d4.loss_dice: 1.0649, decode.d5.loss_cls: 0.2957, decode.d5.loss_mask: 0.5864, decode.d5.loss_dice: 1.0585, decode.d6.loss_cls: 0.2913, decode.d6.loss_mask: 0.5790, decode.d6.loss_dice: 1.0505, decode.d7.loss_cls: 0.3086, decode.d7.loss_mask: 0.5790, decode.d7.loss_dice: 1.0662, decode.d8.loss_cls: 0.3133, decode.d8.loss_mask: 0.5814, decode.d8.loss_dice: 1.0504, loss: 20.7686
2023-02-22 06:58:54,041 - mmseg - INFO - Iter [9500/80000]	lr: 1.265e-06, eta: 2 days, 6:45:05, time: 2.746, data_time: 0.024, memory: 31493, decode.loss_cls: 0.3575, decode.loss_mask: 0.6066, decode.loss_dice: 1.0538, decode.d0.loss_cls: 1.4316, decode.d0.loss_mask: 0.6573, decode.d0.loss_dice: 1.1332, decode.d1.loss_cls: 0.3532, decode.d1.loss_mask: 0.6239, decode.d1.loss_dice: 1.1062, decode.d2.loss_cls: 0.3694, decode.d2.loss_mask: 0.6148, decode.d2.loss_dice: 1.0939, decode.d3.loss_cls: 0.3727, decode.d3.loss_mask: 0.6040, decode.d3.loss_dice: 1.0613, decode.d4.loss_cls: 0.3362, decode.d4.loss_mask: 0.6116, decode.d4.loss_dice: 1.0652, decode.d5.loss_cls: 0.3643, decode.d5.loss_mask: 0.6009, decode.d5.loss_dice: 1.0496, decode.d6.loss_cls: 0.3622, decode.d6.loss_mask: 0.6067, decode.d6.loss_dice: 1.0516, decode.d7.loss_cls: 0.3884, decode.d7.loss_mask: 0.5988, decode.d7.loss_dice: 1.0519, decode.d8.loss_cls: 0.3872, decode.d8.loss_mask: 0.5966, decode.d8.loss_dice: 1.0598, loss: 21.5705
2023-02-22 07:01:11,350 - mmseg - INFO - Iter [9550/80000]	lr: 1.264e-06, eta: 2 days, 6:42:27, time: 2.746, data_time: 0.024, memory: 31493, decode.loss_cls: 0.2898, decode.loss_mask: 0.5997, decode.loss_dice: 1.0167, decode.d0.loss_cls: 1.3945, decode.d0.loss_mask: 0.6226, decode.d0.loss_dice: 1.0831, decode.d1.loss_cls: 0.3350, decode.d1.loss_mask: 0.6096, decode.d1.loss_dice: 1.0519, decode.d2.loss_cls: 0.3106, decode.d2.loss_mask: 0.6215, decode.d2.loss_dice: 1.0156, decode.d3.loss_cls: 0.3016, decode.d3.loss_mask: 0.5959, decode.d3.loss_dice: 1.0111, decode.d4.loss_cls: 0.2917, decode.d4.loss_mask: 0.6039, decode.d4.loss_dice: 1.0086, decode.d5.loss_cls: 0.2951, decode.d5.loss_mask: 0.6080, decode.d5.loss_dice: 1.0118, decode.d6.loss_cls: 0.2812, decode.d6.loss_mask: 0.6112, decode.d6.loss_dice: 1.0167, decode.d7.loss_cls: 0.2991, decode.d7.loss_mask: 0.5944, decode.d7.loss_dice: 1.0196, decode.d8.loss_cls: 0.2977, decode.d8.loss_mask: 0.5965, decode.d8.loss_dice: 1.0239, loss: 20.4187
2023-02-22 07:03:28,843 - mmseg - INFO - Iter [9600/80000]	lr: 1.264e-06, eta: 2 days, 6:39:50, time: 2.750, data_time: 0.024, memory: 31493, decode.loss_cls: 0.3712, decode.loss_mask: 0.6084, decode.loss_dice: 1.0144, decode.d0.loss_cls: 1.3685, decode.d0.loss_mask: 0.6507, decode.d0.loss_dice: 1.0992, decode.d1.loss_cls: 0.3595, decode.d1.loss_mask: 0.6101, decode.d1.loss_dice: 1.0433, decode.d2.loss_cls: 0.3810, decode.d2.loss_mask: 0.6010, decode.d2.loss_dice: 1.0256, decode.d3.loss_cls: 0.3564, decode.d3.loss_mask: 0.6002, decode.d3.loss_dice: 1.0382, decode.d4.loss_cls: 0.3588, decode.d4.loss_mask: 0.5989, decode.d4.loss_dice: 1.0305, decode.d5.loss_cls: 0.3760, decode.d5.loss_mask: 0.6060, decode.d5.loss_dice: 1.0107, decode.d6.loss_cls: 0.3656, decode.d6.loss_mask: 0.6056, decode.d6.loss_dice: 1.0095, decode.d7.loss_cls: 0.3538, decode.d7.loss_mask: 0.6052, decode.d7.loss_dice: 1.0249, decode.d8.loss_cls: 0.3665, decode.d8.loss_mask: 0.6092, decode.d8.loss_dice: 1.0305, loss: 21.0796
2023-02-22 07:05:46,042 - mmseg - INFO - Iter [9650/80000]	lr: 1.263e-06, eta: 2 days, 6:37:12, time: 2.744, data_time: 0.024, memory: 31493, decode.loss_cls: 0.3051, decode.loss_mask: 0.6045, decode.loss_dice: 1.0063, decode.d0.loss_cls: 1.4170, decode.d0.loss_mask: 0.6444, decode.d0.loss_dice: 1.0801, decode.d1.loss_cls: 0.3609, decode.d1.loss_mask: 0.6271, decode.d1.loss_dice: 1.0275, decode.d2.loss_cls: 0.3366, decode.d2.loss_mask: 0.6117, decode.d2.loss_dice: 0.9955, decode.d3.loss_cls: 0.3702, decode.d3.loss_mask: 0.6046, decode.d3.loss_dice: 0.9799, decode.d4.loss_cls: 0.3385, decode.d4.loss_mask: 0.6134, decode.d4.loss_dice: 0.9880, decode.d5.loss_cls: 0.3469, decode.d5.loss_mask: 0.6059, decode.d5.loss_dice: 1.0034, decode.d6.loss_cls: 0.3247, decode.d6.loss_mask: 0.6054, decode.d6.loss_dice: 0.9976, decode.d7.loss_cls: 0.3127, decode.d7.loss_mask: 0.6082, decode.d7.loss_dice: 0.9973, decode.d8.loss_cls: 0.3146, decode.d8.loss_mask: 0.6042, decode.d8.loss_dice: 1.0087, loss: 20.6410
2023-02-22 07:08:03,524 - mmseg - INFO - Iter [9700/80000]	lr: 1.262e-06, eta: 2 days, 6:34:36, time: 2.750, data_time: 0.023, memory: 31493, decode.loss_cls: 0.2951, decode.loss_mask: 0.6267, decode.loss_dice: 1.0287, decode.d0.loss_cls: 1.3796, decode.d0.loss_mask: 0.6314, decode.d0.loss_dice: 1.1054, decode.d1.loss_cls: 0.3418, decode.d1.loss_mask: 0.6169, decode.d1.loss_dice: 1.0335, decode.d2.loss_cls: 0.3046, decode.d2.loss_mask: 0.6224, decode.d2.loss_dice: 1.0169, decode.d3.loss_cls: 0.2735, decode.d3.loss_mask: 0.6133, decode.d3.loss_dice: 1.0248, decode.d4.loss_cls: 0.2971, decode.d4.loss_mask: 0.6162, decode.d4.loss_dice: 1.0228, decode.d5.loss_cls: 0.2961, decode.d5.loss_mask: 0.6195, decode.d5.loss_dice: 1.0110, decode.d6.loss_cls: 0.2986, decode.d6.loss_mask: 0.6159, decode.d6.loss_dice: 1.0037, decode.d7.loss_cls: 0.2849, decode.d7.loss_mask: 0.6171, decode.d7.loss_dice: 1.0054, decode.d8.loss_cls: 0.2649, decode.d8.loss_mask: 0.6246, decode.d8.loss_dice: 1.0286, loss: 20.5212
2023-02-22 07:10:20,899 - mmseg - INFO - Iter [9750/80000]	lr: 1.261e-06, eta: 2 days, 6:31:59, time: 2.747, data_time: 0.024, memory: 31493, decode.loss_cls: 0.2866, decode.loss_mask: 0.6158, decode.loss_dice: 1.0114, decode.d0.loss_cls: 1.3770, decode.d0.loss_mask: 0.6454, decode.d0.loss_dice: 1.0924, decode.d1.loss_cls: 0.3073, decode.d1.loss_mask: 0.6374, decode.d1.loss_dice: 1.0581, decode.d2.loss_cls: 0.2961, decode.d2.loss_mask: 0.6286, decode.d2.loss_dice: 1.0126, decode.d3.loss_cls: 0.3133, decode.d3.loss_mask: 0.6187, decode.d3.loss_dice: 1.0134, decode.d4.loss_cls: 0.3079, decode.d4.loss_mask: 0.6114, decode.d4.loss_dice: 1.0164, decode.d5.loss_cls: 0.3108, decode.d5.loss_mask: 0.6164, decode.d5.loss_dice: 1.0159, decode.d6.loss_cls: 0.3051, decode.d6.loss_mask: 0.6143, decode.d6.loss_dice: 1.0054, decode.d7.loss_cls: 0.3028, decode.d7.loss_mask: 0.6116, decode.d7.loss_dice: 0.9970, decode.d8.loss_cls: 0.3024, decode.d8.loss_mask: 0.6106, decode.d8.loss_dice: 1.0164, loss: 20.5585
2023-02-22 07:12:38,269 - mmseg - INFO - Iter [9800/80000]	lr: 1.260e-06, eta: 2 days, 6:29:22, time: 2.747, data_time: 0.024, memory: 31493, decode.loss_cls: 0.3048, decode.loss_mask: 0.6016, decode.loss_dice: 1.0666, decode.d0.loss_cls: 1.3233, decode.d0.loss_mask: 0.6183, decode.d0.loss_dice: 1.1291, decode.d1.loss_cls: 0.3064, decode.d1.loss_mask: 0.6023, decode.d1.loss_dice: 1.0910, decode.d2.loss_cls: 0.2920, decode.d2.loss_mask: 0.5932, decode.d2.loss_dice: 1.0783, decode.d3.loss_cls: 0.3103, decode.d3.loss_mask: 0.5874, decode.d3.loss_dice: 1.0771, decode.d4.loss_cls: 0.2868, decode.d4.loss_mask: 0.6031, decode.d4.loss_dice: 1.0586, decode.d5.loss_cls: 0.3112, decode.d5.loss_mask: 0.5970, decode.d5.loss_dice: 1.0757, decode.d6.loss_cls: 0.3239, decode.d6.loss_mask: 0.5996, decode.d6.loss_dice: 1.0696, decode.d7.loss_cls: 0.3251, decode.d7.loss_mask: 0.5943, decode.d7.loss_dice: 1.0647, decode.d8.loss_cls: 0.3339, decode.d8.loss_mask: 0.5964, decode.d8.loss_dice: 1.0739, loss: 20.8956
2023-02-22 07:14:55,428 - mmseg - INFO - Iter [9850/80000]	lr: 1.259e-06, eta: 2 days, 6:26:44, time: 2.743, data_time: 0.024, memory: 31493, decode.loss_cls: 0.3038, decode.loss_mask: 0.6632, decode.loss_dice: 1.0937, decode.d0.loss_cls: 1.3418, decode.d0.loss_mask: 0.6877, decode.d0.loss_dice: 1.1635, decode.d1.loss_cls: 0.3476, decode.d1.loss_mask: 0.6899, decode.d1.loss_dice: 1.1331, decode.d2.loss_cls: 0.3135, decode.d2.loss_mask: 0.6744, decode.d2.loss_dice: 1.0911, decode.d3.loss_cls: 0.3432, decode.d3.loss_mask: 0.6708, decode.d3.loss_dice: 1.0854, decode.d4.loss_cls: 0.3012, decode.d4.loss_mask: 0.6741, decode.d4.loss_dice: 1.1089, decode.d5.loss_cls: 0.3147, decode.d5.loss_mask: 0.6831, decode.d5.loss_dice: 1.1011, decode.d6.loss_cls: 0.3024, decode.d6.loss_mask: 0.6751, decode.d6.loss_dice: 1.1184, decode.d7.loss_cls: 0.3166, decode.d7.loss_mask: 0.6727, decode.d7.loss_dice: 1.1049, decode.d8.loss_cls: 0.2990, decode.d8.loss_mask: 0.6747, decode.d8.loss_dice: 1.1122, loss: 22.0619
2023-02-22 07:17:12,701 - mmseg - INFO - Iter [9900/80000]	lr: 1.258e-06, eta: 2 days, 6:24:07, time: 2.745, data_time: 0.023, memory: 31493, decode.loss_cls: 0.3796, decode.loss_mask: 0.6371, decode.loss_dice: 1.0501, decode.d0.loss_cls: 1.3248, decode.d0.loss_mask: 0.6912, decode.d0.loss_dice: 1.1165, decode.d1.loss_cls: 0.3798, decode.d1.loss_mask: 0.6419, decode.d1.loss_dice: 1.0890, decode.d2.loss_cls: 0.3508, decode.d2.loss_mask: 0.6396, decode.d2.loss_dice: 1.0663, decode.d3.loss_cls: 0.3695, decode.d3.loss_mask: 0.6426, decode.d3.loss_dice: 1.0606, decode.d4.loss_cls: 0.3634, decode.d4.loss_mask: 0.6508, decode.d4.loss_dice: 1.0730, decode.d5.loss_cls: 0.3656, decode.d5.loss_mask: 0.6378, decode.d5.loss_dice: 1.0695, decode.d6.loss_cls: 0.3772, decode.d6.loss_mask: 0.6268, decode.d6.loss_dice: 1.0550, decode.d7.loss_cls: 0.3803, decode.d7.loss_mask: 0.6349, decode.d7.loss_dice: 1.0571, decode.d8.loss_cls: 0.3804, decode.d8.loss_mask: 0.6370, decode.d8.loss_dice: 1.0472, loss: 21.7955
2023-02-22 07:19:30,119 - mmseg - INFO - Iter [9950/80000]	lr: 1.257e-06, eta: 2 days, 6:21:32, time: 2.748, data_time: 0.025, memory: 31493, decode.loss_cls: 0.3300, decode.loss_mask: 0.6570, decode.loss_dice: 1.1117, decode.d0.loss_cls: 1.3868, decode.d0.loss_mask: 0.6744, decode.d0.loss_dice: 1.1851, decode.d1.loss_cls: 0.3283, decode.d1.loss_mask: 0.6896, decode.d1.loss_dice: 1.1602, decode.d2.loss_cls: 0.3052, decode.d2.loss_mask: 0.6664, decode.d2.loss_dice: 1.1359, decode.d3.loss_cls: 0.3329, decode.d3.loss_mask: 0.6586, decode.d3.loss_dice: 1.0982, decode.d4.loss_cls: 0.3340, decode.d4.loss_mask: 0.6493, decode.d4.loss_dice: 1.1168, decode.d5.loss_cls: 0.3424, decode.d5.loss_mask: 0.6571, decode.d5.loss_dice: 1.0931, decode.d6.loss_cls: 0.3216, decode.d6.loss_mask: 0.6718, decode.d6.loss_dice: 1.1371, decode.d7.loss_cls: 0.3148, decode.d7.loss_mask: 0.6592, decode.d7.loss_dice: 1.1408, decode.d8.loss_cls: 0.3227, decode.d8.loss_mask: 0.6713, decode.d8.loss_dice: 1.1180, loss: 22.2701
2023-02-22 07:21:49,889 - mmseg - INFO - Saving checkpoint at 10000 iterations
2023-02-22 07:22:16,964 - mmseg - INFO - Exp name: my_city.py
2023-02-22 07:22:16,965 - mmseg - INFO - Iter [10000/80000]	lr: 1.256e-06, eta: 2 days, 6:22:22, time: 3.337, data_time: 0.072, memory: 31493, decode.loss_cls: 0.3458, decode.loss_mask: 0.6064, decode.loss_dice: 1.0120, decode.d0.loss_cls: 1.3665, decode.d0.loss_mask: 0.6204, decode.d0.loss_dice: 1.0981, decode.d1.loss_cls: 0.3607, decode.d1.loss_mask: 0.6142, decode.d1.loss_dice: 1.0494, decode.d2.loss_cls: 0.3267, decode.d2.loss_mask: 0.6046, decode.d2.loss_dice: 1.0161, decode.d3.loss_cls: 0.3314, decode.d3.loss_mask: 0.5937, decode.d3.loss_dice: 1.0038, decode.d4.loss_cls: 0.3583, decode.d4.loss_mask: 0.5881, decode.d4.loss_dice: 1.0037, decode.d5.loss_cls: 0.3409, decode.d5.loss_mask: 0.5918, decode.d5.loss_dice: 1.0165, decode.d6.loss_cls: 0.3330, decode.d6.loss_mask: 0.5977, decode.d6.loss_dice: 1.0213, decode.d7.loss_cls: 0.3295, decode.d7.loss_mask: 0.6077, decode.d7.loss_dice: 1.0190, decode.d8.loss_cls: 0.3272, decode.d8.loss_mask: 0.6096, decode.d8.loss_dice: 1.0207, loss: 20.7148
[                                                  ] 0/357, elapsed: 0s, ETA:[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[                                ] 1/357, 0.0 task/s, elapsed: 66s, ETA: 23666s[                               ] 2/357, 0.0 task/s, elapsed: 129s, ETA: 22882s[                               ] 3/357, 0.0 task/s, elapsed: 191s, ETA: 22541s[                               ] 4/357, 0.0 task/s, elapsed: 253s, ETA: 22356s[                               ] 5/357, 0.0 task/s, elapsed: 316s, ETA: 22227s[                               ] 6/357, 0.0 task/s, elapsed: 382s, ETA: 22353s[                               ] 7/357, 0.0 task/s, elapsed: 448s, ETA: 22410s[                               ] 8/357, 0.0 task/s, elapsed: 514s, ETA: 22443s[                               ] 9/357, 0.0 task/s, elapsed: 577s, ETA: 22301s[                              ] 10/357, 0.0 task/s, elapsed: 640s, ETA: 22196s[                              ] 11/357, 0.0 task/s, elapsed: 709s, ETA: 22303s[>                             ] 12/357, 0.0 task/s, elapsed: 771s, ETA: 22178s[>                             ] 13/357, 0.0 task/s, elapsed: 838s, ETA: 22186s[>                             ] 14/357, 0.0 task/s, elapsed: 906s, ETA: 22203s[>                             ] 15/357, 0.0 task/s, elapsed: 973s, ETA: 22176s[>                            ] 16/357, 0.0 task/s, elapsed: 1041s, ETA: 22183s[>                            ] 17/357, 0.0 task/s, elapsed: 1109s, ETA: 22177s[>                            ] 18/357, 0.0 task/s, elapsed: 1171s, ETA: 22056s[>                            ] 19/357, 0.0 task/s, elapsed: 1234s, ETA: 21951s[>                            ] 20/357, 0.0 task/s, elapsed: 1301s, ETA: 21920s[>                            ] 21/357, 0.0 task/s, elapsed: 1367s, ETA: 21873s[>                            ] 22/357, 0.0 task/s, elapsed: 1430s, ETA: 21769s[>                            ] 23/357, 0.0 task/s, elapsed: 1492s, ETA: 21665s[>                            ] 24/357, 0.0 task/s, elapsed: 1554s, ETA: 21562s[>>                           ] 25/357, 0.0 task/s, elapsed: 1616s, ETA: 21461s[>>                           ] 26/357, 0.0 task/s, elapsed: 1678s, ETA: 21363s[>>                           ] 27/357, 0.0 task/s, elapsed: 1740s, ETA: 21269s[>>                           ] 28/357, 0.0 task/s, elapsed: 1802s, ETA: 21175s[>>                           ] 29/357, 0.0 task/s, elapsed: 1864s, ETA: 21085s[>>                           ] 30/357, 0.0 task/s, elapsed: 1926s, ETA: 20995s[>>                           ] 31/357, 0.0 task/s, elapsed: 1992s, ETA: 20949s[>>                           ] 32/357, 0.0 task/s, elapsed: 2058s, ETA: 20902s[>>                           ] 33/357, 0.0 task/s, elapsed: 2124s, ETA: 20854s[>>                           ] 34/357, 0.0 task/s, elapsed: 2192s, ETA: 20825s[>>                           ] 35/357, 0.0 task/s, elapsed: 2260s, ETA: 20794s[>>                           ] 36/357, 0.0 task/s, elapsed: 2328s, ETA: 20760s[>>>                          ] 37/357, 0.0 task/s, elapsed: 2396s, ETA: 20724s[>>>                          ] 38/357, 0.0 task/s, elapsed: 2462s, ETA: 20669s[>>>                          ] 39/357, 0.0 task/s, elapsed: 2528s, ETA: 20613s[>>>                          ] 40/357, 0.0 task/s, elapsed: 2590s, ETA: 20526s[>>>                          ] 41/357, 0.0 task/s, elapsed: 2652s, ETA: 20440s[>>>                          ] 42/357, 0.0 task/s, elapsed: 2718s, ETA: 20384s[>>>                          ] 43/357, 0.0 task/s, elapsed: 2780s, ETA: 20300s[>>>                          ] 44/357, 0.0 task/s, elapsed: 2842s, ETA: 20218s[>>>                          ] 45/357, 0.0 task/s, elapsed: 2904s, ETA: 20134s[>>>                          ] 46/357, 0.0 task/s, elapsed: 2970s, ETA: 20078s[>>>                          ] 47/357, 0.0 task/s, elapsed: 3036s, ETA: 20023s[>>>                          ] 48/357, 0.0 task/s, elapsed: 3098s, ETA: 19942s[>>>                          ] 49/357, 0.0 task/s, elapsed: 3160s, ETA: 19861s[>>>>                         ] 50/357, 0.0 task/s, elapsed: 3226s, ETA: 19805s[>>>>                         ] 51/357, 0.0 task/s, elapsed: 3288s, ETA: 19726s[>>>>                         ] 52/357, 0.0 task/s, elapsed: 3350s, ETA: 19647s[>>>>                         ] 53/357, 0.0 task/s, elapsed: 3415s, ETA: 19591s[>>>>                         ] 54/357, 0.0 task/s, elapsed: 3481s, ETA: 19535s[>>>>                         ] 55/357, 0.0 task/s, elapsed: 3548s, ETA: 19479s[>>>>                         ] 56/357, 0.0 task/s, elapsed: 3613s, ETA: 19422s[>>>>                         ] 57/357, 0.0 task/s, elapsed: 3676s, ETA: 19345s[>>>>                         ] 58/357, 0.0 task/s, elapsed: 3738s, ETA: 19268s[>>>>                         ] 59/357, 0.0 task/s, elapsed: 3800s, ETA: 19191s[>>>>                         ] 60/357, 0.0 task/s, elapsed: 3862s, ETA: 19115s[>>>>                         ] 61/357, 0.0 task/s, elapsed: 3924s, ETA: 19039s[>>>>>                        ] 62/357, 0.0 task/s, elapsed: 3990s, ETA: 18983s[>>>>>                        ] 63/357, 0.0 task/s, elapsed: 4052s, ETA: 18907s[>>>>>                        ] 64/357, 0.0 task/s, elapsed: 4114s, ETA: 18833s[>>>>>                        ] 65/357, 0.0 task/s, elapsed: 4176s, ETA: 18759s[>>>>>                        ] 66/357, 0.0 task/s, elapsed: 4238s, ETA: 18684s[>>>>>                        ] 67/357, 0.0 task/s, elapsed: 4304s, ETA: 18627s[>>>>>                        ] 68/357, 0.0 task/s, elapsed: 4370s, ETA: 18571s[>>>>>                        ] 69/357, 0.0 task/s, elapsed: 4436s, ETA: 18514s[>>>>>                        ] 70/357, 0.0 task/s, elapsed: 4498s, ETA: 18440s[>>>>>                        ] 71/357, 0.0 task/s, elapsed: 4560s, ETA: 18367s[>>>>>                        ] 72/357, 0.0 task/s, elapsed: 4622s, ETA: 18295s[>>>>>                        ] 73/357, 0.0 task/s, elapsed: 4684s, ETA: 18222s[>>>>>>                       ] 74/357, 0.0 task/s, elapsed: 4746s, ETA: 18152s[>>>>>>                       ] 75/357, 0.0 task/s, elapsed: 4808s, ETA: 18080s[>>>>>>                       ] 76/357, 0.0 task/s, elapsed: 4871s, ETA: 18010s[>>>>>>                       ] 77/357, 0.0 task/s, elapsed: 4933s, ETA: 17938s[>>>>>>                       ] 78/357, 0.0 task/s, elapsed: 4995s, ETA: 17867s[>>>>>>                       ] 79/357, 0.0 task/s, elapsed: 5059s, ETA: 17803s[>>>>>>                       ] 80/357, 0.0 task/s, elapsed: 5121s, ETA: 17732s[>>>>>>                       ] 81/357, 0.0 task/s, elapsed: 5187s, ETA: 17675s[>>>>>>                       ] 82/357, 0.0 task/s, elapsed: 5250s, ETA: 17606s[>>>>>>                       ] 83/357, 0.0 task/s, elapsed: 5312s, ETA: 17535s[>>>>>>                       ] 84/357, 0.0 task/s, elapsed: 5374s, ETA: 17464s[>>>>>>                       ] 85/357, 0.0 task/s, elapsed: 5440s, ETA: 17407s[>>>>>>                       ] 86/357, 0.0 task/s, elapsed: 5506s, ETA: 17349s[>>>>>>>                      ] 87/357, 0.0 task/s, elapsed: 5574s, ETA: 17298s[>>>>>>>                      ] 88/357, 0.0 task/s, elapsed: 5642s, ETA: 17246s[>>>>>>>                      ] 89/357, 0.0 task/s, elapsed: 5708s, ETA: 17189s[>>>>>>>                      ] 90/357, 0.0 task/s, elapsed: 5776s, ETA: 17136s[>>>>>>>                      ] 91/357, 0.0 task/s, elapsed: 5844s, ETA: 17083s[>>>>>>>                      ] 92/357, 0.0 task/s, elapsed: 5910s, ETA: 17024s[>>>>>>>                      ] 93/357, 0.0 task/s, elapsed: 5976s, ETA: 16964s[>>>>>>>                      ] 94/357, 0.0 task/s, elapsed: 6038s, ETA: 16894s[>>>>>>>                      ] 95/357, 0.0 task/s, elapsed: 6100s, ETA: 16823s[>>>>>>>                      ] 96/357, 0.0 task/s, elapsed: 6162s, ETA: 16753s[>>>>>>>                      ] 97/357, 0.0 task/s, elapsed: 6224s, ETA: 16683s[>>>>>>>                      ] 98/357, 0.0 task/s, elapsed: 6286s, ETA: 16613s[>>>>>>>>                     ] 99/357, 0.0 task/s, elapsed: 6348s, ETA: 16544s[>>>>>>>                     ] 100/357, 0.0 task/s, elapsed: 6410s, ETA: 16474s[>>>>>>>                     ] 101/357, 0.0 task/s, elapsed: 6472s, ETA: 16405s[>>>>>>>>                    ] 102/357, 0.0 task/s, elapsed: 6534s, ETA: 16336s[>>>>>>>>                    ] 103/357, 0.0 task/s, elapsed: 6596s, ETA: 16267s[>>>>>>>>                    ] 104/357, 0.0 task/s, elapsed: 6658s, ETA: 16198s[>>>>>>>>                    ] 105/357, 0.0 task/s, elapsed: 6720s, ETA: 16129s[>>>>>>>>                    ] 106/357, 0.0 task/s, elapsed: 6782s, ETA: 16060s[>>>>>>>>                    ] 107/357, 0.0 task/s, elapsed: 6844s, ETA: 15992s[>>>>>>>>                    ] 108/357, 0.0 task/s, elapsed: 6907s, ETA: 15924s[>>>>>>>>                    ] 109/357, 0.0 task/s, elapsed: 6973s, ETA: 15864s[>>>>>>>>                    ] 110/357, 0.0 task/s, elapsed: 7041s, ETA: 15809s[>>>>>>>>                    ] 111/357, 0.0 task/s, elapsed: 7107s, ETA: 15750s[>>>>>>>>                    ] 112/357, 0.0 task/s, elapsed: 7173s, ETA: 15690s[>>>>>>>>                    ] 113/357, 0.0 task/s, elapsed: 7241s, ETA: 15635s[>>>>>>>>                    ] 114/357, 0.0 task/s, elapsed: 7303s, ETA: 15566s[>>>>>>>>>                   ] 115/357, 0.0 task/s, elapsed: 7365s, ETA: 15498s[>>>>>>>>>                   ] 116/357, 0.0 task/s, elapsed: 7427s, ETA: 15430s[>>>>>>>>>                   ] 117/357, 0.0 task/s, elapsed: 7489s, ETA: 15361s[>>>>>>>>>                   ] 118/357, 0.0 task/s, elapsed: 7551s, ETA: 15294s[>>>>>>>>>                   ] 119/357, 0.0 task/s, elapsed: 7613s, ETA: 15226s[>>>>>>>>>                   ] 120/357, 0.0 task/s, elapsed: 7675s, ETA: 15158s[>>>>>>>>>                   ] 121/357, 0.0 task/s, elapsed: 7737s, ETA: 15091s[>>>>>>>>>                   ] 122/357, 0.0 task/s, elapsed: 7799s, ETA: 15023s[>>>>>>>>>                   ] 123/357, 0.0 task/s, elapsed: 7861s, ETA: 14956s[>>>>>>>>>                   ] 124/357, 0.0 task/s, elapsed: 7923s, ETA: 14888s[>>>>>>>>>                   ] 125/357, 0.0 task/s, elapsed: 7986s, ETA: 14821s[>>>>>>>>>                   ] 126/357, 0.0 task/s, elapsed: 8053s, ETA: 14765s[>>>>>>>>>                   ] 127/357, 0.0 task/s, elapsed: 8121s, ETA: 14708s[>>>>>>>>>>                  ] 128/357, 0.0 task/s, elapsed: 8189s, ETA: 14651s[>>>>>>>>>>                  ] 129/357, 0.0 task/s, elapsed: 8259s, ETA: 14596s[>>>>>>>>>>                  ] 130/357, 0.0 task/s, elapsed: 8323s, ETA: 14533s[>>>>>>>>>>                  ] 131/357, 0.0 task/s, elapsed: 8385s, ETA: 14466s[>>>>>>>>>>                  ] 132/357, 0.0 task/s, elapsed: 8453s, ETA: 14409s[>>>>>>>>>>                  ] 133/357, 0.0 task/s, elapsed: 8522s, ETA: 14352s[>>>>>>>>>>                  ] 134/357, 0.0 task/s, elapsed: 8589s, ETA: 14293s[>>>>>>>>>>                  ] 135/357, 0.0 task/s, elapsed: 8655s, ETA: 14233s[>>>>>>>>>>                  ] 136/357, 0.0 task/s, elapsed: 8717s, ETA: 14166s[>>>>>>>>>>                  ] 137/357, 0.0 task/s, elapsed: 8779s, ETA: 14098s[>>>>>>>>>>                  ] 138/357, 0.0 task/s, elapsed: 8845s, ETA: 14037s[>>>>>>>>>>                  ] 139/357, 0.0 task/s, elapsed: 8912s, ETA: 13976s[>>>>>>>>>>                  ] 140/357, 0.0 task/s, elapsed: 8980s, ETA: 13918s[>>>>>>>>>>>                 ] 141/357, 0.0 task/s, elapsed: 9047s, ETA: 13860s[>>>>>>>>>>>                 ] 142/357, 0.0 task/s, elapsed: 9110s, ETA: 13793s[>>>>>>>>>>>                 ] 143/357, 0.0 task/s, elapsed: 9172s, ETA: 13725s[>>>>>>>>>>>                 ] 144/357, 0.0 task/s, elapsed: 9233s, ETA: 13658s[>>>>>>>>>>>                 ] 145/357, 0.0 task/s, elapsed: 9295s, ETA: 13590s[>>>>>>>>>>>                 ] 146/357, 0.0 task/s, elapsed: 9357s, ETA: 13524s[>>>>>>>>>>>                 ] 147/357, 0.0 task/s, elapsed: 9420s, ETA: 13457s[>>>>>>>>>>>                 ] 148/357, 0.0 task/s, elapsed: 9482s, ETA: 13390s[>>>>>>>>>>>                 ] 149/357, 0.0 task/s, elapsed: 9544s, ETA: 13323s[>>>>>>>>>>>                 ] 150/357, 0.0 task/s, elapsed: 9606s, ETA: 13256s[>>>>>>>>>>>                 ] 151/357, 0.0 task/s, elapsed: 9672s, ETA: 13195s[>>>>>>>>>>>                 ] 152/357, 0.0 task/s, elapsed: 9734s, ETA: 13129s[>>>>>>>>>>>>                ] 153/357, 0.0 task/s, elapsed: 9800s, ETA: 13067s[>>>>>>>>>>>>                ] 154/357, 0.0 task/s, elapsed: 9868s, ETA: 13008s[>>>>>>>>>>>>                ] 155/357, 0.0 task/s, elapsed: 9936s, ETA: 12949s[>>>>>>>>>>>                ] 156/357, 0.0 task/s, elapsed: 10002s, ETA: 12888s[>>>>>>>>>>>                ] 157/357, 0.0 task/s, elapsed: 10068s, ETA: 12826s[>>>>>>>>>>>                ] 158/357, 0.0 task/s, elapsed: 10134s, ETA: 12764s[>>>>>>>>>>>>               ] 159/357, 0.0 task/s, elapsed: 10196s, ETA: 12697s[>>>>>>>>>>>>               ] 160/357, 0.0 task/s, elapsed: 10259s, ETA: 12631s[>>>>>>>>>>>>               ] 161/357, 0.0 task/s, elapsed: 10321s, ETA: 12565s[>>>>>>>>>>>>               ] 162/357, 0.0 task/s, elapsed: 10383s, ETA: 12498s[>>>>>>>>>>>>               ] 163/357, 0.0 task/s, elapsed: 10445s, ETA: 12432s[>>>>>>>>>>>>               ] 164/357, 0.0 task/s, elapsed: 10507s, ETA: 12365s[>>>>>>>>>>>>               ] 165/357, 0.0 task/s, elapsed: 10570s, ETA: 12299s[>>>>>>>>>>>>               ] 166/357, 0.0 task/s, elapsed: 10632s, ETA: 12233s[>>>>>>>>>>>>               ] 167/357, 0.0 task/s, elapsed: 10694s, ETA: 12167s[>>>>>>>>>>>>               ] 168/357, 0.0 task/s, elapsed: 10756s, ETA: 12101s[>>>>>>>>>>>>               ] 169/357, 0.0 task/s, elapsed: 10818s, ETA: 12034s[>>>>>>>>>>>>               ] 170/357, 0.0 task/s, elapsed: 10880s, ETA: 11968s[>>>>>>>>>>>>               ] 171/357, 0.0 task/s, elapsed: 10942s, ETA: 11902s[>>>>>>>>>>>>>              ] 172/357, 0.0 task/s, elapsed: 11004s, ETA: 11836s[>>>>>>>>>>>>>              ] 173/357, 0.0 task/s, elapsed: 11067s, ETA: 11770s[>>>>>>>>>>>>>              ] 174/357, 0.0 task/s, elapsed: 11129s, ETA: 11704s[>>>>>>>>>>>>>              ] 175/357, 0.0 task/s, elapsed: 11193s, ETA: 11641s[>>>>>>>>>>>>>              ] 176/357, 0.0 task/s, elapsed: 11264s, ETA: 11584s[>>>>>>>>>>>>>              ] 177/357, 0.0 task/s, elapsed: 11332s, ETA: 11524s[>>>>>>>>>>>>>              ] 178/357, 0.0 task/s, elapsed: 11399s, ETA: 11463s[>>>>>>>>>>>>>              ] 179/357, 0.0 task/s, elapsed: 11468s, ETA: 11404s[>>>>>>>>>>>>>              ] 180/357, 0.0 task/s, elapsed: 11534s, ETA: 11342s[>>>>>>>>>>>>>              ] 181/357, 0.0 task/s, elapsed: 11600s, ETA: 11280s[>>>>>>>>>>>>>              ] 182/357, 0.0 task/s, elapsed: 11668s, ETA: 11219s[>>>>>>>>>>>>>              ] 183/357, 0.0 task/s, elapsed: 11736s, ETA: 11159s[>>>>>>>>>>>>>              ] 184/357, 0.0 task/s, elapsed: 11798s, ETA: 11093s[>>>>>>>>>>>>>              ] 185/357, 0.0 task/s, elapsed: 11864s, ETA: 11030s[>>>>>>>>>>>>>>             ] 186/357, 0.0 task/s, elapsed: 11930s, ETA: 10968s[>>>>>>>>>>>>>>             ] 187/357, 0.0 task/s, elapsed: 11998s, ETA: 10907s[>>>>>>>>>>>>>>             ] 188/357, 0.0 task/s, elapsed: 12060s, ETA: 10841s[>>>>>>>>>>>>>>             ] 189/357, 0.0 task/s, elapsed: 12122s, ETA: 10775s[>>>>>>>>>>>>>>             ] 190/357, 0.0 task/s, elapsed: 12184s, ETA: 10709s[>>>>>>>>>>>>>>             ] 191/357, 0.0 task/s, elapsed: 12250s, ETA: 10647s[>>>>>>>>>>>>>>             ] 192/357, 0.0 task/s, elapsed: 12312s, ETA: 10581s[>>>>>>>>>>>>>>             ] 193/357, 0.0 task/s, elapsed: 12374s, ETA: 10515s[>>>>>>>>>>>>>>             ] 194/357, 0.0 task/s, elapsed: 12437s, ETA: 10449s[>>>>>>>>>>>>>>             ] 195/357, 0.0 task/s, elapsed: 12499s, ETA: 10383s[>>>>>>>>>>>>>>             ] 196/357, 0.0 task/s, elapsed: 12561s, ETA: 10318s[>>>>>>>>>>>>>>             ] 197/357, 0.0 task/s, elapsed: 12623s, ETA: 10252s[>>>>>>>>>>>>>>             ] 198/357, 0.0 task/s, elapsed: 12689s, ETA: 10190s[>>>>>>>>>>>>>>>            ] 199/357, 0.0 task/s, elapsed: 12755s, ETA: 10127s[>>>>>>>>>>>>>>>            ] 200/357, 0.0 task/s, elapsed: 12817s, ETA: 10061s[>>>>>>>>>>>>>>>            ] 201/357, 0.0 task/s, elapsed: 12879s, ETA:  9996s[>>>>>>>>>>>>>>>            ] 202/357, 0.0 task/s, elapsed: 12941s, ETA:  9930s[>>>>>>>>>>>>>>>            ] 203/357, 0.0 task/s, elapsed: 13007s, ETA:  9868s[>>>>>>>>>>>>>>>            ] 204/357, 0.0 task/s, elapsed: 13073s, ETA:  9805s[>>>>>>>>>>>>>>>            ] 205/357, 0.0 task/s, elapsed: 13135s, ETA:  9739s[>>>>>>>>>>>>>>>            ] 206/357, 0.0 task/s, elapsed: 13201s, ETA:  9677s[>>>>>>>>>>>>>>>            ] 207/357, 0.0 task/s, elapsed: 13263s, ETA:  9611s[>>>>>>>>>>>>>>>            ] 208/357, 0.0 task/s, elapsed: 13325s, ETA:  9546s[>>>>>>>>>>>>>>>            ] 209/357, 0.0 task/s, elapsed: 13391s, ETA:  9483s[>>>>>>>>>>>>>>>            ] 210/357, 0.0 task/s, elapsed: 13457s, ETA:  9420s[>>>>>>>>>>>>>>>            ] 211/357, 0.0 task/s, elapsed: 13523s, ETA:  9357s[>>>>>>>>>>>>>>>>           ] 212/357, 0.0 task/s, elapsed: 13589s, ETA:  9294s[>>>>>>>>>>>>>>>>           ] 213/357, 0.0 task/s, elapsed: 13655s, ETA:  9232s[>>>>>>>>>>>>>>>>           ] 214/357, 0.0 task/s, elapsed: 13723s, ETA:  9170s[>>>>>>>>>>>>>>>>           ] 215/357, 0.0 task/s, elapsed: 13785s, ETA:  9105s[>>>>>>>>>>>>>>>>           ] 216/357, 0.0 task/s, elapsed: 13851s, ETA:  9042s[>>>>>>>>>>>>>>>>           ] 217/357, 0.0 task/s, elapsed: 13917s, ETA:  8979s[>>>>>>>>>>>>>>>>           ] 218/357, 0.0 task/s, elapsed: 13983s, ETA:  8916s[>>>>>>>>>>>>>>>>           ] 219/357, 0.0 task/s, elapsed: 14049s, ETA:  8853s[>>>>>>>>>>>>>>>>           ] 220/357, 0.0 task/s, elapsed: 14115s, ETA:  8790s[>>>>>>>>>>>>>>>>           ] 221/357, 0.0 task/s, elapsed: 14181s, ETA:  8727s[>>>>>>>>>>>>>>>>           ] 222/357, 0.0 task/s, elapsed: 14247s, ETA:  8663s[>>>>>>>>>>>>>>>>           ] 223/357, 0.0 task/s, elapsed: 14309s, ETA:  8598s[>>>>>>>>>>>>>>>>           ] 224/357, 0.0 task/s, elapsed: 14371s, ETA:  8533s[>>>>>>>>>>>>>>>>>          ] 225/357, 0.0 task/s, elapsed: 14437s, ETA:  8470s[>>>>>>>>>>>>>>>>>          ] 226/357, 0.0 task/s, elapsed: 14505s, ETA:  8408s[>>>>>>>>>>>>>>>>>          ] 227/357, 0.0 task/s, elapsed: 14573s, ETA:  8346s[>>>>>>>>>>>>>>>>>          ] 228/357, 0.0 task/s, elapsed: 14641s, ETA:  8284s[>>>>>>>>>>>>>>>>>          ] 229/357, 0.0 task/s, elapsed: 14703s, ETA:  8218s[>>>>>>>>>>>>>>>>>          ] 230/357, 0.0 task/s, elapsed: 14765s, ETA:  8153s[>>>>>>>>>>>>>>>>>          ] 231/357, 0.0 task/s, elapsed: 14827s, ETA:  8087s[>>>>>>>>>>>>>>>>>          ] 232/357, 0.0 task/s, elapsed: 14889s, ETA:  8022s[>>>>>>>>>>>>>>>>>          ] 233/357, 0.0 task/s, elapsed: 14951s, ETA:  7957s[>>>>>>>>>>>>>>>>>          ] 234/357, 0.0 task/s, elapsed: 15013s, ETA:  7891s[>>>>>>>>>>>>>>>>>          ] 235/357, 0.0 task/s, elapsed: 15075s, ETA:  7826s[>>>>>>>>>>>>>>>>>          ] 236/357, 0.0 task/s, elapsed: 15137s, ETA:  7761s[>>>>>>>>>>>>>>>>>          ] 237/357, 0.0 task/s, elapsed: 15199s, ETA:  7696s[>>>>>>>>>>>>>>>>>>         ] 238/357, 0.0 task/s, elapsed: 15261s, ETA:  7631s[>>>>>>>>>>>>>>>>>>         ] 239/357, 0.0 task/s, elapsed: 15323s, ETA:  7566s[>>>>>>>>>>>>>>>>>>         ] 240/357, 0.0 task/s, elapsed: 15385s, ETA:  7500s[>>>>>>>>>>>>>>>>>>         ] 241/357, 0.0 task/s, elapsed: 15448s, ETA:  7435s[>>>>>>>>>>>>>>>>>>         ] 242/357, 0.0 task/s, elapsed: 15514s, ETA:  7372s[>>>>>>>>>>>>>>>>>>         ] 243/357, 0.0 task/s, elapsed: 15577s, ETA:  7308s[>>>>>>>>>>>>>>>>>>         ] 244/357, 0.0 task/s, elapsed: 15640s, ETA:  7243s[>>>>>>>>>>>>>>>>>>         ] 245/357, 0.0 task/s, elapsed: 15703s, ETA:  7179s[>>>>>>>>>>>>>>>>>>         ] 246/357, 0.0 task/s, elapsed: 15766s, ETA:  7114s[>>>>>>>>>>>>>>>>>>         ] 247/357, 0.0 task/s, elapsed: 15828s, ETA:  7049s[>>>>>>>>>>>>>>>>>>         ] 248/357, 0.0 task/s, elapsed: 15895s, ETA:  6986s[>>>>>>>>>>>>>>>>>>         ] 249/357, 0.0 task/s, elapsed: 15957s, ETA:  6921s[>>>>>>>>>>>>>>>>>>         ] 250/357, 0.0 task/s, elapsed: 16024s, ETA:  6858s[>>>>>>>>>>>>>>>>>>         ] 251/357, 0.0 task/s, elapsed: 16090s, ETA:  6795s[>>>>>>>>>>>>>>>>>>>        ] 252/357, 0.0 task/s, elapsed: 16156s, ETA:  6732s[>>>>>>>>>>>>>>>>>>>        ] 253/357, 0.0 task/s, elapsed: 16218s, ETA:  6667s[>>>>>>>>>>>>>>>>>>>        ] 254/357, 0.0 task/s, elapsed: 16280s, ETA:  6602s[>>>>>>>>>>>>>>>>>>>        ] 255/357, 0.0 task/s, elapsed: 16346s, ETA:  6539s[>>>>>>>>>>>>>>>>>>>        ] 256/357, 0.0 task/s, elapsed: 16412s, ETA:  6475s[>>>>>>>>>>>>>>>>>>>        ] 257/357, 0.0 task/s, elapsed: 16474s, ETA:  6410s[>>>>>>>>>>>>>>>>>>>        ] 258/357, 0.0 task/s, elapsed: 16540s, ETA:  6347s[>>>>>>>>>>>>>>>>>>>        ] 259/357, 0.0 task/s, elapsed: 16606s, ETA:  6283s[>>>>>>>>>>>>>>>>>>>        ] 260/357, 0.0 task/s, elapsed: 16672s, ETA:  6220s[>>>>>>>>>>>>>>>>>>>        ] 261/357, 0.0 task/s, elapsed: 16735s, ETA:  6155s[>>>>>>>>>>>>>>>>>>>        ] 262/357, 0.0 task/s, elapsed: 16801s, ETA:  6092s[>>>>>>>>>>>>>>>>>>>        ] 263/357, 0.0 task/s, elapsed: 16863s, ETA:  6027s[>>>>>>>>>>>>>>>>>>>        ] 264/357, 0.0 task/s, elapsed: 16926s, ETA:  5962s[>>>>>>>>>>>>>>>>>>>>       ] 265/357, 0.0 task/s, elapsed: 16988s, ETA:  5898s[>>>>>>>>>>>>>>>>>>>>       ] 266/357, 0.0 task/s, elapsed: 17054s, ETA:  5834s[>>>>>>>>>>>>>>>>>>>>       ] 267/357, 0.0 task/s, elapsed: 17120s, ETA:  5771s[>>>>>>>>>>>>>>>>>>>>       ] 268/357, 0.0 task/s, elapsed: 17186s, ETA:  5707s[>>>>>>>>>>>>>>>>>>>>       ] 269/357, 0.0 task/s, elapsed: 17252s, ETA:  5644s[>>>>>>>>>>>>>>>>>>>>       ] 270/357, 0.0 task/s, elapsed: 17320s, ETA:  5581s[>>>>>>>>>>>>>>>>>>>>       ] 271/357, 0.0 task/s, elapsed: 17388s, ETA:  5518s[>>>>>>>>>>>>>>>>>>>>       ] 272/357, 0.0 task/s, elapsed: 17450s, ETA:  5453s[>>>>>>>>>>>>>>>>>>>>       ] 273/357, 0.0 task/s, elapsed: 17512s, ETA:  5388s[>>>>>>>>>>>>>>>>>>>>       ] 274/357, 0.0 task/s, elapsed: 17574s, ETA:  5324s[>>>>>>>>>>>>>>>>>>>>       ] 275/357, 0.0 task/s, elapsed: 17637s, ETA:  5259s[>>>>>>>>>>>>>>>>>>>>       ] 276/357, 0.0 task/s, elapsed: 17703s, ETA:  5195s[>>>>>>>>>>>>>>>>>>>>       ] 277/357, 0.0 task/s, elapsed: 17769s, ETA:  5132s[>>>>>>>>>>>>>>>>>>>>>      ] 278/357, 0.0 task/s, elapsed: 17835s, ETA:  5068s[>>>>>>>>>>>>>>>>>>>>>      ] 279/357, 0.0 task/s, elapsed: 17897s, ETA:  5003s[>>>>>>>>>>>>>>>>>>>>>      ] 280/357, 0.0 task/s, elapsed: 17959s, ETA:  4939s[>>>>>>>>>>>>>>>>>>>>>      ] 281/357, 0.0 task/s, elapsed: 18025s, ETA:  4875s[>>>>>>>>>>>>>>>>>>>>>      ] 282/357, 0.0 task/s, elapsed: 18087s, ETA:  4810s[>>>>>>>>>>>>>>>>>>>>>      ] 283/357, 0.0 task/s, elapsed: 18149s, ETA:  4746s[>>>>>>>>>>>>>>>>>>>>>      ] 284/357, 0.0 task/s, elapsed: 18211s, ETA:  4681s[>>>>>>>>>>>>>>>>>>>>>      ] 285/357, 0.0 task/s, elapsed: 18273s, ETA:  4616s[>>>>>>>>>>>>>>>>>>>>>      ] 286/357, 0.0 task/s, elapsed: 18335s, ETA:  4552s[>>>>>>>>>>>>>>>>>>>>>      ] 287/357, 0.0 task/s, elapsed: 18398s, ETA:  4487s[>>>>>>>>>>>>>>>>>>>>>      ] 288/357, 0.0 task/s, elapsed: 18460s, ETA:  4423s[>>>>>>>>>>>>>>>>>>>>>      ] 289/357, 0.0 task/s, elapsed: 18526s, ETA:  4359s[>>>>>>>>>>>>>>>>>>>>>      ] 290/357, 0.0 task/s, elapsed: 18588s, ETA:  4294s[>>>>>>>>>>>>>>>>>>>>>>     ] 291/357, 0.0 task/s, elapsed: 18650s, ETA:  4230s[>>>>>>>>>>>>>>>>>>>>>>     ] 292/357, 0.0 task/s, elapsed: 18712s, ETA:  4165s[>>>>>>>>>>>>>>>>>>>>>>     ] 293/357, 0.0 task/s, elapsed: 18774s, ETA:  4101s[>>>>>>>>>>>>>>>>>>>>>>     ] 294/357, 0.0 task/s, elapsed: 18840s, ETA:  4037s[>>>>>>>>>>>>>>>>>>>>>>     ] 295/357, 0.0 task/s, elapsed: 18906s, ETA:  3973s[>>>>>>>>>>>>>>>>>>>>>>     ] 296/357, 0.0 task/s, elapsed: 18974s, ETA:  3910s[>>>>>>>>>>>>>>>>>>>>>>     ] 297/357, 0.0 task/s, elapsed: 19040s, ETA:  3846s[>>>>>>>>>>>>>>>>>>>>>>     ] 298/357, 0.0 task/s, elapsed: 19102s, ETA:  3782s[>>>>>>>>>>>>>>>>>>>>>>     ] 299/357, 0.0 task/s, elapsed: 19164s, ETA:  3717s[>>>>>>>>>>>>>>>>>>>>>>     ] 300/357, 0.0 task/s, elapsed: 19229s, ETA:  3654s[>>>>>>>>>>>>>>>>>>>>>>     ] 301/357, 0.0 task/s, elapsed: 19295s, ETA:  3590s[>>>>>>>>>>>>>>>>>>>>>>     ] 302/357, 0.0 task/s, elapsed: 19357s, ETA:  3525s[>>>>>>>>>>>>>>>>>>>>>>     ] 303/357, 0.0 task/s, elapsed: 19420s, ETA:  3461s[>>>>>>>>>>>>>>>>>>>>>>     ] 304/357, 0.0 task/s, elapsed: 19488s, ETA:  3398s[>>>>>>>>>>>>>>>>>>>>>>>    ] 305/357, 0.0 task/s, elapsed: 19556s, ETA:  3334s[>>>>>>>>>>>>>>>>>>>>>>>    ] 306/357, 0.0 task/s, elapsed: 19619s, ETA:  3270s[>>>>>>>>>>>>>>>>>>>>>>>    ] 307/357, 0.0 task/s, elapsed: 19685s, ETA:  3206s[>>>>>>>>>>>>>>>>>>>>>>>    ] 308/357, 0.0 task/s, elapsed: 19751s, ETA:  3142s[>>>>>>>>>>>>>>>>>>>>>>>    ] 309/357, 0.0 task/s, elapsed: 19817s, ETA:  3078s[>>>>>>>>>>>>>>>>>>>>>>>    ] 310/357, 0.0 task/s, elapsed: 19879s, ETA:  3014s[>>>>>>>>>>>>>>>>>>>>>>>    ] 311/357, 0.0 task/s, elapsed: 19941s, ETA:  2950s[>>>>>>>>>>>>>>>>>>>>>>>    ] 312/357, 0.0 task/s, elapsed: 20003s, ETA:  2885s[>>>>>>>>>>>>>>>>>>>>>>>    ] 313/357, 0.0 task/s, elapsed: 20071s, ETA:  2822s[>>>>>>>>>>>>>>>>>>>>>>>    ] 314/357, 0.0 task/s, elapsed: 20133s, ETA:  2757s[>>>>>>>>>>>>>>>>>>>>>>>    ] 315/357, 0.0 task/s, elapsed: 20195s, ETA:  2693s[>>>>>>>>>>>>>>>>>>>>>>>    ] 316/357, 0.0 task/s, elapsed: 20257s, ETA:  2628s[>>>>>>>>>>>>>>>>>>>>>>>    ] 317/357, 0.0 task/s, elapsed: 20324s, ETA:  2564s[>>>>>>>>>>>>>>>>>>>>>>>>   ] 318/357, 0.0 task/s, elapsed: 20386s, ETA:  2500s[>>>>>>>>>>>>>>>>>>>>>>>>   ] 319/357, 0.0 task/s, elapsed: 20452s, ETA:  2436s[>>>>>>>>>>>>>>>>>>>>>>>>   ] 320/357, 0.0 task/s, elapsed: 20518s, ETA:  2372s[>>>>>>>>>>>>>>>>>>>>>>>>   ] 321/357, 0.0 task/s, elapsed: 20580s, ETA:  2308s[>>>>>>>>>>>>>>>>>>>>>>>>   ] 322/357, 0.0 task/s, elapsed: 20646s, ETA:  2244s[>>>>>>>>>>>>>>>>>>>>>>>>   ] 323/357, 0.0 task/s, elapsed: 20712s, ETA:  2180s[>>>>>>>>>>>>>>>>>>>>>>>>   ] 324/357, 0.0 task/s, elapsed: 20774s, ETA:  2116s[>>>>>>>>>>>>>>>>>>>>>>>>   ] 325/357, 0.0 task/s, elapsed: 20836s, ETA:  2052s[>>>>>>>>>>>>>>>>>>>>>>>>   ] 326/357, 0.0 task/s, elapsed: 20898s, ETA:  1987s[>>>>>>>>>>>>>>>>>>>>>>>>   ] 327/357, 0.0 task/s, elapsed: 20960s, ETA:  1923s[>>>>>>>>>>>>>>>>>>>>>>>>   ] 328/357, 0.0 task/s, elapsed: 21022s, ETA:  1859s[>>>>>>>>>>>>>>>>>>>>>>>>   ] 329/357, 0.0 task/s, elapsed: 21084s, ETA:  1794s[>>>>>>>>>>>>>>>>>>>>>>>>   ] 330/357, 0.0 task/s, elapsed: 21146s, ETA:  1730s[>>>>>>>>>>>>>>>>>>>>>>>>>  ] 331/357, 0.0 task/s, elapsed: 21212s, ETA:  1666s[>>>>>>>>>>>>>>>>>>>>>>>>>  ] 332/357, 0.0 task/s, elapsed: 21278s, ETA:  1602s[>>>>>>>>>>>>>>>>>>>>>>>>>  ] 333/357, 0.0 task/s, elapsed: 21344s, ETA:  1538s[>>>>>>>>>>>>>>>>>>>>>>>>>  ] 334/357, 0.0 task/s, elapsed: 21410s, ETA:  1474s[>>>>>>>>>>>>>>>>>>>>>>>>>  ] 335/357, 0.0 task/s, elapsed: 21476s, ETA:  1410s[>>>>>>>>>>>>>>>>>>>>>>>>>  ] 336/357, 0.0 task/s, elapsed: 21538s, ETA:  1346s[>>>>>>>>>>>>>>>>>>>>>>>>>  ] 337/357, 0.0 task/s, elapsed: 21600s, ETA:  1282s[>>>>>>>>>>>>>>>>>>>>>>>>>  ] 338/357, 0.0 task/s, elapsed: 21666s, ETA:  1218s[>>>>>>>>>>>>>>>>>>>>>>>>>  ] 339/357, 0.0 task/s, elapsed: 21728s, ETA:  1154s[>>>>>>>>>>>>>>>>>>>>>>>>>  ] 340/357, 0.0 task/s, elapsed: 21790s, ETA:  1090s[>>>>>>>>>>>>>>>>>>>>>>>>>  ] 341/357, 0.0 task/s, elapsed: 21856s, ETA:  1026s[>>>>>>>>>>>>>>>>>>>>>>>>>  ] 342/357, 0.0 task/s, elapsed: 21922s, ETA:   961s[>>>>>>>>>>>>>>>>>>>>>>>>>  ] 343/357, 0.0 task/s, elapsed: 21988s, ETA:   897s[>>>>>>>>>>>>>>>>>>>>>>>>>> ] 344/357, 0.0 task/s, elapsed: 22056s, ETA:   834s[>>>>>>>>>>>>>>>>>>>>>>>>>> ] 345/357, 0.0 task/s, elapsed: 22118s, ETA:   769s[>>>>>>>>>>>>>>>>>>>>>>>>>> ] 346/357, 0.0 task/s, elapsed: 22180s, ETA:   705s[>>>>>>>>>>>>>>>>>>>>>>>>>> ] 347/357, 0.0 task/s, elapsed: 22242s, ETA:   641s[>>>>>>>>>>>>>>>>>>>>>>>>>> ] 348/357, 0.0 task/s, elapsed: 22310s, ETA:   577s[>>>>>>>>>>>>>>>>>>>>>>>>>> ] 349/357, 0.0 task/s, elapsed: 22378s, ETA:   513s[>>>>>>>>>>>>>>>>>>>>>>>>>> ] 350/357, 0.0 task/s, elapsed: 22446s, ETA:   449s[>>>>>>>>>>>>>>>>>>>>>>>>>> ] 351/357, 0.0 task/s, elapsed: 22512s, ETA:   385s[>>>>>>>>>>>>>>>>>>>>>>>>>> ] 352/357, 0.0 task/s, elapsed: 22574s, ETA:   321s[>>>>>>>>>>>>>>>>>>>>>>>>>> ] 353/357, 0.0 task/s, elapsed: 22636s, ETA:   257s[>>>>>>>>>>>>>>>>>>>>>>>>>> ] 354/357, 0.0 task/s, elapsed: 22698s, ETA:   192s[>>>>>>>>>>>>>>>>>>>>>>>>>> ] 355/357, 0.0 task/s, elapsed: 22760s, ETA:   128s[>>>>>>>>>>>>>>>>>>>>>>>>>> ] 356/357, 0.0 task/s, elapsed: 22822s, ETA:    64s[>>>>>>>>>>>>>>>>>>>>>>>>>>>] 357/357, 0.0 task/s, elapsed: 22890s, ETA:     0s2023-02-22 13:43:47,413 - mmseg - INFO - per class results:
2023-02-22 13:43:47,416 - mmseg - INFO - 
+------------------+-------+-------+
|      Class       |  IoU  |  Acc  |
+------------------+-------+-------+
|      WATER       | 93.65 | 98.07 |
|     ASPHALT      | 86.95 | 95.74 |
|      GRASS       |  85.5 | 90.34 |
|      HUMAN       | 77.73 | 87.78 |
|      ANIMAL      | 17.78 | 28.22 |
| HIGH_VEGETATION  | 89.19 | 95.23 |
|  GROUND_VEHICLE  | 94.17 | 96.88 |
|      FACADE      | 88.72 | 92.91 |
|       WIRE       | 42.98 | 60.62 |
| GARDEN_FURNITURE | 69.43 | 89.61 |
|     CONCRETE     |  87.5 | 91.48 |
|       ROOF       | 96.77 | 98.47 |
|      GRAVEL      | 59.15 | 78.81 |
|       SOIL       | 38.68 | 60.66 |
| PRIMEAIR_PATTERN | 85.87 | 94.01 |
|       SNOW       | 74.88 | 83.91 |
+------------------+-------+-------+
2023-02-22 13:43:47,417 - mmseg - INFO - Summary:
2023-02-22 13:43:47,417 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 92.15 | 74.31 | 83.92 |
+-------+-------+-------+
2023-02-22 13:44:07,912 - mmseg - INFO - Now best checkpoint is saved as best_mIoU_iter_10000.pth.
2023-02-22 13:44:07,912 - mmseg - INFO - Best mIoU is 0.7431 at 10000 iter.
2023-02-22 13:44:07,921 - mmseg - INFO - Exp name: my_city.py
2023-02-22 13:44:07,921 - mmseg - INFO - Iter(val) [357]	aAcc: 0.9215, mIoU: 0.7431, mAcc: 0.8392, IoU.WATER: 0.9365, IoU.ASPHALT: 0.8695, IoU.GRASS: 0.8550, IoU.HUMAN: 0.7773, IoU.ANIMAL: 0.1778, IoU.HIGH_VEGETATION: 0.8919, IoU.GROUND_VEHICLE: 0.9417, IoU.FACADE: 0.8872, IoU.WIRE: 0.4298, IoU.GARDEN_FURNITURE: 0.6943, IoU.CONCRETE: 0.8750, IoU.ROOF: 0.9677, IoU.GRAVEL: 0.5915, IoU.SOIL: 0.3868, IoU.PRIMEAIR_PATTERN: 0.8587, IoU.SNOW: 0.7488, Acc.WATER: 0.9807, Acc.ASPHALT: 0.9574, Acc.GRASS: 0.9034, Acc.HUMAN: 0.8778, Acc.ANIMAL: 0.2822, Acc.HIGH_VEGETATION: 0.9523, Acc.GROUND_VEHICLE: 0.9688, Acc.FACADE: 0.9291, Acc.WIRE: 0.6062, Acc.GARDEN_FURNITURE: 0.8961, Acc.CONCRETE: 0.9148, Acc.ROOF: 0.9847, Acc.GRAVEL: 0.7881, Acc.SOIL: 0.6066, Acc.PRIMEAIR_PATTERN: 0.9401, Acc.SNOW: 0.8391
2023-02-22 13:46:24,957 - mmseg - INFO - Iter [10050/80000]	lr: 1.255e-06, eta: 4 days, 2:37:28, time: 460.960, data_time: 458.236, memory: 31493, decode.loss_cls: 0.3262, decode.loss_mask: 0.5763, decode.loss_dice: 1.0458, decode.d0.loss_cls: 1.3021, decode.d0.loss_mask: 0.5688, decode.d0.loss_dice: 1.1181, decode.d1.loss_cls: 0.3298, decode.d1.loss_mask: 0.5731, decode.d1.loss_dice: 1.0742, decode.d2.loss_cls: 0.3244, decode.d2.loss_mask: 0.5653, decode.d2.loss_dice: 1.0533, decode.d3.loss_cls: 0.3346, decode.d3.loss_mask: 0.5603, decode.d3.loss_dice: 1.0433, decode.d4.loss_cls: 0.3341, decode.d4.loss_mask: 0.5591, decode.d4.loss_dice: 1.0391, decode.d5.loss_cls: 0.3562, decode.d5.loss_mask: 0.5623, decode.d5.loss_dice: 1.0449, decode.d6.loss_cls: 0.3568, decode.d6.loss_mask: 0.5556, decode.d6.loss_dice: 1.0496, decode.d7.loss_cls: 0.3525, decode.d7.loss_mask: 0.5573, decode.d7.loss_dice: 1.0340, decode.d8.loss_cls: 0.3475, decode.d8.loss_mask: 0.5548, decode.d8.loss_dice: 1.0459, loss: 20.5452
2023-02-22 13:48:41,760 - mmseg - INFO - Iter [10100/80000]	lr: 1.255e-06, eta: 4 days, 2:19:44, time: 2.736, data_time: 0.018, memory: 31493, decode.loss_cls: 0.3381, decode.loss_mask: 0.5915, decode.loss_dice: 0.9654, decode.d0.loss_cls: 1.3275, decode.d0.loss_mask: 0.6190, decode.d0.loss_dice: 1.0480, decode.d1.loss_cls: 0.3921, decode.d1.loss_mask: 0.6121, decode.d1.loss_dice: 1.0256, decode.d2.loss_cls: 0.3617, decode.d2.loss_mask: 0.5969, decode.d2.loss_dice: 0.9891, decode.d3.loss_cls: 0.3810, decode.d3.loss_mask: 0.5890, decode.d3.loss_dice: 0.9747, decode.d4.loss_cls: 0.3720, decode.d4.loss_mask: 0.5934, decode.d4.loss_dice: 0.9740, decode.d5.loss_cls: 0.3327, decode.d5.loss_mask: 0.5942, decode.d5.loss_dice: 0.9783, decode.d6.loss_cls: 0.3641, decode.d6.loss_mask: 0.5853, decode.d6.loss_dice: 0.9617, decode.d7.loss_cls: 0.3599, decode.d7.loss_mask: 0.5875, decode.d7.loss_dice: 0.9779, decode.d8.loss_cls: 0.3446, decode.d8.loss_mask: 0.5902, decode.d8.loss_dice: 0.9632, loss: 20.3908
2023-02-22 13:50:58,694 - mmseg - INFO - Iter [10150/80000]	lr: 1.254e-06, eta: 4 days, 2:02:11, time: 2.739, data_time: 0.018, memory: 31493, decode.loss_cls: 0.2902, decode.loss_mask: 0.6231, decode.loss_dice: 1.0617, decode.d0.loss_cls: 1.3006, decode.d0.loss_mask: 0.6456, decode.d0.loss_dice: 1.1474, decode.d1.loss_cls: 0.3018, decode.d1.loss_mask: 0.6387, decode.d1.loss_dice: 1.1120, decode.d2.loss_cls: 0.2949, decode.d2.loss_mask: 0.6241, decode.d2.loss_dice: 1.0854, decode.d3.loss_cls: 0.2889, decode.d3.loss_mask: 0.6261, decode.d3.loss_dice: 1.0688, decode.d4.loss_cls: 0.3083, decode.d4.loss_mask: 0.6241, decode.d4.loss_dice: 1.0575, decode.d5.loss_cls: 0.3105, decode.d5.loss_mask: 0.6236, decode.d5.loss_dice: 1.0545, decode.d6.loss_cls: 0.2944, decode.d6.loss_mask: 0.6200, decode.d6.loss_dice: 1.0637, decode.d7.loss_cls: 0.2887, decode.d7.loss_mask: 0.6214, decode.d7.loss_dice: 1.0552, decode.d8.loss_cls: 0.2945, decode.d8.loss_mask: 0.6164, decode.d8.loss_dice: 1.0451, loss: 20.9874
2023-02-22 13:53:15,588 - mmseg - INFO - Iter [10200/80000]	lr: 1.253e-06, eta: 4 days, 1:44:46, time: 2.738, data_time: 0.018, memory: 31493, decode.loss_cls: 0.3285, decode.loss_mask: 0.6661, decode.loss_dice: 1.1067, decode.d0.loss_cls: 1.3402, decode.d0.loss_mask: 0.7025, decode.d0.loss_dice: 1.1646, decode.d1.loss_cls: 0.3399, decode.d1.loss_mask: 0.6830, decode.d1.loss_dice: 1.1338, decode.d2.loss_cls: 0.3401, decode.d2.loss_mask: 0.6723, decode.d2.loss_dice: 1.1112, decode.d3.loss_cls: 0.3264, decode.d3.loss_mask: 0.6785, decode.d3.loss_dice: 1.1144, decode.d4.loss_cls: 0.3107, decode.d4.loss_mask: 0.6797, decode.d4.loss_dice: 1.1186, decode.d5.loss_cls: 0.3502, decode.d5.loss_mask: 0.6796, decode.d5.loss_dice: 1.1197, decode.d6.loss_cls: 0.3279, decode.d6.loss_mask: 0.6745, decode.d6.loss_dice: 1.1057, decode.d7.loss_cls: 0.3265, decode.d7.loss_mask: 0.6712, decode.d7.loss_dice: 1.0924, decode.d8.loss_cls: 0.3422, decode.d8.loss_mask: 0.6658, decode.d8.loss_dice: 1.0879, loss: 22.2610
2023-02-22 13:55:32,346 - mmseg - INFO - Iter [10250/80000]	lr: 1.252e-06, eta: 4 days, 1:27:29, time: 2.735, data_time: 0.018, memory: 31493, decode.loss_cls: 0.3278, decode.loss_mask: 0.6086, decode.loss_dice: 0.9982, decode.d0.loss_cls: 1.3438, decode.d0.loss_mask: 0.6171, decode.d0.loss_dice: 1.0366, decode.d1.loss_cls: 0.3327, decode.d1.loss_mask: 0.6283, decode.d1.loss_dice: 1.0327, decode.d2.loss_cls: 0.3224, decode.d2.loss_mask: 0.6127, decode.d2.loss_dice: 0.9991, decode.d3.loss_cls: 0.3193, decode.d3.loss_mask: 0.6114, decode.d3.loss_dice: 1.0056, decode.d4.loss_cls: 0.2890, decode.d4.loss_mask: 0.6135, decode.d4.loss_dice: 1.0091, decode.d5.loss_cls: 0.3116, decode.d5.loss_mask: 0.6147, decode.d5.loss_dice: 0.9938, decode.d6.loss_cls: 0.2911, decode.d6.loss_mask: 0.6143, decode.d6.loss_dice: 1.0000, decode.d7.loss_cls: 0.3069, decode.d7.loss_mask: 0.6140, decode.d7.loss_dice: 0.9966, decode.d8.loss_cls: 0.3244, decode.d8.loss_mask: 0.6063, decode.d8.loss_dice: 0.9985, loss: 20.3801
2023-02-22 13:57:49,107 - mmseg - INFO - Iter [10300/80000]	lr: 1.251e-06, eta: 4 days, 1:10:21, time: 2.735, data_time: 0.017, memory: 31493, decode.loss_cls: 0.3132, decode.loss_mask: 0.6181, decode.loss_dice: 1.0343, decode.d0.loss_cls: 1.3035, decode.d0.loss_mask: 0.6731, decode.d0.loss_dice: 1.1070, decode.d1.loss_cls: 0.3361, decode.d1.loss_mask: 0.6398, decode.d1.loss_dice: 1.0895, decode.d2.loss_cls: 0.3263, decode.d2.loss_mask: 0.6336, decode.d2.loss_dice: 1.0477, decode.d3.loss_cls: 0.3206, decode.d3.loss_mask: 0.6287, decode.d3.loss_dice: 1.0370, decode.d4.loss_cls: 0.3230, decode.d4.loss_mask: 0.6276, decode.d4.loss_dice: 1.0241, decode.d5.loss_cls: 0.3270, decode.d5.loss_mask: 0.6223, decode.d5.loss_dice: 1.0237, decode.d6.loss_cls: 0.3147, decode.d6.loss_mask: 0.6180, decode.d6.loss_dice: 1.0221, decode.d7.loss_cls: 0.3234, decode.d7.loss_mask: 0.6226, decode.d7.loss_dice: 1.0339, decode.d8.loss_cls: 0.2978, decode.d8.loss_mask: 0.6187, decode.d8.loss_dice: 1.0372, loss: 20.9446
2023-02-22 14:00:05,450 - mmseg - INFO - Iter [10350/80000]	lr: 1.250e-06, eta: 4 days, 0:53:19, time: 2.727, data_time: 0.017, memory: 31493, decode.loss_cls: 0.2915, decode.loss_mask: 0.5999, decode.loss_dice: 1.0259, decode.d0.loss_cls: 1.3077, decode.d0.loss_mask: 0.6174, decode.d0.loss_dice: 1.1154, decode.d1.loss_cls: 0.2983, decode.d1.loss_mask: 0.6010, decode.d1.loss_dice: 1.0789, decode.d2.loss_cls: 0.2836, decode.d2.loss_mask: 0.5982, decode.d2.loss_dice: 1.0349, decode.d3.loss_cls: 0.3080, decode.d3.loss_mask: 0.5863, decode.d3.loss_dice: 1.0219, decode.d4.loss_cls: 0.2954, decode.d4.loss_mask: 0.5956, decode.d4.loss_dice: 1.0210, decode.d5.loss_cls: 0.2924, decode.d5.loss_mask: 0.5801, decode.d5.loss_dice: 1.0187, decode.d6.loss_cls: 0.2948, decode.d6.loss_mask: 0.5814, decode.d6.loss_dice: 1.0222, decode.d7.loss_cls: 0.3013, decode.d7.loss_mask: 0.5847, decode.d7.loss_dice: 1.0207, decode.d8.loss_cls: 0.2970, decode.d8.loss_mask: 0.5896, decode.d8.loss_dice: 1.0282, loss: 20.2919
2023-02-22 14:02:21,932 - mmseg - INFO - Iter [10400/80000]	lr: 1.249e-06, eta: 4 days, 0:36:27, time: 2.730, data_time: 0.018, memory: 31493, decode.loss_cls: 0.3520, decode.loss_mask: 0.6374, decode.loss_dice: 1.1111, decode.d0.loss_cls: 1.3668, decode.d0.loss_mask: 0.6650, decode.d0.loss_dice: 1.1660, decode.d1.loss_cls: 0.4036, decode.d1.loss_mask: 0.6561, decode.d1.loss_dice: 1.1457, decode.d2.loss_cls: 0.3602, decode.d2.loss_mask: 0.6459, decode.d2.loss_dice: 1.1159, decode.d3.loss_cls: 0.3466, decode.d3.loss_mask: 0.6420, decode.d3.loss_dice: 1.1161, decode.d4.loss_cls: 0.3283, decode.d4.loss_mask: 0.6501, decode.d4.loss_dice: 1.0995, decode.d5.loss_cls: 0.3627, decode.d5.loss_mask: 0.6442, decode.d5.loss_dice: 1.0898, decode.d6.loss_cls: 0.3629, decode.d6.loss_mask: 0.6367, decode.d6.loss_dice: 1.0942, decode.d7.loss_cls: 0.3625, decode.d7.loss_mask: 0.6382, decode.d7.loss_dice: 1.0980, decode.d8.loss_cls: 0.3535, decode.d8.loss_mask: 0.6376, decode.d8.loss_dice: 1.1103, loss: 22.1989
2023-02-22 14:04:38,377 - mmseg - INFO - Iter [10450/80000]	lr: 1.248e-06, eta: 4 days, 0:19:42, time: 2.729, data_time: 0.017, memory: 31493, decode.loss_cls: 0.2935, decode.loss_mask: 0.5606, decode.loss_dice: 0.9742, decode.d0.loss_cls: 1.2998, decode.d0.loss_mask: 0.5939, decode.d0.loss_dice: 1.0240, decode.d1.loss_cls: 0.3310, decode.d1.loss_mask: 0.5718, decode.d1.loss_dice: 0.9990, decode.d2.loss_cls: 0.2936, decode.d2.loss_mask: 0.5720, decode.d2.loss_dice: 0.9712, decode.d3.loss_cls: 0.2978, decode.d3.loss_mask: 0.5699, decode.d3.loss_dice: 0.9945, decode.d4.loss_cls: 0.2915, decode.d4.loss_mask: 0.5757, decode.d4.loss_dice: 0.9671, decode.d5.loss_cls: 0.3207, decode.d5.loss_mask: 0.5661, decode.d5.loss_dice: 0.9797, decode.d6.loss_cls: 0.2905, decode.d6.loss_mask: 0.5622, decode.d6.loss_dice: 0.9664, decode.d7.loss_cls: 0.2791, decode.d7.loss_mask: 0.5604, decode.d7.loss_dice: 0.9693, decode.d8.loss_cls: 0.2767, decode.d8.loss_mask: 0.5643, decode.d8.loss_dice: 0.9659, loss: 19.4823
2023-02-22 14:06:55,042 - mmseg - INFO - Iter [10500/80000]	lr: 1.247e-06, eta: 4 days, 0:03:07, time: 2.733, data_time: 0.017, memory: 31493, decode.loss_cls: 0.2467, decode.loss_mask: 0.5824, decode.loss_dice: 1.0097, decode.d0.loss_cls: 1.2652, decode.d0.loss_mask: 0.5902, decode.d0.loss_dice: 1.0469, decode.d1.loss_cls: 0.2773, decode.d1.loss_mask: 0.5794, decode.d1.loss_dice: 1.0443, decode.d2.loss_cls: 0.2655, decode.d2.loss_mask: 0.5777, decode.d2.loss_dice: 1.0335, decode.d3.loss_cls: 0.2690, decode.d3.loss_mask: 0.5760, decode.d3.loss_dice: 1.0130, decode.d4.loss_cls: 0.2579, decode.d4.loss_mask: 0.5802, decode.d4.loss_dice: 1.0143, decode.d5.loss_cls: 0.2627, decode.d5.loss_mask: 0.5759, decode.d5.loss_dice: 1.0086, decode.d6.loss_cls: 0.2535, decode.d6.loss_mask: 0.5745, decode.d6.loss_dice: 0.9988, decode.d7.loss_cls: 0.2594, decode.d7.loss_mask: 0.5766, decode.d7.loss_dice: 0.9928, decode.d8.loss_cls: 0.2671, decode.d8.loss_mask: 0.5779, decode.d8.loss_dice: 1.0098, loss: 19.5869
2023-02-22 14:09:11,600 - mmseg - INFO - Iter [10550/80000]	lr: 1.246e-06, eta: 3 days, 23:46:40, time: 2.731, data_time: 0.017, memory: 31493, decode.loss_cls: 0.3104, decode.loss_mask: 0.5838, decode.loss_dice: 1.0199, decode.d0.loss_cls: 1.2356, decode.d0.loss_mask: 0.6217, decode.d0.loss_dice: 1.0783, decode.d1.loss_cls: 0.3306, decode.d1.loss_mask: 0.6027, decode.d1.loss_dice: 1.0734, decode.d2.loss_cls: 0.3349, decode.d2.loss_mask: 0.5941, decode.d2.loss_dice: 1.0462, decode.d3.loss_cls: 0.3332, decode.d3.loss_mask: 0.5855, decode.d3.loss_dice: 1.0317, decode.d4.loss_cls: 0.3283, decode.d4.loss_mask: 0.5943, decode.d4.loss_dice: 1.0082, decode.d5.loss_cls: 0.2970, decode.d5.loss_mask: 0.5914, decode.d5.loss_dice: 1.0144, decode.d6.loss_cls: 0.3116, decode.d6.loss_mask: 0.5904, decode.d6.loss_dice: 1.0233, decode.d7.loss_cls: 0.3258, decode.d7.loss_mask: 0.5835, decode.d7.loss_dice: 1.0160, decode.d8.loss_cls: 0.3040, decode.d8.loss_mask: 0.5890, decode.d8.loss_dice: 1.0218, loss: 20.3809
2023-02-22 14:11:27,931 - mmseg - INFO - Iter [10600/80000]	lr: 1.246e-06, eta: 3 days, 23:30:19, time: 2.727, data_time: 0.017, memory: 31493, decode.loss_cls: 0.3160, decode.loss_mask: 0.5737, decode.loss_dice: 0.9950, decode.d0.loss_cls: 1.2044, decode.d0.loss_mask: 0.5842, decode.d0.loss_dice: 1.0367, decode.d1.loss_cls: 0.2968, decode.d1.loss_mask: 0.5805, decode.d1.loss_dice: 1.0297, decode.d2.loss_cls: 0.3117, decode.d2.loss_mask: 0.5719, decode.d2.loss_dice: 0.9980, decode.d3.loss_cls: 0.3228, decode.d3.loss_mask: 0.5704, decode.d3.loss_dice: 1.0100, decode.d4.loss_cls: 0.3157, decode.d4.loss_mask: 0.5662, decode.d4.loss_dice: 1.0020, decode.d5.loss_cls: 0.3452, decode.d5.loss_mask: 0.5643, decode.d5.loss_dice: 1.0159, decode.d6.loss_cls: 0.3399, decode.d6.loss_mask: 0.5594, decode.d6.loss_dice: 0.9961, decode.d7.loss_cls: 0.3378, decode.d7.loss_mask: 0.5682, decode.d7.loss_dice: 0.9943, decode.d8.loss_cls: 0.3151, decode.d8.loss_mask: 0.5777, decode.d8.loss_dice: 0.9885, loss: 19.8881
2023-02-22 14:13:44,577 - mmseg - INFO - Iter [10650/80000]	lr: 1.245e-06, eta: 3 days, 23:14:08, time: 2.733, data_time: 0.017, memory: 31493, decode.loss_cls: 0.2799, decode.loss_mask: 0.6018, decode.loss_dice: 1.0376, decode.d0.loss_cls: 1.2767, decode.d0.loss_mask: 0.6271, decode.d0.loss_dice: 1.0821, decode.d1.loss_cls: 0.2868, decode.d1.loss_mask: 0.5975, decode.d1.loss_dice: 1.0462, decode.d2.loss_cls: 0.2729, decode.d2.loss_mask: 0.5856, decode.d2.loss_dice: 1.0344, decode.d3.loss_cls: 0.2865, decode.d3.loss_mask: 0.5960, decode.d3.loss_dice: 1.0390, decode.d4.loss_cls: 0.2818, decode.d4.loss_mask: 0.6022, decode.d4.loss_dice: 1.0222, decode.d5.loss_cls: 0.3234, decode.d5.loss_mask: 0.5959, decode.d5.loss_dice: 1.0338, decode.d6.loss_cls: 0.3018, decode.d6.loss_mask: 0.6003, decode.d6.loss_dice: 1.0012, decode.d7.loss_cls: 0.2939, decode.d7.loss_mask: 0.5898, decode.d7.loss_dice: 1.0243, decode.d8.loss_cls: 0.3074, decode.d8.loss_mask: 0.5908, decode.d8.loss_dice: 1.0069, loss: 20.2260
2023-02-22 14:16:00,876 - mmseg - INFO - Iter [10700/80000]	lr: 1.244e-06, eta: 3 days, 22:58:02, time: 2.726, data_time: 0.017, memory: 31493, decode.loss_cls: 0.3312, decode.loss_mask: 0.6350, decode.loss_dice: 1.0249, decode.d0.loss_cls: 1.2534, decode.d0.loss_mask: 0.6424, decode.d0.loss_dice: 1.1129, decode.d1.loss_cls: 0.3544, decode.d1.loss_mask: 0.6415, decode.d1.loss_dice: 1.0504, decode.d2.loss_cls: 0.3282, decode.d2.loss_mask: 0.6408, decode.d2.loss_dice: 1.0382, decode.d3.loss_cls: 0.3648, decode.d3.loss_mask: 0.6274, decode.d3.loss_dice: 1.0175, decode.d4.loss_cls: 0.3416, decode.d4.loss_mask: 0.6322, decode.d4.loss_dice: 1.0188, decode.d5.loss_cls: 0.3490, decode.d5.loss_mask: 0.6330, decode.d5.loss_dice: 1.0198, decode.d6.loss_cls: 0.3447, decode.d6.loss_mask: 0.6386, decode.d6.loss_dice: 1.0170, decode.d7.loss_cls: 0.3431, decode.d7.loss_mask: 0.6332, decode.d7.loss_dice: 1.0307, decode.d8.loss_cls: 0.3214, decode.d8.loss_mask: 0.6357, decode.d8.loss_dice: 1.0270, loss: 21.0488
2023-02-22 14:18:19,805 - mmseg - INFO - Iter [10750/80000]	lr: 1.243e-06, eta: 3 days, 22:42:22, time: 2.779, data_time: 0.065, memory: 31493, decode.loss_cls: 0.2389, decode.loss_mask: 0.5407, decode.loss_dice: 0.9727, decode.d0.loss_cls: 1.2489, decode.d0.loss_mask: 0.5597, decode.d0.loss_dice: 1.0255, decode.d1.loss_cls: 0.2669, decode.d1.loss_mask: 0.5477, decode.d1.loss_dice: 0.9942, decode.d2.loss_cls: 0.2555, decode.d2.loss_mask: 0.5389, decode.d2.loss_dice: 0.9729, decode.d3.loss_cls: 0.2545, decode.d3.loss_mask: 0.5406, decode.d3.loss_dice: 0.9745, decode.d4.loss_cls: 0.2395, decode.d4.loss_mask: 0.5386, decode.d4.loss_dice: 0.9744, decode.d5.loss_cls: 0.2682, decode.d5.loss_mask: 0.5389, decode.d5.loss_dice: 0.9854, decode.d6.loss_cls: 0.2491, decode.d6.loss_mask: 0.5426, decode.d6.loss_dice: 0.9808, decode.d7.loss_cls: 0.2396, decode.d7.loss_mask: 0.5396, decode.d7.loss_dice: 0.9738, decode.d8.loss_cls: 0.2533, decode.d8.loss_mask: 0.5428, decode.d8.loss_dice: 0.9737, loss: 18.7724
2023-02-22 14:20:36,003 - mmseg - INFO - Iter [10800/80000]	lr: 1.242e-06, eta: 3 days, 22:26:31, time: 2.724, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2551, decode.loss_mask: 0.5477, decode.loss_dice: 0.9740, decode.d0.loss_cls: 1.1982, decode.d0.loss_mask: 0.5749, decode.d0.loss_dice: 1.0435, decode.d1.loss_cls: 0.2702, decode.d1.loss_mask: 0.5580, decode.d1.loss_dice: 1.0077, decode.d2.loss_cls: 0.2508, decode.d2.loss_mask: 0.5530, decode.d2.loss_dice: 0.9997, decode.d3.loss_cls: 0.2449, decode.d3.loss_mask: 0.5495, decode.d3.loss_dice: 0.9815, decode.d4.loss_cls: 0.2457, decode.d4.loss_mask: 0.5480, decode.d4.loss_dice: 0.9765, decode.d5.loss_cls: 0.2255, decode.d5.loss_mask: 0.5588, decode.d5.loss_dice: 0.9846, decode.d6.loss_cls: 0.2507, decode.d6.loss_mask: 0.5552, decode.d6.loss_dice: 0.9848, decode.d7.loss_cls: 0.2692, decode.d7.loss_mask: 0.5504, decode.d7.loss_dice: 0.9725, decode.d8.loss_cls: 0.2683, decode.d8.loss_mask: 0.5515, decode.d8.loss_dice: 0.9648, loss: 18.9155
2023-02-22 14:22:52,479 - mmseg - INFO - Iter [10850/80000]	lr: 1.241e-06, eta: 3 days, 22:10:49, time: 2.730, data_time: 0.018, memory: 31493, decode.loss_cls: 0.2338, decode.loss_mask: 0.6078, decode.loss_dice: 0.9855, decode.d0.loss_cls: 1.2293, decode.d0.loss_mask: 0.6344, decode.d0.loss_dice: 1.0461, decode.d1.loss_cls: 0.2683, decode.d1.loss_mask: 0.6159, decode.d1.loss_dice: 1.0221, decode.d2.loss_cls: 0.2807, decode.d2.loss_mask: 0.6157, decode.d2.loss_dice: 0.9821, decode.d3.loss_cls: 0.2532, decode.d3.loss_mask: 0.6092, decode.d3.loss_dice: 0.9860, decode.d4.loss_cls: 0.2406, decode.d4.loss_mask: 0.6164, decode.d4.loss_dice: 0.9803, decode.d5.loss_cls: 0.2472, decode.d5.loss_mask: 0.6126, decode.d5.loss_dice: 0.9892, decode.d6.loss_cls: 0.2231, decode.d6.loss_mask: 0.6124, decode.d6.loss_dice: 0.9865, decode.d7.loss_cls: 0.2507, decode.d7.loss_mask: 0.6097, decode.d7.loss_dice: 0.9796, decode.d8.loss_cls: 0.2431, decode.d8.loss_mask: 0.6058, decode.d8.loss_dice: 0.9825, loss: 19.5495
2023-02-22 14:25:08,694 - mmseg - INFO - Iter [10900/80000]	lr: 1.240e-06, eta: 3 days, 21:55:14, time: 2.724, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2688, decode.loss_mask: 0.5942, decode.loss_dice: 1.0151, decode.d0.loss_cls: 1.2305, decode.d0.loss_mask: 0.6073, decode.d0.loss_dice: 1.0780, decode.d1.loss_cls: 0.2621, decode.d1.loss_mask: 0.6008, decode.d1.loss_dice: 1.0559, decode.d2.loss_cls: 0.2686, decode.d2.loss_mask: 0.6020, decode.d2.loss_dice: 1.0310, decode.d3.loss_cls: 0.3004, decode.d3.loss_mask: 0.5865, decode.d3.loss_dice: 1.0187, decode.d4.loss_cls: 0.2884, decode.d4.loss_mask: 0.5934, decode.d4.loss_dice: 1.0254, decode.d5.loss_cls: 0.2897, decode.d5.loss_mask: 0.5940, decode.d5.loss_dice: 1.0269, decode.d6.loss_cls: 0.2628, decode.d6.loss_mask: 0.5970, decode.d6.loss_dice: 1.0171, decode.d7.loss_cls: 0.2724, decode.d7.loss_mask: 0.5933, decode.d7.loss_dice: 1.0158, decode.d8.loss_cls: 0.2802, decode.d8.loss_mask: 0.5947, decode.d8.loss_dice: 1.0095, loss: 19.9803
2023-02-22 14:27:25,075 - mmseg - INFO - Iter [10950/80000]	lr: 1.239e-06, eta: 3 days, 21:39:46, time: 2.728, data_time: 0.018, memory: 31493, decode.loss_cls: 0.3452, decode.loss_mask: 0.6202, decode.loss_dice: 1.0201, decode.d0.loss_cls: 1.2592, decode.d0.loss_mask: 0.6562, decode.d0.loss_dice: 1.1416, decode.d1.loss_cls: 0.3364, decode.d1.loss_mask: 0.6330, decode.d1.loss_dice: 1.0937, decode.d2.loss_cls: 0.3517, decode.d2.loss_mask: 0.6259, decode.d2.loss_dice: 1.0706, decode.d3.loss_cls: 0.3461, decode.d3.loss_mask: 0.6118, decode.d3.loss_dice: 1.0408, decode.d4.loss_cls: 0.3585, decode.d4.loss_mask: 0.6183, decode.d4.loss_dice: 1.0387, decode.d5.loss_cls: 0.3569, decode.d5.loss_mask: 0.6234, decode.d5.loss_dice: 1.0621, decode.d6.loss_cls: 0.3772, decode.d6.loss_mask: 0.6171, decode.d6.loss_dice: 1.0197, decode.d7.loss_cls: 0.3721, decode.d7.loss_mask: 0.6158, decode.d7.loss_dice: 1.0404, decode.d8.loss_cls: 0.3823, decode.d8.loss_mask: 0.6157, decode.d8.loss_dice: 1.0082, loss: 21.2591
2023-02-22 14:29:41,527 - mmseg - INFO - Saving checkpoint at 11000 iterations
2023-02-22 14:30:01,594 - mmseg - INFO - Exp name: my_city.py
2023-02-22 14:30:01,594 - mmseg - INFO - Iter [11000/80000]	lr: 1.238e-06, eta: 3 days, 21:26:32, time: 3.130, data_time: 0.018, memory: 31493, decode.loss_cls: 0.2984, decode.loss_mask: 0.6313, decode.loss_dice: 1.0473, decode.d0.loss_cls: 1.2291, decode.d0.loss_mask: 0.6575, decode.d0.loss_dice: 1.1330, decode.d1.loss_cls: 0.3750, decode.d1.loss_mask: 0.6369, decode.d1.loss_dice: 1.0899, decode.d2.loss_cls: 0.3279, decode.d2.loss_mask: 0.6197, decode.d2.loss_dice: 1.0541, decode.d3.loss_cls: 0.3271, decode.d3.loss_mask: 0.6258, decode.d3.loss_dice: 1.0359, decode.d4.loss_cls: 0.3106, decode.d4.loss_mask: 0.6291, decode.d4.loss_dice: 1.0607, decode.d5.loss_cls: 0.3109, decode.d5.loss_mask: 0.6349, decode.d5.loss_dice: 1.0570, decode.d6.loss_cls: 0.3125, decode.d6.loss_mask: 0.6206, decode.d6.loss_dice: 1.0319, decode.d7.loss_cls: 0.3078, decode.d7.loss_mask: 0.6207, decode.d7.loss_dice: 1.0470, decode.d8.loss_cls: 0.3080, decode.d8.loss_mask: 0.6248, decode.d8.loss_dice: 1.0395, loss: 21.0050
2023-02-22 14:32:17,986 - mmseg - INFO - Iter [11050/80000]	lr: 1.237e-06, eta: 3 days, 21:11:18, time: 2.728, data_time: 0.018, memory: 31493, decode.loss_cls: 0.2846, decode.loss_mask: 0.5306, decode.loss_dice: 0.9450, decode.d0.loss_cls: 1.1765, decode.d0.loss_mask: 0.5506, decode.d0.loss_dice: 1.0212, decode.d1.loss_cls: 0.3198, decode.d1.loss_mask: 0.5476, decode.d1.loss_dice: 0.9777, decode.d2.loss_cls: 0.2908, decode.d2.loss_mask: 0.5351, decode.d2.loss_dice: 0.9621, decode.d3.loss_cls: 0.2856, decode.d3.loss_mask: 0.5329, decode.d3.loss_dice: 0.9529, decode.d4.loss_cls: 0.3171, decode.d4.loss_mask: 0.5276, decode.d4.loss_dice: 0.9529, decode.d5.loss_cls: 0.3194, decode.d5.loss_mask: 0.5277, decode.d5.loss_dice: 0.9467, decode.d6.loss_cls: 0.2970, decode.d6.loss_mask: 0.5279, decode.d6.loss_dice: 0.9295, decode.d7.loss_cls: 0.3001, decode.d7.loss_mask: 0.5285, decode.d7.loss_dice: 0.9574, decode.d8.loss_cls: 0.3186, decode.d8.loss_mask: 0.5289, decode.d8.loss_dice: 0.9381, loss: 18.8304
2023-02-22 14:34:34,508 - mmseg - INFO - Iter [11100/80000]	lr: 1.237e-06, eta: 3 days, 20:56:13, time: 2.730, data_time: 0.018, memory: 31493, decode.loss_cls: 0.3336, decode.loss_mask: 0.5779, decode.loss_dice: 1.0641, decode.d0.loss_cls: 1.2311, decode.d0.loss_mask: 0.6187, decode.d0.loss_dice: 1.1436, decode.d1.loss_cls: 0.3472, decode.d1.loss_mask: 0.5882, decode.d1.loss_dice: 1.0998, decode.d2.loss_cls: 0.3015, decode.d2.loss_mask: 0.5839, decode.d2.loss_dice: 1.0848, decode.d3.loss_cls: 0.3373, decode.d3.loss_mask: 0.5741, decode.d3.loss_dice: 1.0490, decode.d4.loss_cls: 0.3243, decode.d4.loss_mask: 0.5762, decode.d4.loss_dice: 1.0530, decode.d5.loss_cls: 0.3242, decode.d5.loss_mask: 0.5759, decode.d5.loss_dice: 1.0573, decode.d6.loss_cls: 0.3225, decode.d6.loss_mask: 0.5817, decode.d6.loss_dice: 1.0478, decode.d7.loss_cls: 0.3251, decode.d7.loss_mask: 0.5788, decode.d7.loss_dice: 1.0540, decode.d8.loss_cls: 0.3326, decode.d8.loss_mask: 0.5782, decode.d8.loss_dice: 1.0501, loss: 20.7164
2023-02-22 14:36:50,833 - mmseg - INFO - Iter [11150/80000]	lr: 1.236e-06, eta: 3 days, 20:41:12, time: 2.726, data_time: 0.019, memory: 31493, decode.loss_cls: 0.3667, decode.loss_mask: 0.5756, decode.loss_dice: 1.0684, decode.d0.loss_cls: 1.2408, decode.d0.loss_mask: 0.6064, decode.d0.loss_dice: 1.1216, decode.d1.loss_cls: 0.3754, decode.d1.loss_mask: 0.5816, decode.d1.loss_dice: 1.0497, decode.d2.loss_cls: 0.3648, decode.d2.loss_mask: 0.5782, decode.d2.loss_dice: 1.0423, decode.d3.loss_cls: 0.3627, decode.d3.loss_mask: 0.5877, decode.d3.loss_dice: 1.0500, decode.d4.loss_cls: 0.3447, decode.d4.loss_mask: 0.5837, decode.d4.loss_dice: 1.0592, decode.d5.loss_cls: 0.3574, decode.d5.loss_mask: 0.5748, decode.d5.loss_dice: 1.0619, decode.d6.loss_cls: 0.3505, decode.d6.loss_mask: 0.5808, decode.d6.loss_dice: 1.0602, decode.d7.loss_cls: 0.3797, decode.d7.loss_mask: 0.5725, decode.d7.loss_dice: 1.0539, decode.d8.loss_cls: 0.3642, decode.d8.loss_mask: 0.5758, decode.d8.loss_dice: 1.0515, loss: 20.9426
2023-02-22 14:39:07,579 - mmseg - INFO - Iter [11200/80000]	lr: 1.235e-06, eta: 3 days, 20:26:21, time: 2.735, data_time: 0.019, memory: 31493, decode.loss_cls: 0.3017, decode.loss_mask: 0.5194, decode.loss_dice: 1.0244, decode.d0.loss_cls: 1.2440, decode.d0.loss_mask: 0.5527, decode.d0.loss_dice: 1.1016, decode.d1.loss_cls: 0.3576, decode.d1.loss_mask: 0.5283, decode.d1.loss_dice: 1.0439, decode.d2.loss_cls: 0.3217, decode.d2.loss_mask: 0.5299, decode.d2.loss_dice: 1.0365, decode.d3.loss_cls: 0.3243, decode.d3.loss_mask: 0.5262, decode.d3.loss_dice: 1.0247, decode.d4.loss_cls: 0.3030, decode.d4.loss_mask: 0.5272, decode.d4.loss_dice: 1.0353, decode.d5.loss_cls: 0.3173, decode.d5.loss_mask: 0.5318, decode.d5.loss_dice: 1.0238, decode.d6.loss_cls: 0.3140, decode.d6.loss_mask: 0.5324, decode.d6.loss_dice: 1.0350, decode.d7.loss_cls: 0.3126, decode.d7.loss_mask: 0.5288, decode.d7.loss_dice: 1.0313, decode.d8.loss_cls: 0.3242, decode.d8.loss_mask: 0.5243, decode.d8.loss_dice: 1.0265, loss: 19.8045
2023-02-22 14:41:24,174 - mmseg - INFO - Iter [11250/80000]	lr: 1.234e-06, eta: 3 days, 20:11:36, time: 2.732, data_time: 0.019, memory: 31493, decode.loss_cls: 0.3110, decode.loss_mask: 0.6094, decode.loss_dice: 1.0629, decode.d0.loss_cls: 1.2707, decode.d0.loss_mask: 0.6519, decode.d0.loss_dice: 1.1436, decode.d1.loss_cls: 0.3467, decode.d1.loss_mask: 0.6190, decode.d1.loss_dice: 1.0998, decode.d2.loss_cls: 0.3424, decode.d2.loss_mask: 0.6120, decode.d2.loss_dice: 1.0927, decode.d3.loss_cls: 0.3498, decode.d3.loss_mask: 0.6056, decode.d3.loss_dice: 1.0452, decode.d4.loss_cls: 0.3248, decode.d4.loss_mask: 0.6108, decode.d4.loss_dice: 1.0482, decode.d5.loss_cls: 0.3302, decode.d5.loss_mask: 0.6031, decode.d5.loss_dice: 1.0534, decode.d6.loss_cls: 0.3134, decode.d6.loss_mask: 0.6055, decode.d6.loss_dice: 1.0541, decode.d7.loss_cls: 0.3259, decode.d7.loss_mask: 0.6048, decode.d7.loss_dice: 1.0528, decode.d8.loss_cls: 0.3172, decode.d8.loss_mask: 0.6034, decode.d8.loss_dice: 1.0692, loss: 21.0791
2023-02-22 14:43:40,972 - mmseg - INFO - Iter [11300/80000]	lr: 1.233e-06, eta: 3 days, 19:56:59, time: 2.736, data_time: 0.018, memory: 31493, decode.loss_cls: 0.3453, decode.loss_mask: 0.5963, decode.loss_dice: 1.0372, decode.d0.loss_cls: 1.2111, decode.d0.loss_mask: 0.6379, decode.d0.loss_dice: 1.1193, decode.d1.loss_cls: 0.4112, decode.d1.loss_mask: 0.6042, decode.d1.loss_dice: 1.0741, decode.d2.loss_cls: 0.3440, decode.d2.loss_mask: 0.6057, decode.d2.loss_dice: 1.0484, decode.d3.loss_cls: 0.3449, decode.d3.loss_mask: 0.5931, decode.d3.loss_dice: 1.0381, decode.d4.loss_cls: 0.3493, decode.d4.loss_mask: 0.5945, decode.d4.loss_dice: 1.0294, decode.d5.loss_cls: 0.3496, decode.d5.loss_mask: 0.5947, decode.d5.loss_dice: 1.0492, decode.d6.loss_cls: 0.3541, decode.d6.loss_mask: 0.5957, decode.d6.loss_dice: 1.0436, decode.d7.loss_cls: 0.3745, decode.d7.loss_mask: 0.5893, decode.d7.loss_dice: 1.0374, decode.d8.loss_cls: 0.3347, decode.d8.loss_mask: 0.6004, decode.d8.loss_dice: 1.0315, loss: 20.9389
2023-02-22 14:45:57,997 - mmseg - INFO - Iter [11350/80000]	lr: 1.232e-06, eta: 3 days, 19:42:30, time: 2.740, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2935, decode.loss_mask: 0.5627, decode.loss_dice: 1.0062, decode.d0.loss_cls: 1.2003, decode.d0.loss_mask: 0.6121, decode.d0.loss_dice: 1.0622, decode.d1.loss_cls: 0.2533, decode.d1.loss_mask: 0.5794, decode.d1.loss_dice: 1.0459, decode.d2.loss_cls: 0.2546, decode.d2.loss_mask: 0.5705, decode.d2.loss_dice: 1.0110, decode.d3.loss_cls: 0.2664, decode.d3.loss_mask: 0.5701, decode.d3.loss_dice: 1.0113, decode.d4.loss_cls: 0.2704, decode.d4.loss_mask: 0.5691, decode.d4.loss_dice: 1.0060, decode.d5.loss_cls: 0.2884, decode.d5.loss_mask: 0.5714, decode.d5.loss_dice: 1.0164, decode.d6.loss_cls: 0.2893, decode.d6.loss_mask: 0.5672, decode.d6.loss_dice: 1.0119, decode.d7.loss_cls: 0.2778, decode.d7.loss_mask: 0.5720, decode.d7.loss_dice: 1.0079, decode.d8.loss_cls: 0.2763, decode.d8.loss_mask: 0.5641, decode.d8.loss_dice: 1.0136, loss: 19.6015
2023-02-22 14:48:15,402 - mmseg - INFO - Iter [11400/80000]	lr: 1.231e-06, eta: 3 days, 19:28:09, time: 2.748, data_time: 0.020, memory: 31493, decode.loss_cls: 0.2483, decode.loss_mask: 0.5615, decode.loss_dice: 1.0502, decode.d0.loss_cls: 1.2084, decode.d0.loss_mask: 0.5771, decode.d0.loss_dice: 1.1092, decode.d1.loss_cls: 0.2724, decode.d1.loss_mask: 0.5690, decode.d1.loss_dice: 1.0835, decode.d2.loss_cls: 0.2330, decode.d2.loss_mask: 0.5589, decode.d2.loss_dice: 1.0579, decode.d3.loss_cls: 0.2574, decode.d3.loss_mask: 0.5566, decode.d3.loss_dice: 1.0513, decode.d4.loss_cls: 0.2610, decode.d4.loss_mask: 0.5541, decode.d4.loss_dice: 1.0423, decode.d5.loss_cls: 0.2541, decode.d5.loss_mask: 0.5524, decode.d5.loss_dice: 1.0531, decode.d6.loss_cls: 0.2489, decode.d6.loss_mask: 0.5590, decode.d6.loss_dice: 1.0436, decode.d7.loss_cls: 0.2371, decode.d7.loss_mask: 0.5592, decode.d7.loss_dice: 1.0417, decode.d8.loss_cls: 0.2552, decode.d8.loss_mask: 0.5573, decode.d8.loss_dice: 1.0467, loss: 19.6605
2023-02-22 14:50:35,122 - mmseg - INFO - Iter [11450/80000]	lr: 1.230e-06, eta: 3 days, 19:14:09, time: 2.794, data_time: 0.069, memory: 31493, decode.loss_cls: 0.3084, decode.loss_mask: 0.5663, decode.loss_dice: 0.9875, decode.d0.loss_cls: 1.1783, decode.d0.loss_mask: 0.5927, decode.d0.loss_dice: 1.0511, decode.d1.loss_cls: 0.2938, decode.d1.loss_mask: 0.5790, decode.d1.loss_dice: 1.0125, decode.d2.loss_cls: 0.3161, decode.d2.loss_mask: 0.5711, decode.d2.loss_dice: 0.9970, decode.d3.loss_cls: 0.2921, decode.d3.loss_mask: 0.5681, decode.d3.loss_dice: 1.0002, decode.d4.loss_cls: 0.2824, decode.d4.loss_mask: 0.5722, decode.d4.loss_dice: 0.9917, decode.d5.loss_cls: 0.2981, decode.d5.loss_mask: 0.5692, decode.d5.loss_dice: 0.9905, decode.d6.loss_cls: 0.2757, decode.d6.loss_mask: 0.5660, decode.d6.loss_dice: 0.9857, decode.d7.loss_cls: 0.2919, decode.d7.loss_mask: 0.5649, decode.d7.loss_dice: 0.9801, decode.d8.loss_cls: 0.2908, decode.d8.loss_mask: 0.5567, decode.d8.loss_dice: 0.9704, loss: 19.5004
2023-02-22 14:52:52,439 - mmseg - INFO - Iter [11500/80000]	lr: 1.229e-06, eta: 3 days, 19:00:00, time: 2.746, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2958, decode.loss_mask: 0.5619, decode.loss_dice: 0.9555, decode.d0.loss_cls: 1.1640, decode.d0.loss_mask: 0.5839, decode.d0.loss_dice: 1.0688, decode.d1.loss_cls: 0.2792, decode.d1.loss_mask: 0.5725, decode.d1.loss_dice: 1.0005, decode.d2.loss_cls: 0.2739, decode.d2.loss_mask: 0.5696, decode.d2.loss_dice: 0.9723, decode.d3.loss_cls: 0.2789, decode.d3.loss_mask: 0.5699, decode.d3.loss_dice: 0.9736, decode.d4.loss_cls: 0.2700, decode.d4.loss_mask: 0.5645, decode.d4.loss_dice: 0.9616, decode.d5.loss_cls: 0.3003, decode.d5.loss_mask: 0.5650, decode.d5.loss_dice: 0.9697, decode.d6.loss_cls: 0.2842, decode.d6.loss_mask: 0.5654, decode.d6.loss_dice: 0.9733, decode.d7.loss_cls: 0.2928, decode.d7.loss_mask: 0.5649, decode.d7.loss_dice: 0.9602, decode.d8.loss_cls: 0.3045, decode.d8.loss_mask: 0.5643, decode.d8.loss_dice: 0.9541, loss: 19.2152
2023-02-22 14:55:09,408 - mmseg - INFO - Iter [11550/80000]	lr: 1.229e-06, eta: 3 days, 18:45:56, time: 2.740, data_time: 0.020, memory: 31493, decode.loss_cls: 0.3056, decode.loss_mask: 0.5984, decode.loss_dice: 1.0485, decode.d0.loss_cls: 1.2606, decode.d0.loss_mask: 0.6198, decode.d0.loss_dice: 1.1046, decode.d1.loss_cls: 0.3131, decode.d1.loss_mask: 0.6080, decode.d1.loss_dice: 1.0814, decode.d2.loss_cls: 0.3013, decode.d2.loss_mask: 0.5853, decode.d2.loss_dice: 1.0438, decode.d3.loss_cls: 0.2951, decode.d3.loss_mask: 0.5958, decode.d3.loss_dice: 1.0531, decode.d4.loss_cls: 0.3020, decode.d4.loss_mask: 0.5909, decode.d4.loss_dice: 1.0348, decode.d5.loss_cls: 0.3079, decode.d5.loss_mask: 0.5922, decode.d5.loss_dice: 1.0398, decode.d6.loss_cls: 0.2797, decode.d6.loss_mask: 0.5950, decode.d6.loss_dice: 1.0549, decode.d7.loss_cls: 0.2861, decode.d7.loss_mask: 0.5970, decode.d7.loss_dice: 1.0577, decode.d8.loss_cls: 0.2812, decode.d8.loss_mask: 0.5987, decode.d8.loss_dice: 1.0428, loss: 20.4753
2023-02-22 14:57:25,944 - mmseg - INFO - Iter [11600/80000]	lr: 1.228e-06, eta: 3 days, 18:31:55, time: 2.731, data_time: 0.018, memory: 31493, decode.loss_cls: 0.3225, decode.loss_mask: 0.6563, decode.loss_dice: 0.9794, decode.d0.loss_cls: 1.1438, decode.d0.loss_mask: 0.7117, decode.d0.loss_dice: 1.0775, decode.d1.loss_cls: 0.3235, decode.d1.loss_mask: 0.6556, decode.d1.loss_dice: 1.0131, decode.d2.loss_cls: 0.3202, decode.d2.loss_mask: 0.6603, decode.d2.loss_dice: 1.0099, decode.d3.loss_cls: 0.3353, decode.d3.loss_mask: 0.6597, decode.d3.loss_dice: 0.9948, decode.d4.loss_cls: 0.3087, decode.d4.loss_mask: 0.6587, decode.d4.loss_dice: 1.0094, decode.d5.loss_cls: 0.3298, decode.d5.loss_mask: 0.6588, decode.d5.loss_dice: 0.9864, decode.d6.loss_cls: 0.3330, decode.d6.loss_mask: 0.6520, decode.d6.loss_dice: 0.9730, decode.d7.loss_cls: 0.3213, decode.d7.loss_mask: 0.6456, decode.d7.loss_dice: 0.9666, decode.d8.loss_cls: 0.3114, decode.d8.loss_mask: 0.6529, decode.d8.loss_dice: 0.9799, loss: 20.6512
2023-02-22 14:59:42,689 - mmseg - INFO - Iter [11650/80000]	lr: 1.227e-06, eta: 3 days, 18:18:01, time: 2.735, data_time: 0.019, memory: 31493, decode.loss_cls: 0.3061, decode.loss_mask: 0.5722, decode.loss_dice: 1.0159, decode.d0.loss_cls: 1.1439, decode.d0.loss_mask: 0.6091, decode.d0.loss_dice: 1.0980, decode.d1.loss_cls: 0.3397, decode.d1.loss_mask: 0.5822, decode.d1.loss_dice: 1.0367, decode.d2.loss_cls: 0.3121, decode.d2.loss_mask: 0.5758, decode.d2.loss_dice: 1.0477, decode.d3.loss_cls: 0.3089, decode.d3.loss_mask: 0.5747, decode.d3.loss_dice: 1.0346, decode.d4.loss_cls: 0.3081, decode.d4.loss_mask: 0.5729, decode.d4.loss_dice: 1.0322, decode.d5.loss_cls: 0.3074, decode.d5.loss_mask: 0.5748, decode.d5.loss_dice: 1.0387, decode.d6.loss_cls: 0.3163, decode.d6.loss_mask: 0.5762, decode.d6.loss_dice: 1.0291, decode.d7.loss_cls: 0.2921, decode.d7.loss_mask: 0.5817, decode.d7.loss_dice: 1.0350, decode.d8.loss_cls: 0.3077, decode.d8.loss_mask: 0.5767, decode.d8.loss_dice: 1.0236, loss: 20.1301
2023-02-22 15:01:58,993 - mmseg - INFO - Iter [11700/80000]	lr: 1.226e-06, eta: 3 days, 18:04:11, time: 2.726, data_time: 0.019, memory: 31493, decode.loss_cls: 0.3304, decode.loss_mask: 0.5518, decode.loss_dice: 0.9894, decode.d0.loss_cls: 1.1938, decode.d0.loss_mask: 0.5932, decode.d0.loss_dice: 1.1056, decode.d1.loss_cls: 0.3623, decode.d1.loss_mask: 0.5695, decode.d1.loss_dice: 1.0255, decode.d2.loss_cls: 0.3199, decode.d2.loss_mask: 0.5609, decode.d2.loss_dice: 1.0298, decode.d3.loss_cls: 0.3243, decode.d3.loss_mask: 0.5649, decode.d3.loss_dice: 1.0308, decode.d4.loss_cls: 0.3355, decode.d4.loss_mask: 0.5664, decode.d4.loss_dice: 1.0191, decode.d5.loss_cls: 0.3391, decode.d5.loss_mask: 0.5588, decode.d5.loss_dice: 1.0267, decode.d6.loss_cls: 0.3366, decode.d6.loss_mask: 0.5590, decode.d6.loss_dice: 1.0082, decode.d7.loss_cls: 0.3283, decode.d7.loss_mask: 0.5601, decode.d7.loss_dice: 1.0039, decode.d8.loss_cls: 0.3352, decode.d8.loss_mask: 0.5521, decode.d8.loss_dice: 0.9946, loss: 20.0756
2023-02-22 15:04:15,575 - mmseg - INFO - Iter [11750/80000]	lr: 1.225e-06, eta: 3 days, 17:50:28, time: 2.732, data_time: 0.019, memory: 31493, decode.loss_cls: 0.3465, decode.loss_mask: 0.6201, decode.loss_dice: 1.0636, decode.d0.loss_cls: 1.2223, decode.d0.loss_mask: 0.6390, decode.d0.loss_dice: 1.1346, decode.d1.loss_cls: 0.3536, decode.d1.loss_mask: 0.6067, decode.d1.loss_dice: 1.0968, decode.d2.loss_cls: 0.3482, decode.d2.loss_mask: 0.6043, decode.d2.loss_dice: 1.0828, decode.d3.loss_cls: 0.3340, decode.d3.loss_mask: 0.6079, decode.d3.loss_dice: 1.0730, decode.d4.loss_cls: 0.3221, decode.d4.loss_mask: 0.6188, decode.d4.loss_dice: 1.0710, decode.d5.loss_cls: 0.3393, decode.d5.loss_mask: 0.6141, decode.d5.loss_dice: 1.0692, decode.d6.loss_cls: 0.3385, decode.d6.loss_mask: 0.6084, decode.d6.loss_dice: 1.0632, decode.d7.loss_cls: 0.3738, decode.d7.loss_mask: 0.6057, decode.d7.loss_dice: 1.0529, decode.d8.loss_cls: 0.3293, decode.d8.loss_mask: 0.6062, decode.d8.loss_dice: 1.0631, loss: 21.2087
2023-02-22 15:06:32,040 - mmseg - INFO - Iter [11800/80000]	lr: 1.224e-06, eta: 3 days, 17:36:50, time: 2.729, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2914, decode.loss_mask: 0.5323, decode.loss_dice: 0.9708, decode.d0.loss_cls: 1.1498, decode.d0.loss_mask: 0.5888, decode.d0.loss_dice: 1.0536, decode.d1.loss_cls: 0.2807, decode.d1.loss_mask: 0.5524, decode.d1.loss_dice: 0.9855, decode.d2.loss_cls: 0.2883, decode.d2.loss_mask: 0.5525, decode.d2.loss_dice: 0.9737, decode.d3.loss_cls: 0.2855, decode.d3.loss_mask: 0.5387, decode.d3.loss_dice: 0.9725, decode.d4.loss_cls: 0.2862, decode.d4.loss_mask: 0.5269, decode.d4.loss_dice: 0.9789, decode.d5.loss_cls: 0.2787, decode.d5.loss_mask: 0.5265, decode.d5.loss_dice: 0.9697, decode.d6.loss_cls: 0.2842, decode.d6.loss_mask: 0.5282, decode.d6.loss_dice: 0.9495, decode.d7.loss_cls: 0.2953, decode.d7.loss_mask: 0.5303, decode.d7.loss_dice: 0.9491, decode.d8.loss_cls: 0.2907, decode.d8.loss_mask: 0.5383, decode.d8.loss_dice: 0.9588, loss: 18.9080
2023-02-22 15:08:48,613 - mmseg - INFO - Iter [11850/80000]	lr: 1.223e-06, eta: 3 days, 17:23:19, time: 2.731, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2249, decode.loss_mask: 0.5625, decode.loss_dice: 0.9495, decode.d0.loss_cls: 1.1504, decode.d0.loss_mask: 0.5763, decode.d0.loss_dice: 1.0007, decode.d1.loss_cls: 0.2752, decode.d1.loss_mask: 0.5670, decode.d1.loss_dice: 0.9728, decode.d2.loss_cls: 0.2847, decode.d2.loss_mask: 0.5590, decode.d2.loss_dice: 0.9546, decode.d3.loss_cls: 0.2565, decode.d3.loss_mask: 0.5598, decode.d3.loss_dice: 0.9625, decode.d4.loss_cls: 0.2515, decode.d4.loss_mask: 0.5621, decode.d4.loss_dice: 0.9509, decode.d5.loss_cls: 0.2492, decode.d5.loss_mask: 0.5587, decode.d5.loss_dice: 0.9370, decode.d6.loss_cls: 0.2428, decode.d6.loss_mask: 0.5613, decode.d6.loss_dice: 0.9416, decode.d7.loss_cls: 0.2545, decode.d7.loss_mask: 0.5643, decode.d7.loss_dice: 0.9468, decode.d8.loss_cls: 0.2275, decode.d8.loss_mask: 0.5639, decode.d8.loss_dice: 0.9513, loss: 18.6199
2023-02-22 15:11:05,330 - mmseg - INFO - Iter [11900/80000]	lr: 1.222e-06, eta: 3 days, 17:09:54, time: 2.734, data_time: 0.020, memory: 31493, decode.loss_cls: 0.3253, decode.loss_mask: 0.5664, decode.loss_dice: 1.0737, decode.d0.loss_cls: 1.1502, decode.d0.loss_mask: 0.5992, decode.d0.loss_dice: 1.1554, decode.d1.loss_cls: 0.3462, decode.d1.loss_mask: 0.5737, decode.d1.loss_dice: 1.1030, decode.d2.loss_cls: 0.3179, decode.d2.loss_mask: 0.5646, decode.d2.loss_dice: 1.0818, decode.d3.loss_cls: 0.3253, decode.d3.loss_mask: 0.5662, decode.d3.loss_dice: 1.0631, decode.d4.loss_cls: 0.3366, decode.d4.loss_mask: 0.5662, decode.d4.loss_dice: 1.0800, decode.d5.loss_cls: 0.3422, decode.d5.loss_mask: 0.5695, decode.d5.loss_dice: 1.0595, decode.d6.loss_cls: 0.3497, decode.d6.loss_mask: 0.5740, decode.d6.loss_dice: 1.0556, decode.d7.loss_cls: 0.3344, decode.d7.loss_mask: 0.5703, decode.d7.loss_dice: 1.0718, decode.d8.loss_cls: 0.3442, decode.d8.loss_mask: 0.5678, decode.d8.loss_dice: 1.0663, loss: 20.7003
2023-02-22 15:13:22,102 - mmseg - INFO - Iter [11950/80000]	lr: 1.221e-06, eta: 3 days, 16:56:35, time: 2.735, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2611, decode.loss_mask: 0.5443, decode.loss_dice: 0.9514, decode.d0.loss_cls: 1.1271, decode.d0.loss_mask: 0.5755, decode.d0.loss_dice: 1.0108, decode.d1.loss_cls: 0.2862, decode.d1.loss_mask: 0.5541, decode.d1.loss_dice: 0.9784, decode.d2.loss_cls: 0.2618, decode.d2.loss_mask: 0.5470, decode.d2.loss_dice: 0.9420, decode.d3.loss_cls: 0.2765, decode.d3.loss_mask: 0.5472, decode.d3.loss_dice: 0.9507, decode.d4.loss_cls: 0.2683, decode.d4.loss_mask: 0.5481, decode.d4.loss_dice: 0.9595, decode.d5.loss_cls: 0.2670, decode.d5.loss_mask: 0.5433, decode.d5.loss_dice: 0.9560, decode.d6.loss_cls: 0.2731, decode.d6.loss_mask: 0.5462, decode.d6.loss_dice: 0.9389, decode.d7.loss_cls: 0.2660, decode.d7.loss_mask: 0.5468, decode.d7.loss_dice: 0.9595, decode.d8.loss_cls: 0.2535, decode.d8.loss_mask: 0.5430, decode.d8.loss_dice: 0.9371, loss: 18.6203
2023-02-22 15:15:38,641 - mmseg - INFO - Saving checkpoint at 12000 iterations
2023-02-22 15:15:59,134 - mmseg - INFO - Exp name: my_city.py
2023-02-22 15:15:59,135 - mmseg - INFO - Iter [12000/80000]	lr: 1.220e-06, eta: 3 days, 16:45:16, time: 3.141, data_time: 0.018, memory: 31493, decode.loss_cls: 0.2968, decode.loss_mask: 0.5732, decode.loss_dice: 0.9786, decode.d0.loss_cls: 1.1261, decode.d0.loss_mask: 0.5946, decode.d0.loss_dice: 1.0202, decode.d1.loss_cls: 0.3397, decode.d1.loss_mask: 0.5871, decode.d1.loss_dice: 1.0012, decode.d2.loss_cls: 0.3146, decode.d2.loss_mask: 0.5797, decode.d2.loss_dice: 0.9953, decode.d3.loss_cls: 0.3282, decode.d3.loss_mask: 0.5778, decode.d3.loss_dice: 0.9691, decode.d4.loss_cls: 0.3292, decode.d4.loss_mask: 0.5688, decode.d4.loss_dice: 0.9634, decode.d5.loss_cls: 0.3336, decode.d5.loss_mask: 0.5745, decode.d5.loss_dice: 0.9611, decode.d6.loss_cls: 0.3244, decode.d6.loss_mask: 0.5712, decode.d6.loss_dice: 0.9758, decode.d7.loss_cls: 0.3162, decode.d7.loss_mask: 0.5743, decode.d7.loss_dice: 0.9822, decode.d8.loss_cls: 0.3108, decode.d8.loss_mask: 0.5751, decode.d8.loss_dice: 0.9709, loss: 19.6138
2023-02-22 15:18:15,773 - mmseg - INFO - Iter [12050/80000]	lr: 1.220e-06, eta: 3 days, 16:32:07, time: 2.733, data_time: 0.019, memory: 31493, decode.loss_cls: 0.3420, decode.loss_mask: 0.5995, decode.loss_dice: 1.0486, decode.d0.loss_cls: 1.1342, decode.d0.loss_mask: 0.6292, decode.d0.loss_dice: 1.1080, decode.d1.loss_cls: 0.3436, decode.d1.loss_mask: 0.6079, decode.d1.loss_dice: 1.0755, decode.d2.loss_cls: 0.3024, decode.d2.loss_mask: 0.6029, decode.d2.loss_dice: 1.0827, decode.d3.loss_cls: 0.3144, decode.d3.loss_mask: 0.6035, decode.d3.loss_dice: 1.0670, decode.d4.loss_cls: 0.3335, decode.d4.loss_mask: 0.6045, decode.d4.loss_dice: 1.0637, decode.d5.loss_cls: 0.3203, decode.d5.loss_mask: 0.6056, decode.d5.loss_dice: 1.0559, decode.d6.loss_cls: 0.3087, decode.d6.loss_mask: 0.6036, decode.d6.loss_dice: 1.0652, decode.d7.loss_cls: 0.3174, decode.d7.loss_mask: 0.5938, decode.d7.loss_dice: 1.0712, decode.d8.loss_cls: 0.3297, decode.d8.loss_mask: 0.5981, decode.d8.loss_dice: 1.0524, loss: 20.7852
2023-02-22 15:20:32,033 - mmseg - INFO - Iter [12100/80000]	lr: 1.219e-06, eta: 3 days, 16:19:01, time: 2.725, data_time: 0.018, memory: 31493, decode.loss_cls: 0.2711, decode.loss_mask: 0.5324, decode.loss_dice: 0.8853, decode.d0.loss_cls: 1.0588, decode.d0.loss_mask: 0.5744, decode.d0.loss_dice: 0.9806, decode.d1.loss_cls: 0.3151, decode.d1.loss_mask: 0.5389, decode.d1.loss_dice: 0.9304, decode.d2.loss_cls: 0.2873, decode.d2.loss_mask: 0.5253, decode.d2.loss_dice: 0.9039, decode.d3.loss_cls: 0.2949, decode.d3.loss_mask: 0.5316, decode.d3.loss_dice: 0.8962, decode.d4.loss_cls: 0.2663, decode.d4.loss_mask: 0.5321, decode.d4.loss_dice: 0.9013, decode.d5.loss_cls: 0.2629, decode.d5.loss_mask: 0.5394, decode.d5.loss_dice: 0.9015, decode.d6.loss_cls: 0.2815, decode.d6.loss_mask: 0.5240, decode.d6.loss_dice: 0.8797, decode.d7.loss_cls: 0.2808, decode.d7.loss_mask: 0.5305, decode.d7.loss_dice: 0.8886, decode.d8.loss_cls: 0.2611, decode.d8.loss_mask: 0.5380, decode.d8.loss_dice: 0.8874, loss: 18.0012
2023-02-22 15:22:51,006 - mmseg - INFO - Iter [12150/80000]	lr: 1.218e-06, eta: 3 days, 16:06:16, time: 2.779, data_time: 0.066, memory: 31493, decode.loss_cls: 0.3003, decode.loss_mask: 0.5960, decode.loss_dice: 1.0537, decode.d0.loss_cls: 1.0874, decode.d0.loss_mask: 0.6185, decode.d0.loss_dice: 1.1119, decode.d1.loss_cls: 0.3259, decode.d1.loss_mask: 0.5978, decode.d1.loss_dice: 1.0843, decode.d2.loss_cls: 0.3163, decode.d2.loss_mask: 0.5944, decode.d2.loss_dice: 1.0602, decode.d3.loss_cls: 0.3027, decode.d3.loss_mask: 0.5848, decode.d3.loss_dice: 1.0452, decode.d4.loss_cls: 0.2923, decode.d4.loss_mask: 0.5947, decode.d4.loss_dice: 1.0492, decode.d5.loss_cls: 0.3064, decode.d5.loss_mask: 0.6014, decode.d5.loss_dice: 1.0458, decode.d6.loss_cls: 0.2915, decode.d6.loss_mask: 0.5952, decode.d6.loss_dice: 1.0345, decode.d7.loss_cls: 0.2928, decode.d7.loss_mask: 0.5914, decode.d7.loss_dice: 1.0588, decode.d8.loss_cls: 0.3034, decode.d8.loss_mask: 0.5985, decode.d8.loss_dice: 1.0522, loss: 20.3875
2023-02-22 15:25:07,616 - mmseg - INFO - Iter [12200/80000]	lr: 1.217e-06, eta: 3 days, 15:53:22, time: 2.732, data_time: 0.018, memory: 31493, decode.loss_cls: 0.3520, decode.loss_mask: 0.6123, decode.loss_dice: 0.9990, decode.d0.loss_cls: 1.1059, decode.d0.loss_mask: 0.6636, decode.d0.loss_dice: 1.1142, decode.d1.loss_cls: 0.3886, decode.d1.loss_mask: 0.6292, decode.d1.loss_dice: 1.0539, decode.d2.loss_cls: 0.3553, decode.d2.loss_mask: 0.6173, decode.d2.loss_dice: 1.0259, decode.d3.loss_cls: 0.3792, decode.d3.loss_mask: 0.6149, decode.d3.loss_dice: 0.9949, decode.d4.loss_cls: 0.3276, decode.d4.loss_mask: 0.6148, decode.d4.loss_dice: 1.0242, decode.d5.loss_cls: 0.3666, decode.d5.loss_mask: 0.6072, decode.d5.loss_dice: 0.9993, decode.d6.loss_cls: 0.3637, decode.d6.loss_mask: 0.6122, decode.d6.loss_dice: 0.9975, decode.d7.loss_cls: 0.3693, decode.d7.loss_mask: 0.6027, decode.d7.loss_dice: 1.0040, decode.d8.loss_cls: 0.3518, decode.d8.loss_mask: 0.6086, decode.d8.loss_dice: 1.0003, loss: 20.7560
2023-02-22 15:27:24,101 - mmseg - INFO - Iter [12250/80000]	lr: 1.216e-06, eta: 3 days, 15:40:33, time: 2.730, data_time: 0.019, memory: 31493, decode.loss_cls: 0.3929, decode.loss_mask: 0.6415, decode.loss_dice: 1.1092, decode.d0.loss_cls: 1.1477, decode.d0.loss_mask: 0.6778, decode.d0.loss_dice: 1.1809, decode.d1.loss_cls: 0.4074, decode.d1.loss_mask: 0.6535, decode.d1.loss_dice: 1.1518, decode.d2.loss_cls: 0.4072, decode.d2.loss_mask: 0.6566, decode.d2.loss_dice: 1.1175, decode.d3.loss_cls: 0.3636, decode.d3.loss_mask: 0.6476, decode.d3.loss_dice: 1.1228, decode.d4.loss_cls: 0.3549, decode.d4.loss_mask: 0.6467, decode.d4.loss_dice: 1.1304, decode.d5.loss_cls: 0.3650, decode.d5.loss_mask: 0.6364, decode.d5.loss_dice: 1.1225, decode.d6.loss_cls: 0.3627, decode.d6.loss_mask: 0.6326, decode.d6.loss_dice: 1.1119, decode.d7.loss_cls: 0.3698, decode.d7.loss_mask: 0.6289, decode.d7.loss_dice: 1.1153, decode.d8.loss_cls: 0.3979, decode.d8.loss_mask: 0.6278, decode.d8.loss_dice: 1.1015, loss: 22.2823
2023-02-22 15:29:40,797 - mmseg - INFO - Iter [12300/80000]	lr: 1.215e-06, eta: 3 days, 15:27:51, time: 2.734, data_time: 0.018, memory: 31493, decode.loss_cls: 0.3363, decode.loss_mask: 0.6042, decode.loss_dice: 1.0500, decode.d0.loss_cls: 1.0958, decode.d0.loss_mask: 0.6302, decode.d0.loss_dice: 1.1146, decode.d1.loss_cls: 0.3469, decode.d1.loss_mask: 0.6084, decode.d1.loss_dice: 1.0765, decode.d2.loss_cls: 0.3352, decode.d2.loss_mask: 0.6021, decode.d2.loss_dice: 1.0592, decode.d3.loss_cls: 0.3224, decode.d3.loss_mask: 0.6044, decode.d3.loss_dice: 1.0644, decode.d4.loss_cls: 0.3428, decode.d4.loss_mask: 0.6033, decode.d4.loss_dice: 1.0523, decode.d5.loss_cls: 0.3236, decode.d5.loss_mask: 0.6059, decode.d5.loss_dice: 1.0592, decode.d6.loss_cls: 0.3575, decode.d6.loss_mask: 0.6008, decode.d6.loss_dice: 1.0505, decode.d7.loss_cls: 0.3613, decode.d7.loss_mask: 0.6032, decode.d7.loss_dice: 1.0497, decode.d8.loss_cls: 0.3429, decode.d8.loss_mask: 0.6042, decode.d8.loss_dice: 1.0456, loss: 20.8533
2023-02-22 15:31:57,747 - mmseg - INFO - Iter [12350/80000]	lr: 1.214e-06, eta: 3 days, 15:15:14, time: 2.739, data_time: 0.018, memory: 31493, decode.loss_cls: 0.3182, decode.loss_mask: 0.6352, decode.loss_dice: 0.9836, decode.d0.loss_cls: 1.1019, decode.d0.loss_mask: 0.6549, decode.d0.loss_dice: 1.0309, decode.d1.loss_cls: 0.3428, decode.d1.loss_mask: 0.6193, decode.d1.loss_dice: 0.9815, decode.d2.loss_cls: 0.3098, decode.d2.loss_mask: 0.6092, decode.d2.loss_dice: 0.9746, decode.d3.loss_cls: 0.3191, decode.d3.loss_mask: 0.6185, decode.d3.loss_dice: 0.9839, decode.d4.loss_cls: 0.3084, decode.d4.loss_mask: 0.6230, decode.d4.loss_dice: 0.9718, decode.d5.loss_cls: 0.3384, decode.d5.loss_mask: 0.6102, decode.d5.loss_dice: 0.9895, decode.d6.loss_cls: 0.3244, decode.d6.loss_mask: 0.6155, decode.d6.loss_dice: 0.9651, decode.d7.loss_cls: 0.3206, decode.d7.loss_mask: 0.6178, decode.d7.loss_dice: 0.9725, decode.d8.loss_cls: 0.3075, decode.d8.loss_mask: 0.6189, decode.d8.loss_dice: 0.9755, loss: 20.0422
2023-02-22 15:34:14,346 - mmseg - INFO - Iter [12400/80000]	lr: 1.213e-06, eta: 3 days, 15:02:41, time: 2.732, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2573, decode.loss_mask: 0.5781, decode.loss_dice: 0.9186, decode.d0.loss_cls: 1.0724, decode.d0.loss_mask: 0.6219, decode.d0.loss_dice: 0.9629, decode.d1.loss_cls: 0.2563, decode.d1.loss_mask: 0.5904, decode.d1.loss_dice: 0.9718, decode.d2.loss_cls: 0.2516, decode.d2.loss_mask: 0.5850, decode.d2.loss_dice: 0.9547, decode.d3.loss_cls: 0.2658, decode.d3.loss_mask: 0.5833, decode.d3.loss_dice: 0.9373, decode.d4.loss_cls: 0.2519, decode.d4.loss_mask: 0.5816, decode.d4.loss_dice: 0.9460, decode.d5.loss_cls: 0.2581, decode.d5.loss_mask: 0.5916, decode.d5.loss_dice: 0.9535, decode.d6.loss_cls: 0.2649, decode.d6.loss_mask: 0.5726, decode.d6.loss_dice: 0.9531, decode.d7.loss_cls: 0.2742, decode.d7.loss_mask: 0.5617, decode.d7.loss_dice: 0.9306, decode.d8.loss_cls: 0.2670, decode.d8.loss_mask: 0.5644, decode.d8.loss_dice: 0.9327, loss: 18.7116
2023-02-22 15:36:31,216 - mmseg - INFO - Iter [12450/80000]	lr: 1.212e-06, eta: 3 days, 14:50:15, time: 2.737, data_time: 0.019, memory: 31493, decode.loss_cls: 0.3223, decode.loss_mask: 0.6040, decode.loss_dice: 0.9560, decode.d0.loss_cls: 1.0984, decode.d0.loss_mask: 0.6701, decode.d0.loss_dice: 1.0705, decode.d1.loss_cls: 0.3485, decode.d1.loss_mask: 0.6208, decode.d1.loss_dice: 1.0083, decode.d2.loss_cls: 0.3421, decode.d2.loss_mask: 0.6072, decode.d2.loss_dice: 0.9715, decode.d3.loss_cls: 0.3197, decode.d3.loss_mask: 0.6119, decode.d3.loss_dice: 0.9580, decode.d4.loss_cls: 0.3097, decode.d4.loss_mask: 0.6152, decode.d4.loss_dice: 0.9609, decode.d5.loss_cls: 0.3173, decode.d5.loss_mask: 0.6090, decode.d5.loss_dice: 0.9728, decode.d6.loss_cls: 0.3293, decode.d6.loss_mask: 0.6195, decode.d6.loss_dice: 0.9492, decode.d7.loss_cls: 0.3165, decode.d7.loss_mask: 0.6160, decode.d7.loss_dice: 0.9769, decode.d8.loss_cls: 0.3388, decode.d8.loss_mask: 0.6097, decode.d8.loss_dice: 0.9589, loss: 20.0088
2023-02-22 15:38:47,955 - mmseg - INFO - Iter [12500/80000]	lr: 1.211e-06, eta: 3 days, 14:37:52, time: 2.735, data_time: 0.019, memory: 31493, decode.loss_cls: 0.3056, decode.loss_mask: 0.5949, decode.loss_dice: 1.0082, decode.d0.loss_cls: 1.0723, decode.d0.loss_mask: 0.6284, decode.d0.loss_dice: 1.0729, decode.d1.loss_cls: 0.3151, decode.d1.loss_mask: 0.6256, decode.d1.loss_dice: 1.0449, decode.d2.loss_cls: 0.2819, decode.d2.loss_mask: 0.6082, decode.d2.loss_dice: 1.0125, decode.d3.loss_cls: 0.2711, decode.d3.loss_mask: 0.6024, decode.d3.loss_dice: 1.0045, decode.d4.loss_cls: 0.2928, decode.d4.loss_mask: 0.5960, decode.d4.loss_dice: 1.0162, decode.d5.loss_cls: 0.2787, decode.d5.loss_mask: 0.5927, decode.d5.loss_dice: 1.0167, decode.d6.loss_cls: 0.2735, decode.d6.loss_mask: 0.5889, decode.d6.loss_dice: 1.0036, decode.d7.loss_cls: 0.2796, decode.d7.loss_mask: 0.5931, decode.d7.loss_dice: 1.0113, decode.d8.loss_cls: 0.2860, decode.d8.loss_mask: 0.5943, decode.d8.loss_dice: 1.0170, loss: 19.8888
2023-02-22 15:41:05,155 - mmseg - INFO - Iter [12550/80000]	lr: 1.211e-06, eta: 3 days, 14:25:37, time: 2.744, data_time: 0.019, memory: 31493, decode.loss_cls: 0.3042, decode.loss_mask: 0.6053, decode.loss_dice: 1.0824, decode.d0.loss_cls: 1.0855, decode.d0.loss_mask: 0.6441, decode.d0.loss_dice: 1.1729, decode.d1.loss_cls: 0.3493, decode.d1.loss_mask: 0.6071, decode.d1.loss_dice: 1.1197, decode.d2.loss_cls: 0.3118, decode.d2.loss_mask: 0.6023, decode.d2.loss_dice: 1.0845, decode.d3.loss_cls: 0.3207, decode.d3.loss_mask: 0.6077, decode.d3.loss_dice: 1.0877, decode.d4.loss_cls: 0.3145, decode.d4.loss_mask: 0.6034, decode.d4.loss_dice: 1.0841, decode.d5.loss_cls: 0.3064, decode.d5.loss_mask: 0.6091, decode.d5.loss_dice: 1.1070, decode.d6.loss_cls: 0.3226, decode.d6.loss_mask: 0.6075, decode.d6.loss_dice: 1.0972, decode.d7.loss_cls: 0.3291, decode.d7.loss_mask: 0.6056, decode.d7.loss_dice: 1.0774, decode.d8.loss_cls: 0.3191, decode.d8.loss_mask: 0.6055, decode.d8.loss_dice: 1.0804, loss: 21.0542
2023-02-22 15:43:23,169 - mmseg - INFO - Iter [12600/80000]	lr: 1.210e-06, eta: 3 days, 14:13:31, time: 2.760, data_time: 0.022, memory: 31493, decode.loss_cls: 0.2375, decode.loss_mask: 0.6188, decode.loss_dice: 1.0510, decode.d0.loss_cls: 1.0996, decode.d0.loss_mask: 0.6534, decode.d0.loss_dice: 1.1409, decode.d1.loss_cls: 0.3074, decode.d1.loss_mask: 0.6312, decode.d1.loss_dice: 1.0885, decode.d2.loss_cls: 0.2819, decode.d2.loss_mask: 0.6236, decode.d2.loss_dice: 1.0687, decode.d3.loss_cls: 0.2650, decode.d3.loss_mask: 0.6232, decode.d3.loss_dice: 1.0444, decode.d4.loss_cls: 0.2644, decode.d4.loss_mask: 0.6202, decode.d4.loss_dice: 1.0362, decode.d5.loss_cls: 0.2688, decode.d5.loss_mask: 0.6173, decode.d5.loss_dice: 1.0631, decode.d6.loss_cls: 0.2609, decode.d6.loss_mask: 0.6198, decode.d6.loss_dice: 1.0413, decode.d7.loss_cls: 0.2798, decode.d7.loss_mask: 0.6080, decode.d7.loss_dice: 1.0461, decode.d8.loss_cls: 0.2735, decode.d8.loss_mask: 0.6140, decode.d8.loss_dice: 1.0501, loss: 20.3985
2023-02-22 15:45:41,492 - mmseg - INFO - Iter [12650/80000]	lr: 1.209e-06, eta: 3 days, 14:01:31, time: 2.766, data_time: 0.020, memory: 31493, decode.loss_cls: 0.2749, decode.loss_mask: 0.5684, decode.loss_dice: 1.0303, decode.d0.loss_cls: 1.0925, decode.d0.loss_mask: 0.5837, decode.d0.loss_dice: 1.1150, decode.d1.loss_cls: 0.2838, decode.d1.loss_mask: 0.5803, decode.d1.loss_dice: 1.0895, decode.d2.loss_cls: 0.2705, decode.d2.loss_mask: 0.5675, decode.d2.loss_dice: 1.0598, decode.d3.loss_cls: 0.2783, decode.d3.loss_mask: 0.5588, decode.d3.loss_dice: 1.0315, decode.d4.loss_cls: 0.2700, decode.d4.loss_mask: 0.5649, decode.d4.loss_dice: 1.0421, decode.d5.loss_cls: 0.2528, decode.d5.loss_mask: 0.5664, decode.d5.loss_dice: 1.0484, decode.d6.loss_cls: 0.2690, decode.d6.loss_mask: 0.5636, decode.d6.loss_dice: 1.0310, decode.d7.loss_cls: 0.2750, decode.d7.loss_mask: 0.5665, decode.d7.loss_dice: 1.0402, decode.d8.loss_cls: 0.2701, decode.d8.loss_mask: 0.5689, decode.d8.loss_dice: 1.0427, loss: 19.7565
2023-02-22 15:47:59,672 - mmseg - INFO - Iter [12700/80000]	lr: 1.208e-06, eta: 3 days, 13:49:35, time: 2.764, data_time: 0.020, memory: 31493, decode.loss_cls: 0.2821, decode.loss_mask: 0.5641, decode.loss_dice: 0.9966, decode.d0.loss_cls: 1.1107, decode.d0.loss_mask: 0.5960, decode.d0.loss_dice: 1.0962, decode.d1.loss_cls: 0.3289, decode.d1.loss_mask: 0.5706, decode.d1.loss_dice: 1.0278, decode.d2.loss_cls: 0.2886, decode.d2.loss_mask: 0.5627, decode.d2.loss_dice: 1.0143, decode.d3.loss_cls: 0.2894, decode.d3.loss_mask: 0.5624, decode.d3.loss_dice: 1.0025, decode.d4.loss_cls: 0.2877, decode.d4.loss_mask: 0.5579, decode.d4.loss_dice: 1.0113, decode.d5.loss_cls: 0.2767, decode.d5.loss_mask: 0.5551, decode.d5.loss_dice: 1.0012, decode.d6.loss_cls: 0.2538, decode.d6.loss_mask: 0.5585, decode.d6.loss_dice: 0.9945, decode.d7.loss_cls: 0.2654, decode.d7.loss_mask: 0.5638, decode.d7.loss_dice: 0.9870, decode.d8.loss_cls: 0.2707, decode.d8.loss_mask: 0.5605, decode.d8.loss_dice: 0.9946, loss: 19.4314
2023-02-22 15:50:16,439 - mmseg - INFO - Iter [12750/80000]	lr: 1.207e-06, eta: 3 days, 13:37:36, time: 2.735, data_time: 0.020, memory: 31493, decode.loss_cls: 0.2970, decode.loss_mask: 0.6042, decode.loss_dice: 0.9765, decode.d0.loss_cls: 1.0946, decode.d0.loss_mask: 0.6382, decode.d0.loss_dice: 1.0427, decode.d1.loss_cls: 0.3319, decode.d1.loss_mask: 0.6085, decode.d1.loss_dice: 1.0261, decode.d2.loss_cls: 0.3362, decode.d2.loss_mask: 0.5914, decode.d2.loss_dice: 1.0035, decode.d3.loss_cls: 0.3190, decode.d3.loss_mask: 0.5833, decode.d3.loss_dice: 0.9701, decode.d4.loss_cls: 0.3093, decode.d4.loss_mask: 0.5852, decode.d4.loss_dice: 0.9744, decode.d5.loss_cls: 0.3361, decode.d5.loss_mask: 0.5814, decode.d5.loss_dice: 0.9785, decode.d6.loss_cls: 0.3368, decode.d6.loss_mask: 0.5887, decode.d6.loss_dice: 0.9585, decode.d7.loss_cls: 0.3225, decode.d7.loss_mask: 0.5907, decode.d7.loss_dice: 0.9600, decode.d8.loss_cls: 0.3213, decode.d8.loss_mask: 0.5934, decode.d8.loss_dice: 0.9632, loss: 19.8231
2023-02-22 15:52:33,117 - mmseg - INFO - Iter [12800/80000]	lr: 1.206e-06, eta: 3 days, 13:25:41, time: 2.734, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2780, decode.loss_mask: 0.5417, decode.loss_dice: 0.9777, decode.d0.loss_cls: 1.0392, decode.d0.loss_mask: 0.5825, decode.d0.loss_dice: 1.0247, decode.d1.loss_cls: 0.2852, decode.d1.loss_mask: 0.5448, decode.d1.loss_dice: 1.0273, decode.d2.loss_cls: 0.2583, decode.d2.loss_mask: 0.5506, decode.d2.loss_dice: 0.9825, decode.d3.loss_cls: 0.2619, decode.d3.loss_mask: 0.5584, decode.d3.loss_dice: 0.9720, decode.d4.loss_cls: 0.2668, decode.d4.loss_mask: 0.5471, decode.d4.loss_dice: 1.0017, decode.d5.loss_cls: 0.2823, decode.d5.loss_mask: 0.5496, decode.d5.loss_dice: 0.9752, decode.d6.loss_cls: 0.2639, decode.d6.loss_mask: 0.5502, decode.d6.loss_dice: 0.9548, decode.d7.loss_cls: 0.2686, decode.d7.loss_mask: 0.5410, decode.d7.loss_dice: 0.9594, decode.d8.loss_cls: 0.2637, decode.d8.loss_mask: 0.5449, decode.d8.loss_dice: 0.9686, loss: 18.8226
2023-02-22 15:54:49,322 - mmseg - INFO - Iter [12850/80000]	lr: 1.205e-06, eta: 3 days, 13:13:48, time: 2.724, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2919, decode.loss_mask: 0.6005, decode.loss_dice: 0.9349, decode.d0.loss_cls: 1.0587, decode.d0.loss_mask: 0.6325, decode.d0.loss_dice: 1.0235, decode.d1.loss_cls: 0.3108, decode.d1.loss_mask: 0.6125, decode.d1.loss_dice: 0.9704, decode.d2.loss_cls: 0.2992, decode.d2.loss_mask: 0.6034, decode.d2.loss_dice: 0.9538, decode.d3.loss_cls: 0.3020, decode.d3.loss_mask: 0.6009, decode.d3.loss_dice: 0.9625, decode.d4.loss_cls: 0.2934, decode.d4.loss_mask: 0.6029, decode.d4.loss_dice: 0.9572, decode.d5.loss_cls: 0.3166, decode.d5.loss_mask: 0.6076, decode.d5.loss_dice: 0.9359, decode.d6.loss_cls: 0.3017, decode.d6.loss_mask: 0.6056, decode.d6.loss_dice: 0.9343, decode.d7.loss_cls: 0.3104, decode.d7.loss_mask: 0.5975, decode.d7.loss_dice: 0.9434, decode.d8.loss_cls: 0.2973, decode.d8.loss_mask: 0.6000, decode.d8.loss_dice: 0.9417, loss: 19.4030
2023-02-22 15:57:08,076 - mmseg - INFO - Iter [12900/80000]	lr: 1.204e-06, eta: 3 days, 13:02:13, time: 2.775, data_time: 0.066, memory: 31493, decode.loss_cls: 0.3108, decode.loss_mask: 0.5927, decode.loss_dice: 0.9535, decode.d0.loss_cls: 1.0573, decode.d0.loss_mask: 0.6356, decode.d0.loss_dice: 1.0655, decode.d1.loss_cls: 0.3603, decode.d1.loss_mask: 0.5963, decode.d1.loss_dice: 0.9864, decode.d2.loss_cls: 0.3208, decode.d2.loss_mask: 0.5935, decode.d2.loss_dice: 0.9801, decode.d3.loss_cls: 0.3059, decode.d3.loss_mask: 0.5928, decode.d3.loss_dice: 0.9615, decode.d4.loss_cls: 0.3009, decode.d4.loss_mask: 0.5943, decode.d4.loss_dice: 0.9576, decode.d5.loss_cls: 0.2952, decode.d5.loss_mask: 0.5965, decode.d5.loss_dice: 0.9621, decode.d6.loss_cls: 0.2980, decode.d6.loss_mask: 0.5943, decode.d6.loss_dice: 0.9533, decode.d7.loss_cls: 0.3028, decode.d7.loss_mask: 0.6040, decode.d7.loss_dice: 0.9471, decode.d8.loss_cls: 0.3073, decode.d8.loss_mask: 0.5964, decode.d8.loss_dice: 0.9577, loss: 19.5806
2023-02-22 15:59:24,410 - mmseg - INFO - Iter [12950/80000]	lr: 1.203e-06, eta: 3 days, 12:50:30, time: 2.727, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2985, decode.loss_mask: 0.5617, decode.loss_dice: 0.9675, decode.d0.loss_cls: 1.0537, decode.d0.loss_mask: 0.5756, decode.d0.loss_dice: 1.0351, decode.d1.loss_cls: 0.3154, decode.d1.loss_mask: 0.5615, decode.d1.loss_dice: 0.9784, decode.d2.loss_cls: 0.3030, decode.d2.loss_mask: 0.5473, decode.d2.loss_dice: 0.9764, decode.d3.loss_cls: 0.3067, decode.d3.loss_mask: 0.5507, decode.d3.loss_dice: 0.9927, decode.d4.loss_cls: 0.2959, decode.d4.loss_mask: 0.5563, decode.d4.loss_dice: 0.9592, decode.d5.loss_cls: 0.3089, decode.d5.loss_mask: 0.5528, decode.d5.loss_dice: 0.9533, decode.d6.loss_cls: 0.3010, decode.d6.loss_mask: 0.5497, decode.d6.loss_dice: 0.9733, decode.d7.loss_cls: 0.2854, decode.d7.loss_mask: 0.5484, decode.d7.loss_dice: 0.9527, decode.d8.loss_cls: 0.2882, decode.d8.loss_mask: 0.5510, decode.d8.loss_dice: 0.9624, loss: 19.0627
2023-02-22 16:01:41,122 - mmseg - INFO - Saving checkpoint at 13000 iterations
2023-02-22 16:02:01,692 - mmseg - INFO - Exp name: my_city.py
2023-02-22 16:02:01,693 - mmseg - INFO - Iter [13000/80000]	lr: 1.202e-06, eta: 3 days, 12:40:39, time: 3.146, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2915, decode.loss_mask: 0.6073, decode.loss_dice: 1.0161, decode.d0.loss_cls: 1.0476, decode.d0.loss_mask: 0.6508, decode.d0.loss_dice: 1.0819, decode.d1.loss_cls: 0.2997, decode.d1.loss_mask: 0.6172, decode.d1.loss_dice: 1.0394, decode.d2.loss_cls: 0.2928, decode.d2.loss_mask: 0.6095, decode.d2.loss_dice: 1.0046, decode.d3.loss_cls: 0.2838, decode.d3.loss_mask: 0.6145, decode.d3.loss_dice: 1.0212, decode.d4.loss_cls: 0.3051, decode.d4.loss_mask: 0.6167, decode.d4.loss_dice: 0.9996, decode.d5.loss_cls: 0.3088, decode.d5.loss_mask: 0.6111, decode.d5.loss_dice: 1.0256, decode.d6.loss_cls: 0.2969, decode.d6.loss_mask: 0.6077, decode.d6.loss_dice: 1.0282, decode.d7.loss_cls: 0.2825, decode.d7.loss_mask: 0.6134, decode.d7.loss_dice: 1.0019, decode.d8.loss_cls: 0.3071, decode.d8.loss_mask: 0.6080, decode.d8.loss_dice: 1.0114, loss: 20.1019
2023-02-22 16:04:18,334 - mmseg - INFO - Iter [13050/80000]	lr: 1.202e-06, eta: 3 days, 12:29:05, time: 2.733, data_time: 0.020, memory: 31493, decode.loss_cls: 0.3646, decode.loss_mask: 0.6843, decode.loss_dice: 1.0662, decode.d0.loss_cls: 1.1299, decode.d0.loss_mask: 0.7086, decode.d0.loss_dice: 1.1551, decode.d1.loss_cls: 0.4353, decode.d1.loss_mask: 0.6743, decode.d1.loss_dice: 1.0783, decode.d2.loss_cls: 0.4086, decode.d2.loss_mask: 0.6658, decode.d2.loss_dice: 1.0616, decode.d3.loss_cls: 0.4140, decode.d3.loss_mask: 0.6618, decode.d3.loss_dice: 1.0581, decode.d4.loss_cls: 0.3914, decode.d4.loss_mask: 0.6746, decode.d4.loss_dice: 1.0539, decode.d5.loss_cls: 0.3990, decode.d5.loss_mask: 0.6876, decode.d5.loss_dice: 1.0664, decode.d6.loss_cls: 0.3785, decode.d6.loss_mask: 0.6807, decode.d6.loss_dice: 1.0617, decode.d7.loss_cls: 0.3932, decode.d7.loss_mask: 0.6691, decode.d7.loss_dice: 1.0575, decode.d8.loss_cls: 0.3823, decode.d8.loss_mask: 0.6826, decode.d8.loss_dice: 1.0515, loss: 22.1964
2023-02-22 16:06:35,252 - mmseg - INFO - Iter [13100/80000]	lr: 1.201e-06, eta: 3 days, 12:17:37, time: 2.738, data_time: 0.020, memory: 31493, decode.loss_cls: 0.3025, decode.loss_mask: 0.6107, decode.loss_dice: 1.0586, decode.d0.loss_cls: 1.0348, decode.d0.loss_mask: 0.6191, decode.d0.loss_dice: 1.1074, decode.d1.loss_cls: 0.3163, decode.d1.loss_mask: 0.6274, decode.d1.loss_dice: 1.0920, decode.d2.loss_cls: 0.3031, decode.d2.loss_mask: 0.6132, decode.d2.loss_dice: 1.0768, decode.d3.loss_cls: 0.3210, decode.d3.loss_mask: 0.6005, decode.d3.loss_dice: 1.0646, decode.d4.loss_cls: 0.3112, decode.d4.loss_mask: 0.6043, decode.d4.loss_dice: 1.0625, decode.d5.loss_cls: 0.3114, decode.d5.loss_mask: 0.6066, decode.d5.loss_dice: 1.0706, decode.d6.loss_cls: 0.3189, decode.d6.loss_mask: 0.6030, decode.d6.loss_dice: 1.0683, decode.d7.loss_cls: 0.3284, decode.d7.loss_mask: 0.6086, decode.d7.loss_dice: 1.0363, decode.d8.loss_cls: 0.3263, decode.d8.loss_mask: 0.6069, decode.d8.loss_dice: 1.0409, loss: 20.6517
2023-02-22 16:08:51,810 - mmseg - INFO - Iter [13150/80000]	lr: 1.200e-06, eta: 3 days, 12:06:12, time: 2.731, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2904, decode.loss_mask: 0.5553, decode.loss_dice: 1.0325, decode.d0.loss_cls: 1.0351, decode.d0.loss_mask: 0.5911, decode.d0.loss_dice: 1.0960, decode.d1.loss_cls: 0.3437, decode.d1.loss_mask: 0.5534, decode.d1.loss_dice: 1.0514, decode.d2.loss_cls: 0.3191, decode.d2.loss_mask: 0.5636, decode.d2.loss_dice: 1.0135, decode.d3.loss_cls: 0.3197, decode.d3.loss_mask: 0.5672, decode.d3.loss_dice: 1.0167, decode.d4.loss_cls: 0.3062, decode.d4.loss_mask: 0.5676, decode.d4.loss_dice: 1.0280, decode.d5.loss_cls: 0.3010, decode.d5.loss_mask: 0.5648, decode.d5.loss_dice: 1.0406, decode.d6.loss_cls: 0.3090, decode.d6.loss_mask: 0.5607, decode.d6.loss_dice: 1.0181, decode.d7.loss_cls: 0.2695, decode.d7.loss_mask: 0.5656, decode.d7.loss_dice: 1.0354, decode.d8.loss_cls: 0.2723, decode.d8.loss_mask: 0.5541, decode.d8.loss_dice: 1.0310, loss: 19.7726
2023-02-22 16:11:08,709 - mmseg - INFO - Iter [13200/80000]	lr: 1.199e-06, eta: 3 days, 11:54:52, time: 2.738, data_time: 0.021, memory: 31493, decode.loss_cls: 0.3147, decode.loss_mask: 0.6015, decode.loss_dice: 1.0147, decode.d0.loss_cls: 1.0360, decode.d0.loss_mask: 0.6364, decode.d0.loss_dice: 1.0834, decode.d1.loss_cls: 0.3058, decode.d1.loss_mask: 0.6067, decode.d1.loss_dice: 1.0648, decode.d2.loss_cls: 0.3180, decode.d2.loss_mask: 0.6002, decode.d2.loss_dice: 1.0299, decode.d3.loss_cls: 0.2985, decode.d3.loss_mask: 0.5966, decode.d3.loss_dice: 1.0302, decode.d4.loss_cls: 0.3009, decode.d4.loss_mask: 0.5985, decode.d4.loss_dice: 1.0165, decode.d5.loss_cls: 0.3074, decode.d5.loss_mask: 0.6048, decode.d5.loss_dice: 1.0194, decode.d6.loss_cls: 0.3039, decode.d6.loss_mask: 0.5985, decode.d6.loss_dice: 1.0263, decode.d7.loss_cls: 0.3116, decode.d7.loss_mask: 0.5969, decode.d7.loss_dice: 1.0139, decode.d8.loss_cls: 0.3080, decode.d8.loss_mask: 0.6015, decode.d8.loss_dice: 0.9996, loss: 20.1450
2023-02-22 16:13:25,209 - mmseg - INFO - Iter [13250/80000]	lr: 1.198e-06, eta: 3 days, 11:43:34, time: 2.730, data_time: 0.020, memory: 31493, decode.loss_cls: 0.2833, decode.loss_mask: 0.5806, decode.loss_dice: 0.9597, decode.d0.loss_cls: 1.0182, decode.d0.loss_mask: 0.5978, decode.d0.loss_dice: 1.0345, decode.d1.loss_cls: 0.3302, decode.d1.loss_mask: 0.5940, decode.d1.loss_dice: 0.9875, decode.d2.loss_cls: 0.3093, decode.d2.loss_mask: 0.5933, decode.d2.loss_dice: 0.9700, decode.d3.loss_cls: 0.3028, decode.d3.loss_mask: 0.5766, decode.d3.loss_dice: 0.9525, decode.d4.loss_cls: 0.2985, decode.d4.loss_mask: 0.5834, decode.d4.loss_dice: 0.9615, decode.d5.loss_cls: 0.2873, decode.d5.loss_mask: 0.5796, decode.d5.loss_dice: 0.9445, decode.d6.loss_cls: 0.2834, decode.d6.loss_mask: 0.5715, decode.d6.loss_dice: 0.9603, decode.d7.loss_cls: 0.2930, decode.d7.loss_mask: 0.5701, decode.d7.loss_dice: 0.9515, decode.d8.loss_cls: 0.2959, decode.d8.loss_mask: 0.5758, decode.d8.loss_dice: 0.9483, loss: 19.1949
2023-02-22 16:15:42,039 - mmseg - INFO - Iter [13300/80000]	lr: 1.197e-06, eta: 3 days, 11:32:22, time: 2.737, data_time: 0.020, memory: 31493, decode.loss_cls: 0.3240, decode.loss_mask: 0.6257, decode.loss_dice: 1.0226, decode.d0.loss_cls: 1.0516, decode.d0.loss_mask: 0.6923, decode.d0.loss_dice: 1.1481, decode.d1.loss_cls: 0.3457, decode.d1.loss_mask: 0.6475, decode.d1.loss_dice: 1.0788, decode.d2.loss_cls: 0.3177, decode.d2.loss_mask: 0.6370, decode.d2.loss_dice: 1.0617, decode.d3.loss_cls: 0.3484, decode.d3.loss_mask: 0.6100, decode.d3.loss_dice: 1.0320, decode.d4.loss_cls: 0.3307, decode.d4.loss_mask: 0.6352, decode.d4.loss_dice: 1.0400, decode.d5.loss_cls: 0.3361, decode.d5.loss_mask: 0.6247, decode.d5.loss_dice: 1.0361, decode.d6.loss_cls: 0.3333, decode.d6.loss_mask: 0.6171, decode.d6.loss_dice: 1.0245, decode.d7.loss_cls: 0.3206, decode.d7.loss_mask: 0.6322, decode.d7.loss_dice: 1.0238, decode.d8.loss_cls: 0.3250, decode.d8.loss_mask: 0.6212, decode.d8.loss_dice: 1.0270, loss: 20.8708
2023-02-22 16:17:58,701 - mmseg - INFO - Iter [13350/80000]	lr: 1.196e-06, eta: 3 days, 11:21:14, time: 2.733, data_time: 0.020, memory: 31493, decode.loss_cls: 0.3063, decode.loss_mask: 0.5855, decode.loss_dice: 0.9900, decode.d0.loss_cls: 1.0807, decode.d0.loss_mask: 0.6364, decode.d0.loss_dice: 1.0897, decode.d1.loss_cls: 0.3259, decode.d1.loss_mask: 0.5974, decode.d1.loss_dice: 1.0214, decode.d2.loss_cls: 0.3269, decode.d2.loss_mask: 0.5958, decode.d2.loss_dice: 0.9996, decode.d3.loss_cls: 0.3027, decode.d3.loss_mask: 0.5924, decode.d3.loss_dice: 0.9945, decode.d4.loss_cls: 0.2985, decode.d4.loss_mask: 0.5931, decode.d4.loss_dice: 1.0159, decode.d5.loss_cls: 0.3197, decode.d5.loss_mask: 0.5965, decode.d5.loss_dice: 1.0207, decode.d6.loss_cls: 0.3282, decode.d6.loss_mask: 0.5908, decode.d6.loss_dice: 0.9917, decode.d7.loss_cls: 0.3167, decode.d7.loss_mask: 0.5799, decode.d7.loss_dice: 0.9873, decode.d8.loss_cls: 0.3186, decode.d8.loss_mask: 0.5936, decode.d8.loss_dice: 0.9680, loss: 19.9645
2023-02-22 16:20:15,269 - mmseg - INFO - Iter [13400/80000]	lr: 1.195e-06, eta: 3 days, 11:10:09, time: 2.731, data_time: 0.020, memory: 31493, decode.loss_cls: 0.3018, decode.loss_mask: 0.6182, decode.loss_dice: 1.0499, decode.d0.loss_cls: 0.9998, decode.d0.loss_mask: 0.6394, decode.d0.loss_dice: 1.1485, decode.d1.loss_cls: 0.3307, decode.d1.loss_mask: 0.6182, decode.d1.loss_dice: 1.0983, decode.d2.loss_cls: 0.3322, decode.d2.loss_mask: 0.6076, decode.d2.loss_dice: 1.0754, decode.d3.loss_cls: 0.3181, decode.d3.loss_mask: 0.6121, decode.d3.loss_dice: 1.0453, decode.d4.loss_cls: 0.3098, decode.d4.loss_mask: 0.6098, decode.d4.loss_dice: 1.0473, decode.d5.loss_cls: 0.3253, decode.d5.loss_mask: 0.6118, decode.d5.loss_dice: 1.0401, decode.d6.loss_cls: 0.2978, decode.d6.loss_mask: 0.6130, decode.d6.loss_dice: 1.0404, decode.d7.loss_cls: 0.2922, decode.d7.loss_mask: 0.6125, decode.d7.loss_dice: 1.0356, decode.d8.loss_cls: 0.3001, decode.d8.loss_mask: 0.6165, decode.d8.loss_dice: 1.0439, loss: 20.5915
2023-02-22 16:22:32,232 - mmseg - INFO - Iter [13450/80000]	lr: 1.194e-06, eta: 3 days, 10:59:09, time: 2.739, data_time: 0.021, memory: 31493, decode.loss_cls: 0.3049, decode.loss_mask: 0.5916, decode.loss_dice: 1.0066, decode.d0.loss_cls: 1.0696, decode.d0.loss_mask: 0.6447, decode.d0.loss_dice: 1.0923, decode.d1.loss_cls: 0.3356, decode.d1.loss_mask: 0.6091, decode.d1.loss_dice: 1.0547, decode.d2.loss_cls: 0.3174, decode.d2.loss_mask: 0.6036, decode.d2.loss_dice: 1.0323, decode.d3.loss_cls: 0.3277, decode.d3.loss_mask: 0.5939, decode.d3.loss_dice: 1.0024, decode.d4.loss_cls: 0.3139, decode.d4.loss_mask: 0.5953, decode.d4.loss_dice: 1.0156, decode.d5.loss_cls: 0.3228, decode.d5.loss_mask: 0.6076, decode.d5.loss_dice: 1.0088, decode.d6.loss_cls: 0.3170, decode.d6.loss_mask: 0.5997, decode.d6.loss_dice: 1.0041, decode.d7.loss_cls: 0.3064, decode.d7.loss_mask: 0.5980, decode.d7.loss_dice: 1.0036, decode.d8.loss_cls: 0.3118, decode.d8.loss_mask: 0.5937, decode.d8.loss_dice: 1.0116, loss: 20.1968
2023-02-22 16:24:49,081 - mmseg - INFO - Iter [13500/80000]	lr: 1.194e-06, eta: 3 days, 10:48:13, time: 2.737, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2945, decode.loss_mask: 0.5631, decode.loss_dice: 1.0418, decode.d0.loss_cls: 1.0305, decode.d0.loss_mask: 0.5940, decode.d0.loss_dice: 1.0913, decode.d1.loss_cls: 0.3138, decode.d1.loss_mask: 0.5746, decode.d1.loss_dice: 1.0790, decode.d2.loss_cls: 0.3109, decode.d2.loss_mask: 0.5637, decode.d2.loss_dice: 1.0296, decode.d3.loss_cls: 0.3299, decode.d3.loss_mask: 0.5594, decode.d3.loss_dice: 1.0285, decode.d4.loss_cls: 0.3047, decode.d4.loss_mask: 0.5604, decode.d4.loss_dice: 1.0322, decode.d5.loss_cls: 0.3067, decode.d5.loss_mask: 0.5571, decode.d5.loss_dice: 1.0384, decode.d6.loss_cls: 0.2949, decode.d6.loss_mask: 0.5577, decode.d6.loss_dice: 1.0226, decode.d7.loss_cls: 0.3149, decode.d7.loss_mask: 0.5589, decode.d7.loss_dice: 1.0419, decode.d8.loss_cls: 0.2997, decode.d8.loss_mask: 0.5617, decode.d8.loss_dice: 1.0344, loss: 19.8907
2023-02-22 16:27:05,971 - mmseg - INFO - Iter [13550/80000]	lr: 1.193e-06, eta: 3 days, 10:37:21, time: 2.738, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2485, decode.loss_mask: 0.5465, decode.loss_dice: 1.0163, decode.d0.loss_cls: 0.9853, decode.d0.loss_mask: 0.5706, decode.d0.loss_dice: 1.0934, decode.d1.loss_cls: 0.2983, decode.d1.loss_mask: 0.5526, decode.d1.loss_dice: 1.0473, decode.d2.loss_cls: 0.2536, decode.d2.loss_mask: 0.5505, decode.d2.loss_dice: 1.0380, decode.d3.loss_cls: 0.2527, decode.d3.loss_mask: 0.5507, decode.d3.loss_dice: 1.0238, decode.d4.loss_cls: 0.2629, decode.d4.loss_mask: 0.5526, decode.d4.loss_dice: 1.0048, decode.d5.loss_cls: 0.2679, decode.d5.loss_mask: 0.5466, decode.d5.loss_dice: 1.0118, decode.d6.loss_cls: 0.2606, decode.d6.loss_mask: 0.5473, decode.d6.loss_dice: 1.0114, decode.d7.loss_cls: 0.2484, decode.d7.loss_mask: 0.5481, decode.d7.loss_dice: 1.0190, decode.d8.loss_cls: 0.2639, decode.d8.loss_mask: 0.5441, decode.d8.loss_dice: 1.0121, loss: 19.1295
2023-02-22 16:29:24,982 - mmseg - INFO - Iter [13600/80000]	lr: 1.192e-06, eta: 3 days, 10:26:43, time: 2.780, data_time: 0.069, memory: 31493, decode.loss_cls: 0.2974, decode.loss_mask: 0.5702, decode.loss_dice: 1.0008, decode.d0.loss_cls: 1.0767, decode.d0.loss_mask: 0.6206, decode.d0.loss_dice: 1.0613, decode.d1.loss_cls: 0.3341, decode.d1.loss_mask: 0.5885, decode.d1.loss_dice: 1.0345, decode.d2.loss_cls: 0.3103, decode.d2.loss_mask: 0.5751, decode.d2.loss_dice: 0.9882, decode.d3.loss_cls: 0.3130, decode.d3.loss_mask: 0.5719, decode.d3.loss_dice: 0.9894, decode.d4.loss_cls: 0.3257, decode.d4.loss_mask: 0.5687, decode.d4.loss_dice: 0.9998, decode.d5.loss_cls: 0.2962, decode.d5.loss_mask: 0.5705, decode.d5.loss_dice: 1.0041, decode.d6.loss_cls: 0.2985, decode.d6.loss_mask: 0.5712, decode.d6.loss_dice: 1.0028, decode.d7.loss_cls: 0.2948, decode.d7.loss_mask: 0.5697, decode.d7.loss_dice: 0.9902, decode.d8.loss_cls: 0.3126, decode.d8.loss_mask: 0.5684, decode.d8.loss_dice: 0.9933, loss: 19.6985
2023-02-22 16:31:41,660 - mmseg - INFO - Iter [13650/80000]	lr: 1.191e-06, eta: 3 days, 10:15:58, time: 2.734, data_time: 0.021, memory: 31493, decode.loss_cls: 0.3142, decode.loss_mask: 0.6149, decode.loss_dice: 1.0310, decode.d0.loss_cls: 1.0021, decode.d0.loss_mask: 0.6560, decode.d0.loss_dice: 1.0913, decode.d1.loss_cls: 0.3067, decode.d1.loss_mask: 0.6126, decode.d1.loss_dice: 1.0522, decode.d2.loss_cls: 0.3185, decode.d2.loss_mask: 0.5993, decode.d2.loss_dice: 1.0231, decode.d3.loss_cls: 0.2999, decode.d3.loss_mask: 0.6052, decode.d3.loss_dice: 1.0333, decode.d4.loss_cls: 0.2969, decode.d4.loss_mask: 0.6081, decode.d4.loss_dice: 1.0202, decode.d5.loss_cls: 0.2957, decode.d5.loss_mask: 0.6097, decode.d5.loss_dice: 1.0487, decode.d6.loss_cls: 0.2943, decode.d6.loss_mask: 0.6089, decode.d6.loss_dice: 1.0141, decode.d7.loss_cls: 0.2978, decode.d7.loss_mask: 0.6060, decode.d7.loss_dice: 1.0215, decode.d8.loss_cls: 0.3226, decode.d8.loss_mask: 0.6181, decode.d8.loss_dice: 1.0301, loss: 20.2532
2023-02-22 16:33:58,316 - mmseg - INFO - Iter [13700/80000]	lr: 1.190e-06, eta: 3 days, 10:05:16, time: 2.733, data_time: 0.021, memory: 31493, decode.loss_cls: 0.3497, decode.loss_mask: 0.5790, decode.loss_dice: 1.0075, decode.d0.loss_cls: 1.0189, decode.d0.loss_mask: 0.6160, decode.d0.loss_dice: 1.0687, decode.d1.loss_cls: 0.3489, decode.d1.loss_mask: 0.5892, decode.d1.loss_dice: 1.0390, decode.d2.loss_cls: 0.2999, decode.d2.loss_mask: 0.5874, decode.d2.loss_dice: 0.9981, decode.d3.loss_cls: 0.3073, decode.d3.loss_mask: 0.5825, decode.d3.loss_dice: 1.0023, decode.d4.loss_cls: 0.3298, decode.d4.loss_mask: 0.5723, decode.d4.loss_dice: 0.9997, decode.d5.loss_cls: 0.3377, decode.d5.loss_mask: 0.5780, decode.d5.loss_dice: 1.0129, decode.d6.loss_cls: 0.3141, decode.d6.loss_mask: 0.5850, decode.d6.loss_dice: 0.9965, decode.d7.loss_cls: 0.3271, decode.d7.loss_mask: 0.5878, decode.d7.loss_dice: 1.0073, decode.d8.loss_cls: 0.3291, decode.d8.loss_mask: 0.5826, decode.d8.loss_dice: 0.9987, loss: 19.9532
2023-02-22 16:36:15,202 - mmseg - INFO - Iter [13750/80000]	lr: 1.189e-06, eta: 3 days, 9:54:39, time: 2.738, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2983, decode.loss_mask: 0.5844, decode.loss_dice: 0.9977, decode.d0.loss_cls: 0.9687, decode.d0.loss_mask: 0.6195, decode.d0.loss_dice: 1.0737, decode.d1.loss_cls: 0.3550, decode.d1.loss_mask: 0.5950, decode.d1.loss_dice: 1.0183, decode.d2.loss_cls: 0.3260, decode.d2.loss_mask: 0.5882, decode.d2.loss_dice: 0.9997, decode.d3.loss_cls: 0.3001, decode.d3.loss_mask: 0.5945, decode.d3.loss_dice: 1.0196, decode.d4.loss_cls: 0.3006, decode.d4.loss_mask: 0.5910, decode.d4.loss_dice: 1.0148, decode.d5.loss_cls: 0.3086, decode.d5.loss_mask: 0.5865, decode.d5.loss_dice: 0.9955, decode.d6.loss_cls: 0.3250, decode.d6.loss_mask: 0.5740, decode.d6.loss_dice: 0.9936, decode.d7.loss_cls: 0.3109, decode.d7.loss_mask: 0.5794, decode.d7.loss_dice: 0.9958, decode.d8.loss_cls: 0.3188, decode.d8.loss_mask: 0.5841, decode.d8.loss_dice: 0.9959, loss: 19.8132
2023-02-22 16:38:32,113 - mmseg - INFO - Iter [13800/80000]	lr: 1.188e-06, eta: 3 days, 9:44:06, time: 2.738, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2885, decode.loss_mask: 0.5604, decode.loss_dice: 0.9590, decode.d0.loss_cls: 0.9800, decode.d0.loss_mask: 0.5956, decode.d0.loss_dice: 1.0351, decode.d1.loss_cls: 0.3163, decode.d1.loss_mask: 0.5610, decode.d1.loss_dice: 0.9782, decode.d2.loss_cls: 0.3055, decode.d2.loss_mask: 0.5531, decode.d2.loss_dice: 0.9524, decode.d3.loss_cls: 0.2943, decode.d3.loss_mask: 0.5514, decode.d3.loss_dice: 0.9611, decode.d4.loss_cls: 0.2734, decode.d4.loss_mask: 0.5605, decode.d4.loss_dice: 0.9704, decode.d5.loss_cls: 0.2968, decode.d5.loss_mask: 0.5580, decode.d5.loss_dice: 0.9744, decode.d6.loss_cls: 0.3099, decode.d6.loss_mask: 0.5582, decode.d6.loss_dice: 0.9661, decode.d7.loss_cls: 0.2699, decode.d7.loss_mask: 0.5614, decode.d7.loss_dice: 0.9549, decode.d8.loss_cls: 0.2761, decode.d8.loss_mask: 0.5612, decode.d8.loss_dice: 0.9593, loss: 18.9426
2023-02-22 16:40:49,052 - mmseg - INFO - Iter [13850/80000]	lr: 1.187e-06, eta: 3 days, 9:33:36, time: 2.739, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2303, decode.loss_mask: 0.5381, decode.loss_dice: 0.9264, decode.d0.loss_cls: 0.9691, decode.d0.loss_mask: 0.5433, decode.d0.loss_dice: 0.9920, decode.d1.loss_cls: 0.2571, decode.d1.loss_mask: 0.5351, decode.d1.loss_dice: 0.9436, decode.d2.loss_cls: 0.2255, decode.d2.loss_mask: 0.5365, decode.d2.loss_dice: 0.9372, decode.d3.loss_cls: 0.2149, decode.d3.loss_mask: 0.5350, decode.d3.loss_dice: 0.9299, decode.d4.loss_cls: 0.2044, decode.d4.loss_mask: 0.5349, decode.d4.loss_dice: 0.9272, decode.d5.loss_cls: 0.2180, decode.d5.loss_mask: 0.5332, decode.d5.loss_dice: 0.9325, decode.d6.loss_cls: 0.1965, decode.d6.loss_mask: 0.5329, decode.d6.loss_dice: 0.9430, decode.d7.loss_cls: 0.2341, decode.d7.loss_mask: 0.5289, decode.d7.loss_dice: 0.9264, decode.d8.loss_cls: 0.2456, decode.d8.loss_mask: 0.5319, decode.d8.loss_dice: 0.9294, loss: 17.7332
2023-02-22 16:43:05,770 - mmseg - INFO - Iter [13900/80000]	lr: 1.186e-06, eta: 3 days, 9:23:09, time: 2.734, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2454, decode.loss_mask: 0.5322, decode.loss_dice: 0.9243, decode.d0.loss_cls: 0.9829, decode.d0.loss_mask: 0.5787, decode.d0.loss_dice: 0.9990, decode.d1.loss_cls: 0.2697, decode.d1.loss_mask: 0.5539, decode.d1.loss_dice: 0.9796, decode.d2.loss_cls: 0.2732, decode.d2.loss_mask: 0.5413, decode.d2.loss_dice: 0.9478, decode.d3.loss_cls: 0.2671, decode.d3.loss_mask: 0.5301, decode.d3.loss_dice: 0.9377, decode.d4.loss_cls: 0.2638, decode.d4.loss_mask: 0.5290, decode.d4.loss_dice: 0.9242, decode.d5.loss_cls: 0.2400, decode.d5.loss_mask: 0.5306, decode.d5.loss_dice: 0.9444, decode.d6.loss_cls: 0.2467, decode.d6.loss_mask: 0.5290, decode.d6.loss_dice: 0.9365, decode.d7.loss_cls: 0.2274, decode.d7.loss_mask: 0.5334, decode.d7.loss_dice: 0.9357, decode.d8.loss_cls: 0.2435, decode.d8.loss_mask: 0.5294, decode.d8.loss_dice: 0.9219, loss: 18.0985
2023-02-22 16:45:23,167 - mmseg - INFO - Iter [13950/80000]	lr: 1.185e-06, eta: 3 days, 9:12:48, time: 2.748, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2728, decode.loss_mask: 0.5639, decode.loss_dice: 0.9511, decode.d0.loss_cls: 0.9462, decode.d0.loss_mask: 0.5869, decode.d0.loss_dice: 1.0197, decode.d1.loss_cls: 0.3085, decode.d1.loss_mask: 0.5737, decode.d1.loss_dice: 0.9790, decode.d2.loss_cls: 0.3129, decode.d2.loss_mask: 0.5632, decode.d2.loss_dice: 0.9517, decode.d3.loss_cls: 0.2888, decode.d3.loss_mask: 0.5696, decode.d3.loss_dice: 0.9358, decode.d4.loss_cls: 0.3188, decode.d4.loss_mask: 0.5697, decode.d4.loss_dice: 0.9388, decode.d5.loss_cls: 0.3017, decode.d5.loss_mask: 0.5638, decode.d5.loss_dice: 0.9496, decode.d6.loss_cls: 0.2956, decode.d6.loss_mask: 0.5647, decode.d6.loss_dice: 0.9384, decode.d7.loss_cls: 0.2818, decode.d7.loss_mask: 0.5677, decode.d7.loss_dice: 0.9496, decode.d8.loss_cls: 0.2900, decode.d8.loss_mask: 0.5647, decode.d8.loss_dice: 0.9477, loss: 18.8663
2023-02-22 16:47:40,335 - mmseg - INFO - Saving checkpoint at 14000 iterations
2023-02-22 16:48:01,562 - mmseg - INFO - Exp name: my_city.py
2023-02-22 16:48:01,562 - mmseg - INFO - Iter [14000/80000]	lr: 1.185e-06, eta: 3 days, 9:04:10, time: 3.168, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2777, decode.loss_mask: 0.5655, decode.loss_dice: 0.9924, decode.d0.loss_cls: 0.9469, decode.d0.loss_mask: 0.5939, decode.d0.loss_dice: 1.0419, decode.d1.loss_cls: 0.2966, decode.d1.loss_mask: 0.5742, decode.d1.loss_dice: 1.0446, decode.d2.loss_cls: 0.2828, decode.d2.loss_mask: 0.5708, decode.d2.loss_dice: 0.9853, decode.d3.loss_cls: 0.2723, decode.d3.loss_mask: 0.5638, decode.d3.loss_dice: 0.9815, decode.d4.loss_cls: 0.2696, decode.d4.loss_mask: 0.5630, decode.d4.loss_dice: 0.9930, decode.d5.loss_cls: 0.2636, decode.d5.loss_mask: 0.5661, decode.d5.loss_dice: 1.0117, decode.d6.loss_cls: 0.2769, decode.d6.loss_mask: 0.5681, decode.d6.loss_dice: 1.0011, decode.d7.loss_cls: 0.2629, decode.d7.loss_mask: 0.5719, decode.d7.loss_dice: 0.9862, decode.d8.loss_cls: 0.2988, decode.d8.loss_mask: 0.5647, decode.d8.loss_dice: 0.9832, loss: 19.1710
2023-02-22 16:50:18,625 - mmseg - INFO - Iter [14050/80000]	lr: 1.184e-06, eta: 3 days, 8:53:55, time: 2.741, data_time: 0.020, memory: 31493, decode.loss_cls: 0.2731, decode.loss_mask: 0.5677, decode.loss_dice: 0.9616, decode.d0.loss_cls: 0.9902, decode.d0.loss_mask: 0.6027, decode.d0.loss_dice: 1.0558, decode.d1.loss_cls: 0.3147, decode.d1.loss_mask: 0.5885, decode.d1.loss_dice: 1.0044, decode.d2.loss_cls: 0.2876, decode.d2.loss_mask: 0.5759, decode.d2.loss_dice: 0.9681, decode.d3.loss_cls: 0.2594, decode.d3.loss_mask: 0.5843, decode.d3.loss_dice: 0.9660, decode.d4.loss_cls: 0.2663, decode.d4.loss_mask: 0.5822, decode.d4.loss_dice: 0.9603, decode.d5.loss_cls: 0.2577, decode.d5.loss_mask: 0.5818, decode.d5.loss_dice: 0.9569, decode.d6.loss_cls: 0.2623, decode.d6.loss_mask: 0.5756, decode.d6.loss_dice: 0.9723, decode.d7.loss_cls: 0.2517, decode.d7.loss_mask: 0.5886, decode.d7.loss_dice: 0.9579, decode.d8.loss_cls: 0.2652, decode.d8.loss_mask: 0.5771, decode.d8.loss_dice: 0.9628, loss: 19.0188
2023-02-22 16:52:35,432 - mmseg - INFO - Iter [14100/80000]	lr: 1.183e-06, eta: 3 days, 8:43:41, time: 2.736, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2607, decode.loss_mask: 0.5572, decode.loss_dice: 1.0156, decode.d0.loss_cls: 0.9679, decode.d0.loss_mask: 0.5859, decode.d0.loss_dice: 1.1019, decode.d1.loss_cls: 0.2621, decode.d1.loss_mask: 0.5728, decode.d1.loss_dice: 1.0986, decode.d2.loss_cls: 0.2898, decode.d2.loss_mask: 0.5646, decode.d2.loss_dice: 1.0631, decode.d3.loss_cls: 0.2823, decode.d3.loss_mask: 0.5593, decode.d3.loss_dice: 1.0143, decode.d4.loss_cls: 0.2691, decode.d4.loss_mask: 0.5578, decode.d4.loss_dice: 1.0311, decode.d5.loss_cls: 0.2730, decode.d5.loss_mask: 0.5599, decode.d5.loss_dice: 1.0186, decode.d6.loss_cls: 0.2749, decode.d6.loss_mask: 0.5614, decode.d6.loss_dice: 1.0277, decode.d7.loss_cls: 0.2695, decode.d7.loss_mask: 0.5561, decode.d7.loss_dice: 1.0133, decode.d8.loss_cls: 0.2653, decode.d8.loss_mask: 0.5622, decode.d8.loss_dice: 1.0049, loss: 19.4411
2023-02-22 16:54:52,369 - mmseg - INFO - Iter [14150/80000]	lr: 1.182e-06, eta: 3 days, 8:33:32, time: 2.738, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2827, decode.loss_mask: 0.5554, decode.loss_dice: 0.9631, decode.d0.loss_cls: 0.9423, decode.d0.loss_mask: 0.5705, decode.d0.loss_dice: 1.0126, decode.d1.loss_cls: 0.3035, decode.d1.loss_mask: 0.5556, decode.d1.loss_dice: 0.9785, decode.d2.loss_cls: 0.2840, decode.d2.loss_mask: 0.5596, decode.d2.loss_dice: 0.9481, decode.d3.loss_cls: 0.2693, decode.d3.loss_mask: 0.5525, decode.d3.loss_dice: 0.9480, decode.d4.loss_cls: 0.2550, decode.d4.loss_mask: 0.5558, decode.d4.loss_dice: 0.9694, decode.d5.loss_cls: 0.2938, decode.d5.loss_mask: 0.5530, decode.d5.loss_dice: 0.9667, decode.d6.loss_cls: 0.2686, decode.d6.loss_mask: 0.5504, decode.d6.loss_dice: 0.9618, decode.d7.loss_cls: 0.2645, decode.d7.loss_mask: 0.5547, decode.d7.loss_dice: 0.9525, decode.d8.loss_cls: 0.2633, decode.d8.loss_mask: 0.5600, decode.d8.loss_dice: 0.9619, loss: 18.6571
2023-02-22 16:57:08,954 - mmseg - INFO - Iter [14200/80000]	lr: 1.181e-06, eta: 3 days, 8:23:24, time: 2.732, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2789, decode.loss_mask: 0.6111, decode.loss_dice: 0.9879, decode.d0.loss_cls: 0.9566, decode.d0.loss_mask: 0.6478, decode.d0.loss_dice: 1.0700, decode.d1.loss_cls: 0.3217, decode.d1.loss_mask: 0.6276, decode.d1.loss_dice: 1.0215, decode.d2.loss_cls: 0.2834, decode.d2.loss_mask: 0.6120, decode.d2.loss_dice: 0.9924, decode.d3.loss_cls: 0.2890, decode.d3.loss_mask: 0.6224, decode.d3.loss_dice: 0.9972, decode.d4.loss_cls: 0.2944, decode.d4.loss_mask: 0.6231, decode.d4.loss_dice: 0.9951, decode.d5.loss_cls: 0.2960, decode.d5.loss_mask: 0.6201, decode.d5.loss_dice: 0.9753, decode.d6.loss_cls: 0.2972, decode.d6.loss_mask: 0.6113, decode.d6.loss_dice: 0.9859, decode.d7.loss_cls: 0.3030, decode.d7.loss_mask: 0.6127, decode.d7.loss_dice: 0.9827, decode.d8.loss_cls: 0.2961, decode.d8.loss_mask: 0.6104, decode.d8.loss_dice: 0.9815, loss: 19.8041
2023-02-22 16:59:25,577 - mmseg - INFO - Iter [14250/80000]	lr: 1.180e-06, eta: 3 days, 8:13:20, time: 2.732, data_time: 0.021, memory: 31493, decode.loss_cls: 0.3259, decode.loss_mask: 0.6515, decode.loss_dice: 1.0113, decode.d0.loss_cls: 1.0334, decode.d0.loss_mask: 0.6942, decode.d0.loss_dice: 1.0966, decode.d1.loss_cls: 0.3476, decode.d1.loss_mask: 0.6638, decode.d1.loss_dice: 1.0514, decode.d2.loss_cls: 0.3132, decode.d2.loss_mask: 0.6528, decode.d2.loss_dice: 1.0193, decode.d3.loss_cls: 0.3383, decode.d3.loss_mask: 0.6479, decode.d3.loss_dice: 1.0092, decode.d4.loss_cls: 0.3270, decode.d4.loss_mask: 0.6506, decode.d4.loss_dice: 1.0195, decode.d5.loss_cls: 0.3129, decode.d5.loss_mask: 0.6540, decode.d5.loss_dice: 1.0057, decode.d6.loss_cls: 0.2976, decode.d6.loss_mask: 0.6519, decode.d6.loss_dice: 1.0098, decode.d7.loss_cls: 0.3077, decode.d7.loss_mask: 0.6486, decode.d7.loss_dice: 1.0242, decode.d8.loss_cls: 0.3034, decode.d8.loss_mask: 0.6512, decode.d8.loss_dice: 1.0048, loss: 20.7251
2023-02-22 17:01:44,566 - mmseg - INFO - Iter [14300/80000]	lr: 1.179e-06, eta: 3 days, 8:03:30, time: 2.780, data_time: 0.069, memory: 31493, decode.loss_cls: 0.3257, decode.loss_mask: 0.5408, decode.loss_dice: 0.9958, decode.d0.loss_cls: 0.9657, decode.d0.loss_mask: 0.5874, decode.d0.loss_dice: 1.0670, decode.d1.loss_cls: 0.3225, decode.d1.loss_mask: 0.5541, decode.d1.loss_dice: 1.0181, decode.d2.loss_cls: 0.3624, decode.d2.loss_mask: 0.5416, decode.d2.loss_dice: 1.0036, decode.d3.loss_cls: 0.3272, decode.d3.loss_mask: 0.5428, decode.d3.loss_dice: 1.0024, decode.d4.loss_cls: 0.3310, decode.d4.loss_mask: 0.5431, decode.d4.loss_dice: 1.0016, decode.d5.loss_cls: 0.3138, decode.d5.loss_mask: 0.5417, decode.d5.loss_dice: 0.9946, decode.d6.loss_cls: 0.3394, decode.d6.loss_mask: 0.5360, decode.d6.loss_dice: 1.0018, decode.d7.loss_cls: 0.3163, decode.d7.loss_mask: 0.5407, decode.d7.loss_dice: 0.9909, decode.d8.loss_cls: 0.3309, decode.d8.loss_mask: 0.5390, decode.d8.loss_dice: 0.9893, loss: 19.4673
2023-02-22 17:04:01,415 - mmseg - INFO - Iter [14350/80000]	lr: 1.178e-06, eta: 3 days, 7:53:33, time: 2.737, data_time: 0.021, memory: 31493, decode.loss_cls: 0.3397, decode.loss_mask: 0.5419, decode.loss_dice: 0.9672, decode.d0.loss_cls: 1.0596, decode.d0.loss_mask: 0.5775, decode.d0.loss_dice: 1.0998, decode.d1.loss_cls: 0.4071, decode.d1.loss_mask: 0.5701, decode.d1.loss_dice: 1.0208, decode.d2.loss_cls: 0.3951, decode.d2.loss_mask: 0.5591, decode.d2.loss_dice: 0.9962, decode.d3.loss_cls: 0.3651, decode.d3.loss_mask: 0.5542, decode.d3.loss_dice: 0.9908, decode.d4.loss_cls: 0.3684, decode.d4.loss_mask: 0.5459, decode.d4.loss_dice: 0.9917, decode.d5.loss_cls: 0.3549, decode.d5.loss_mask: 0.5514, decode.d5.loss_dice: 0.9806, decode.d6.loss_cls: 0.3793, decode.d6.loss_mask: 0.5427, decode.d6.loss_dice: 0.9827, decode.d7.loss_cls: 0.3470, decode.d7.loss_mask: 0.5460, decode.d7.loss_dice: 0.9725, decode.d8.loss_cls: 0.3571, decode.d8.loss_mask: 0.5423, decode.d8.loss_dice: 0.9780, loss: 19.8847
2023-02-22 17:06:18,315 - mmseg - INFO - Iter [14400/80000]	lr: 1.177e-06, eta: 3 days, 7:43:40, time: 2.738, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2483, decode.loss_mask: 0.5589, decode.loss_dice: 0.9753, decode.d0.loss_cls: 0.9364, decode.d0.loss_mask: 0.5904, decode.d0.loss_dice: 1.0552, decode.d1.loss_cls: 0.2900, decode.d1.loss_mask: 0.5700, decode.d1.loss_dice: 1.0123, decode.d2.loss_cls: 0.2800, decode.d2.loss_mask: 0.5648, decode.d2.loss_dice: 1.0056, decode.d3.loss_cls: 0.2648, decode.d3.loss_mask: 0.5562, decode.d3.loss_dice: 0.9828, decode.d4.loss_cls: 0.2543, decode.d4.loss_mask: 0.5566, decode.d4.loss_dice: 0.9822, decode.d5.loss_cls: 0.2698, decode.d5.loss_mask: 0.5641, decode.d5.loss_dice: 0.9843, decode.d6.loss_cls: 0.2723, decode.d6.loss_mask: 0.5612, decode.d6.loss_dice: 0.9741, decode.d7.loss_cls: 0.2602, decode.d7.loss_mask: 0.5611, decode.d7.loss_dice: 0.9780, decode.d8.loss_cls: 0.2598, decode.d8.loss_mask: 0.5585, decode.d8.loss_dice: 0.9838, loss: 18.9113
2023-02-22 17:08:35,051 - mmseg - INFO - Iter [14450/80000]	lr: 1.176e-06, eta: 3 days, 7:33:49, time: 2.735, data_time: 0.020, memory: 31493, decode.loss_cls: 0.3030, decode.loss_mask: 0.5689, decode.loss_dice: 0.9469, decode.d0.loss_cls: 0.9765, decode.d0.loss_mask: 0.6101, decode.d0.loss_dice: 1.0522, decode.d1.loss_cls: 0.3138, decode.d1.loss_mask: 0.5799, decode.d1.loss_dice: 0.9846, decode.d2.loss_cls: 0.2878, decode.d2.loss_mask: 0.5757, decode.d2.loss_dice: 0.9832, decode.d3.loss_cls: 0.2971, decode.d3.loss_mask: 0.5769, decode.d3.loss_dice: 0.9699, decode.d4.loss_cls: 0.3219, decode.d4.loss_mask: 0.5713, decode.d4.loss_dice: 0.9540, decode.d5.loss_cls: 0.3113, decode.d5.loss_mask: 0.5695, decode.d5.loss_dice: 0.9373, decode.d6.loss_cls: 0.3072, decode.d6.loss_mask: 0.5679, decode.d6.loss_dice: 0.9462, decode.d7.loss_cls: 0.2968, decode.d7.loss_mask: 0.5662, decode.d7.loss_dice: 0.9610, decode.d8.loss_cls: 0.3064, decode.d8.loss_mask: 0.5718, decode.d8.loss_dice: 0.9557, loss: 19.1709
2023-02-22 17:10:51,914 - mmseg - INFO - Iter [14500/80000]	lr: 1.176e-06, eta: 3 days, 7:24:02, time: 2.737, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2560, decode.loss_mask: 0.5289, decode.loss_dice: 0.9495, decode.d0.loss_cls: 0.9805, decode.d0.loss_mask: 0.5751, decode.d0.loss_dice: 1.0566, decode.d1.loss_cls: 0.3192, decode.d1.loss_mask: 0.5404, decode.d1.loss_dice: 0.9967, decode.d2.loss_cls: 0.3064, decode.d2.loss_mask: 0.5252, decode.d2.loss_dice: 0.9669, decode.d3.loss_cls: 0.2838, decode.d3.loss_mask: 0.5227, decode.d3.loss_dice: 0.9644, decode.d4.loss_cls: 0.2836, decode.d4.loss_mask: 0.5266, decode.d4.loss_dice: 0.9627, decode.d5.loss_cls: 0.2877, decode.d5.loss_mask: 0.5213, decode.d5.loss_dice: 0.9521, decode.d6.loss_cls: 0.2681, decode.d6.loss_mask: 0.5197, decode.d6.loss_dice: 0.9535, decode.d7.loss_cls: 0.2915, decode.d7.loss_mask: 0.5181, decode.d7.loss_dice: 0.9499, decode.d8.loss_cls: 0.2736, decode.d8.loss_mask: 0.5236, decode.d8.loss_dice: 0.9623, loss: 18.5665
2023-02-22 17:13:08,720 - mmseg - INFO - Iter [14550/80000]	lr: 1.175e-06, eta: 3 days, 7:14:18, time: 2.736, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2753, decode.loss_mask: 0.5511, decode.loss_dice: 0.9915, decode.d0.loss_cls: 0.9435, decode.d0.loss_mask: 0.5893, decode.d0.loss_dice: 1.0670, decode.d1.loss_cls: 0.3104, decode.d1.loss_mask: 0.5597, decode.d1.loss_dice: 1.0128, decode.d2.loss_cls: 0.2959, decode.d2.loss_mask: 0.5486, decode.d2.loss_dice: 0.9960, decode.d3.loss_cls: 0.2806, decode.d3.loss_mask: 0.5488, decode.d3.loss_dice: 0.9846, decode.d4.loss_cls: 0.2752, decode.d4.loss_mask: 0.5529, decode.d4.loss_dice: 0.9936, decode.d5.loss_cls: 0.2779, decode.d5.loss_mask: 0.5566, decode.d5.loss_dice: 1.0043, decode.d6.loss_cls: 0.2779, decode.d6.loss_mask: 0.5498, decode.d6.loss_dice: 0.9927, decode.d7.loss_cls: 0.2458, decode.d7.loss_mask: 0.5541, decode.d7.loss_dice: 0.9831, decode.d8.loss_cls: 0.2446, decode.d8.loss_mask: 0.5508, decode.d8.loss_dice: 0.9977, loss: 19.0122
2023-02-22 17:15:25,370 - mmseg - INFO - Iter [14600/80000]	lr: 1.174e-06, eta: 3 days, 7:04:36, time: 2.733, data_time: 0.020, memory: 31493, decode.loss_cls: 0.3248, decode.loss_mask: 0.5860, decode.loss_dice: 0.9868, decode.d0.loss_cls: 0.9146, decode.d0.loss_mask: 0.6524, decode.d0.loss_dice: 1.0973, decode.d1.loss_cls: 0.3402, decode.d1.loss_mask: 0.6191, decode.d1.loss_dice: 1.0165, decode.d2.loss_cls: 0.3279, decode.d2.loss_mask: 0.5988, decode.d2.loss_dice: 1.0150, decode.d3.loss_cls: 0.2968, decode.d3.loss_mask: 0.5868, decode.d3.loss_dice: 0.9954, decode.d4.loss_cls: 0.3053, decode.d4.loss_mask: 0.5954, decode.d4.loss_dice: 1.0138, decode.d5.loss_cls: 0.2823, decode.d5.loss_mask: 0.5934, decode.d5.loss_dice: 1.0186, decode.d6.loss_cls: 0.2900, decode.d6.loss_mask: 0.5962, decode.d6.loss_dice: 1.0020, decode.d7.loss_cls: 0.3026, decode.d7.loss_mask: 0.5904, decode.d7.loss_dice: 0.9891, decode.d8.loss_cls: 0.2956, decode.d8.loss_mask: 0.5927, decode.d8.loss_dice: 0.9956, loss: 19.8214
2023-02-22 17:17:42,361 - mmseg - INFO - Iter [14650/80000]	lr: 1.173e-06, eta: 3 days, 6:54:58, time: 2.740, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2911, decode.loss_mask: 0.5602, decode.loss_dice: 0.9870, decode.d0.loss_cls: 0.8983, decode.d0.loss_mask: 0.6147, decode.d0.loss_dice: 1.0896, decode.d1.loss_cls: 0.2991, decode.d1.loss_mask: 0.5550, decode.d1.loss_dice: 1.0227, decode.d2.loss_cls: 0.2888, decode.d2.loss_mask: 0.5548, decode.d2.loss_dice: 1.0090, decode.d3.loss_cls: 0.2841, decode.d3.loss_mask: 0.5503, decode.d3.loss_dice: 1.0018, decode.d4.loss_cls: 0.3026, decode.d4.loss_mask: 0.5542, decode.d4.loss_dice: 1.0187, decode.d5.loss_cls: 0.2949, decode.d5.loss_mask: 0.5579, decode.d5.loss_dice: 1.0052, decode.d6.loss_cls: 0.3015, decode.d6.loss_mask: 0.5573, decode.d6.loss_dice: 1.0069, decode.d7.loss_cls: 0.2895, decode.d7.loss_mask: 0.5525, decode.d7.loss_dice: 0.9964, decode.d8.loss_cls: 0.2852, decode.d8.loss_mask: 0.5577, decode.d8.loss_dice: 1.0140, loss: 19.3008
2023-02-22 17:19:59,295 - mmseg - INFO - Iter [14700/80000]	lr: 1.172e-06, eta: 3 days, 6:45:23, time: 2.739, data_time: 0.020, memory: 31493, decode.loss_cls: 0.2502, decode.loss_mask: 0.5901, decode.loss_dice: 0.9036, decode.d0.loss_cls: 0.9211, decode.d0.loss_mask: 0.6133, decode.d0.loss_dice: 1.0213, decode.d1.loss_cls: 0.2797, decode.d1.loss_mask: 0.6112, decode.d1.loss_dice: 0.9696, decode.d2.loss_cls: 0.2718, decode.d2.loss_mask: 0.6041, decode.d2.loss_dice: 0.9462, decode.d3.loss_cls: 0.2506, decode.d3.loss_mask: 0.5968, decode.d3.loss_dice: 0.9384, decode.d4.loss_cls: 0.2509, decode.d4.loss_mask: 0.5980, decode.d4.loss_dice: 0.9394, decode.d5.loss_cls: 0.2561, decode.d5.loss_mask: 0.5931, decode.d5.loss_dice: 0.9406, decode.d6.loss_cls: 0.2613, decode.d6.loss_mask: 0.5815, decode.d6.loss_dice: 0.9115, decode.d7.loss_cls: 0.2603, decode.d7.loss_mask: 0.5934, decode.d7.loss_dice: 0.9086, decode.d8.loss_cls: 0.2495, decode.d8.loss_mask: 0.5909, decode.d8.loss_dice: 0.9240, loss: 18.6273
2023-02-22 17:22:15,991 - mmseg - INFO - Iter [14750/80000]	lr: 1.171e-06, eta: 3 days, 6:35:51, time: 2.734, data_time: 0.020, memory: 31493, decode.loss_cls: 0.2367, decode.loss_mask: 0.5425, decode.loss_dice: 0.8994, decode.d0.loss_cls: 0.8893, decode.d0.loss_mask: 0.5727, decode.d0.loss_dice: 0.9698, decode.d1.loss_cls: 0.2351, decode.d1.loss_mask: 0.5441, decode.d1.loss_dice: 0.9149, decode.d2.loss_cls: 0.2175, decode.d2.loss_mask: 0.5375, decode.d2.loss_dice: 0.9184, decode.d3.loss_cls: 0.2545, decode.d3.loss_mask: 0.5386, decode.d3.loss_dice: 0.9071, decode.d4.loss_cls: 0.2360, decode.d4.loss_mask: 0.5398, decode.d4.loss_dice: 0.9107, decode.d5.loss_cls: 0.2472, decode.d5.loss_mask: 0.5431, decode.d5.loss_dice: 0.9196, decode.d6.loss_cls: 0.2585, decode.d6.loss_mask: 0.5401, decode.d6.loss_dice: 0.8933, decode.d7.loss_cls: 0.2359, decode.d7.loss_mask: 0.5423, decode.d7.loss_dice: 0.9107, decode.d8.loss_cls: 0.2263, decode.d8.loss_mask: 0.5382, decode.d8.loss_dice: 0.9133, loss: 17.6333
2023-02-22 17:24:32,730 - mmseg - INFO - Iter [14800/80000]	lr: 1.170e-06, eta: 3 days, 6:26:21, time: 2.735, data_time: 0.020, memory: 31493, decode.loss_cls: 0.3289, decode.loss_mask: 0.6090, decode.loss_dice: 1.0441, decode.d0.loss_cls: 0.9655, decode.d0.loss_mask: 0.6512, decode.d0.loss_dice: 1.1294, decode.d1.loss_cls: 0.3264, decode.d1.loss_mask: 0.6314, decode.d1.loss_dice: 1.0802, decode.d2.loss_cls: 0.3095, decode.d2.loss_mask: 0.6175, decode.d2.loss_dice: 1.0892, decode.d3.loss_cls: 0.3231, decode.d3.loss_mask: 0.6074, decode.d3.loss_dice: 1.0598, decode.d4.loss_cls: 0.3228, decode.d4.loss_mask: 0.6200, decode.d4.loss_dice: 1.0716, decode.d5.loss_cls: 0.3231, decode.d5.loss_mask: 0.6176, decode.d5.loss_dice: 1.0519, decode.d6.loss_cls: 0.3226, decode.d6.loss_mask: 0.6026, decode.d6.loss_dice: 1.0553, decode.d7.loss_cls: 0.3429, decode.d7.loss_mask: 0.5989, decode.d7.loss_dice: 1.0492, decode.d8.loss_cls: 0.3361, decode.d8.loss_mask: 0.6008, decode.d8.loss_dice: 1.0484, loss: 20.7364
2023-02-22 17:26:49,426 - mmseg - INFO - Iter [14850/80000]	lr: 1.169e-06, eta: 3 days, 6:16:54, time: 2.734, data_time: 0.020, memory: 31493, decode.loss_cls: 0.2711, decode.loss_mask: 0.5990, decode.loss_dice: 1.0100, decode.d0.loss_cls: 0.9101, decode.d0.loss_mask: 0.6350, decode.d0.loss_dice: 1.0615, decode.d1.loss_cls: 0.2920, decode.d1.loss_mask: 0.6146, decode.d1.loss_dice: 1.0422, decode.d2.loss_cls: 0.2754, decode.d2.loss_mask: 0.6049, decode.d2.loss_dice: 1.0102, decode.d3.loss_cls: 0.2612, decode.d3.loss_mask: 0.6005, decode.d3.loss_dice: 0.9803, decode.d4.loss_cls: 0.2534, decode.d4.loss_mask: 0.6022, decode.d4.loss_dice: 1.0058, decode.d5.loss_cls: 0.2726, decode.d5.loss_mask: 0.5975, decode.d5.loss_dice: 1.0040, decode.d6.loss_cls: 0.2662, decode.d6.loss_mask: 0.5894, decode.d6.loss_dice: 0.9967, decode.d7.loss_cls: 0.2655, decode.d7.loss_mask: 0.6014, decode.d7.loss_dice: 1.0029, decode.d8.loss_cls: 0.2643, decode.d8.loss_mask: 0.5984, decode.d8.loss_dice: 1.0065, loss: 19.4949
2023-02-22 17:29:06,397 - mmseg - INFO - Iter [14900/80000]	lr: 1.168e-06, eta: 3 days, 6:07:31, time: 2.739, data_time: 0.020, memory: 31493, decode.loss_cls: 0.3310, decode.loss_mask: 0.5877, decode.loss_dice: 0.9866, decode.d0.loss_cls: 0.9565, decode.d0.loss_mask: 0.6482, decode.d0.loss_dice: 1.0663, decode.d1.loss_cls: 0.3294, decode.d1.loss_mask: 0.5856, decode.d1.loss_dice: 1.0077, decode.d2.loss_cls: 0.3310, decode.d2.loss_mask: 0.5871, decode.d2.loss_dice: 0.9940, decode.d3.loss_cls: 0.3479, decode.d3.loss_mask: 0.5901, decode.d3.loss_dice: 0.9739, decode.d4.loss_cls: 0.3291, decode.d4.loss_mask: 0.5869, decode.d4.loss_dice: 0.9801, decode.d5.loss_cls: 0.3171, decode.d5.loss_mask: 0.5885, decode.d5.loss_dice: 0.9893, decode.d6.loss_cls: 0.3258, decode.d6.loss_mask: 0.5895, decode.d6.loss_dice: 0.9906, decode.d7.loss_cls: 0.3307, decode.d7.loss_mask: 0.5869, decode.d7.loss_dice: 1.0061, decode.d8.loss_cls: 0.3389, decode.d8.loss_mask: 0.5851, decode.d8.loss_dice: 0.9860, loss: 19.8538
2023-02-22 17:31:22,993 - mmseg - INFO - Iter [14950/80000]	lr: 1.167e-06, eta: 3 days, 5:58:10, time: 2.732, data_time: 0.020, memory: 31493, decode.loss_cls: 0.2821, decode.loss_mask: 0.5608, decode.loss_dice: 0.9999, decode.d0.loss_cls: 0.9493, decode.d0.loss_mask: 0.5945, decode.d0.loss_dice: 1.0781, decode.d1.loss_cls: 0.3288, decode.d1.loss_mask: 0.5772, decode.d1.loss_dice: 1.0171, decode.d2.loss_cls: 0.2829, decode.d2.loss_mask: 0.5686, decode.d2.loss_dice: 1.0133, decode.d3.loss_cls: 0.2954, decode.d3.loss_mask: 0.5633, decode.d3.loss_dice: 1.0094, decode.d4.loss_cls: 0.2840, decode.d4.loss_mask: 0.5605, decode.d4.loss_dice: 0.9985, decode.d5.loss_cls: 0.2996, decode.d5.loss_mask: 0.5537, decode.d5.loss_dice: 1.0115, decode.d6.loss_cls: 0.3042, decode.d6.loss_mask: 0.5656, decode.d6.loss_dice: 1.0042, decode.d7.loss_cls: 0.2917, decode.d7.loss_mask: 0.5573, decode.d7.loss_dice: 1.0107, decode.d8.loss_cls: 0.2885, decode.d8.loss_mask: 0.5534, decode.d8.loss_dice: 1.0002, loss: 19.4044
2023-02-22 17:33:42,194 - mmseg - INFO - Saving checkpoint at 15000 iterations
2023-02-22 17:34:02,622 - mmseg - INFO - Exp name: my_city.py
2023-02-22 17:34:02,622 - mmseg - INFO - Iter [15000/80000]	lr: 1.167e-06, eta: 3 days, 5:50:31, time: 3.193, data_time: 0.069, memory: 31493, decode.loss_cls: 0.2598, decode.loss_mask: 0.5421, decode.loss_dice: 0.9680, decode.d0.loss_cls: 0.9145, decode.d0.loss_mask: 0.5942, decode.d0.loss_dice: 1.0659, decode.d1.loss_cls: 0.2740, decode.d1.loss_mask: 0.5547, decode.d1.loss_dice: 0.9900, decode.d2.loss_cls: 0.2571, decode.d2.loss_mask: 0.5452, decode.d2.loss_dice: 0.9832, decode.d3.loss_cls: 0.2563, decode.d3.loss_mask: 0.5406, decode.d3.loss_dice: 0.9657, decode.d4.loss_cls: 0.2590, decode.d4.loss_mask: 0.5411, decode.d4.loss_dice: 0.9764, decode.d5.loss_cls: 0.2709, decode.d5.loss_mask: 0.5475, decode.d5.loss_dice: 0.9808, decode.d6.loss_cls: 0.2814, decode.d6.loss_mask: 0.5342, decode.d6.loss_dice: 0.9600, decode.d7.loss_cls: 0.2747, decode.d7.loss_mask: 0.5353, decode.d7.loss_dice: 0.9704, decode.d8.loss_cls: 0.2568, decode.d8.loss_mask: 0.5396, decode.d8.loss_dice: 0.9565, loss: 18.5961
2023-02-22 17:36:19,420 - mmseg - INFO - Iter [15050/80000]	lr: 1.166e-06, eta: 3 days, 5:41:15, time: 2.736, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2443, decode.loss_mask: 0.6086, decode.loss_dice: 1.0064, decode.d0.loss_cls: 0.8976, decode.d0.loss_mask: 0.6519, decode.d0.loss_dice: 1.0529, decode.d1.loss_cls: 0.2603, decode.d1.loss_mask: 0.6224, decode.d1.loss_dice: 1.0222, decode.d2.loss_cls: 0.2393, decode.d2.loss_mask: 0.6088, decode.d2.loss_dice: 1.0215, decode.d3.loss_cls: 0.2340, decode.d3.loss_mask: 0.6062, decode.d3.loss_dice: 1.0091, decode.d4.loss_cls: 0.2153, decode.d4.loss_mask: 0.6079, decode.d4.loss_dice: 1.0185, decode.d5.loss_cls: 0.2312, decode.d5.loss_mask: 0.6143, decode.d5.loss_dice: 1.0125, decode.d6.loss_cls: 0.2325, decode.d6.loss_mask: 0.6139, decode.d6.loss_dice: 1.0153, decode.d7.loss_cls: 0.2375, decode.d7.loss_mask: 0.6080, decode.d7.loss_dice: 1.0137, decode.d8.loss_cls: 0.2354, decode.d8.loss_mask: 0.6081, decode.d8.loss_dice: 1.0100, loss: 19.3597
2023-02-22 17:38:36,178 - mmseg - INFO - Iter [15100/80000]	lr: 1.165e-06, eta: 3 days, 5:32:03, time: 2.735, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2600, decode.loss_mask: 0.5513, decode.loss_dice: 0.9373, decode.d0.loss_cls: 0.9415, decode.d0.loss_mask: 0.5784, decode.d0.loss_dice: 1.0332, decode.d1.loss_cls: 0.2935, decode.d1.loss_mask: 0.5629, decode.d1.loss_dice: 0.9878, decode.d2.loss_cls: 0.2412, decode.d2.loss_mask: 0.5589, decode.d2.loss_dice: 0.9673, decode.d3.loss_cls: 0.2461, decode.d3.loss_mask: 0.5460, decode.d3.loss_dice: 0.9516, decode.d4.loss_cls: 0.2725, decode.d4.loss_mask: 0.5454, decode.d4.loss_dice: 0.9514, decode.d5.loss_cls: 0.2635, decode.d5.loss_mask: 0.5496, decode.d5.loss_dice: 0.9433, decode.d6.loss_cls: 0.2495, decode.d6.loss_mask: 0.5505, decode.d6.loss_dice: 0.9424, decode.d7.loss_cls: 0.2425, decode.d7.loss_mask: 0.5503, decode.d7.loss_dice: 0.9550, decode.d8.loss_cls: 0.2627, decode.d8.loss_mask: 0.5432, decode.d8.loss_dice: 0.9401, loss: 18.4189
2023-02-22 17:40:53,251 - mmseg - INFO - Iter [15150/80000]	lr: 1.164e-06, eta: 3 days, 5:22:54, time: 2.741, data_time: 0.022, memory: 31493, decode.loss_cls: 0.3001, decode.loss_mask: 0.5670, decode.loss_dice: 1.0097, decode.d0.loss_cls: 0.9828, decode.d0.loss_mask: 0.6005, decode.d0.loss_dice: 1.0784, decode.d1.loss_cls: 0.3497, decode.d1.loss_mask: 0.5624, decode.d1.loss_dice: 1.0568, decode.d2.loss_cls: 0.3099, decode.d2.loss_mask: 0.5668, decode.d2.loss_dice: 1.0472, decode.d3.loss_cls: 0.2904, decode.d3.loss_mask: 0.5651, decode.d3.loss_dice: 1.0205, decode.d4.loss_cls: 0.3003, decode.d4.loss_mask: 0.5635, decode.d4.loss_dice: 1.0384, decode.d5.loss_cls: 0.2938, decode.d5.loss_mask: 0.5635, decode.d5.loss_dice: 1.0254, decode.d6.loss_cls: 0.2905, decode.d6.loss_mask: 0.5649, decode.d6.loss_dice: 1.0203, decode.d7.loss_cls: 0.3034, decode.d7.loss_mask: 0.5662, decode.d7.loss_dice: 1.0100, decode.d8.loss_cls: 0.3027, decode.d8.loss_mask: 0.5640, decode.d8.loss_dice: 1.0126, loss: 19.7269
2023-02-22 17:43:09,833 - mmseg - INFO - Iter [15200/80000]	lr: 1.163e-06, eta: 3 days, 5:13:46, time: 2.732, data_time: 0.021, memory: 31493, decode.loss_cls: 0.3005, decode.loss_mask: 0.5256, decode.loss_dice: 0.9212, decode.d0.loss_cls: 0.9192, decode.d0.loss_mask: 0.5484, decode.d0.loss_dice: 0.9888, decode.d1.loss_cls: 0.3108, decode.d1.loss_mask: 0.5369, decode.d1.loss_dice: 0.9563, decode.d2.loss_cls: 0.2871, decode.d2.loss_mask: 0.5304, decode.d2.loss_dice: 0.9499, decode.d3.loss_cls: 0.2950, decode.d3.loss_mask: 0.5300, decode.d3.loss_dice: 0.9366, decode.d4.loss_cls: 0.2885, decode.d4.loss_mask: 0.5222, decode.d4.loss_dice: 0.9332, decode.d5.loss_cls: 0.2867, decode.d5.loss_mask: 0.5216, decode.d5.loss_dice: 0.9204, decode.d6.loss_cls: 0.2835, decode.d6.loss_mask: 0.5260, decode.d6.loss_dice: 0.9224, decode.d7.loss_cls: 0.2929, decode.d7.loss_mask: 0.5272, decode.d7.loss_dice: 0.9348, decode.d8.loss_cls: 0.2969, decode.d8.loss_mask: 0.5261, decode.d8.loss_dice: 0.9351, loss: 18.2543
2023-02-22 17:45:27,115 - mmseg - INFO - Iter [15250/80000]	lr: 1.162e-06, eta: 3 days, 5:04:43, time: 2.746, data_time: 0.020, memory: 31493, decode.loss_cls: 0.2652, decode.loss_mask: 0.5963, decode.loss_dice: 0.9690, decode.d0.loss_cls: 0.8932, decode.d0.loss_mask: 0.6318, decode.d0.loss_dice: 1.0336, decode.d1.loss_cls: 0.3229, decode.d1.loss_mask: 0.6070, decode.d1.loss_dice: 0.9906, decode.d2.loss_cls: 0.2949, decode.d2.loss_mask: 0.5954, decode.d2.loss_dice: 0.9727, decode.d3.loss_cls: 0.2838, decode.d3.loss_mask: 0.6023, decode.d3.loss_dice: 0.9789, decode.d4.loss_cls: 0.2664, decode.d4.loss_mask: 0.6099, decode.d4.loss_dice: 0.9757, decode.d5.loss_cls: 0.3016, decode.d5.loss_mask: 0.6103, decode.d5.loss_dice: 0.9778, decode.d6.loss_cls: 0.2808, decode.d6.loss_mask: 0.5926, decode.d6.loss_dice: 0.9579, decode.d7.loss_cls: 0.2604, decode.d7.loss_mask: 0.5963, decode.d7.loss_dice: 0.9704, decode.d8.loss_cls: 0.2750, decode.d8.loss_mask: 0.5924, decode.d8.loss_dice: 0.9710, loss: 19.2762
2023-02-22 17:47:44,284 - mmseg - INFO - Iter [15300/80000]	lr: 1.161e-06, eta: 3 days, 4:55:43, time: 2.743, data_time: 0.023, memory: 31493, decode.loss_cls: 0.2922, decode.loss_mask: 0.5839, decode.loss_dice: 0.9476, decode.d0.loss_cls: 0.8778, decode.d0.loss_mask: 0.6067, decode.d0.loss_dice: 1.0111, decode.d1.loss_cls: 0.2799, decode.d1.loss_mask: 0.5867, decode.d1.loss_dice: 0.9699, decode.d2.loss_cls: 0.2898, decode.d2.loss_mask: 0.5728, decode.d2.loss_dice: 0.9452, decode.d3.loss_cls: 0.2764, decode.d3.loss_mask: 0.5675, decode.d3.loss_dice: 0.9582, decode.d4.loss_cls: 0.2834, decode.d4.loss_mask: 0.5765, decode.d4.loss_dice: 0.9443, decode.d5.loss_cls: 0.3070, decode.d5.loss_mask: 0.5702, decode.d5.loss_dice: 0.9472, decode.d6.loss_cls: 0.3272, decode.d6.loss_mask: 0.5674, decode.d6.loss_dice: 0.9339, decode.d7.loss_cls: 0.3132, decode.d7.loss_mask: 0.5720, decode.d7.loss_dice: 0.9547, decode.d8.loss_cls: 0.3045, decode.d8.loss_mask: 0.5814, decode.d8.loss_dice: 0.9346, loss: 18.8834
2023-02-22 17:50:01,435 - mmseg - INFO - Iter [15350/80000]	lr: 1.160e-06, eta: 3 days, 4:46:45, time: 2.743, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2474, decode.loss_mask: 0.5775, decode.loss_dice: 0.9942, decode.d0.loss_cls: 0.8871, decode.d0.loss_mask: 0.6197, decode.d0.loss_dice: 1.0370, decode.d1.loss_cls: 0.2979, decode.d1.loss_mask: 0.5960, decode.d1.loss_dice: 1.0221, decode.d2.loss_cls: 0.2459, decode.d2.loss_mask: 0.5869, decode.d2.loss_dice: 0.9973, decode.d3.loss_cls: 0.2549, decode.d3.loss_mask: 0.5788, decode.d3.loss_dice: 0.9914, decode.d4.loss_cls: 0.2532, decode.d4.loss_mask: 0.5784, decode.d4.loss_dice: 0.9691, decode.d5.loss_cls: 0.2462, decode.d5.loss_mask: 0.5784, decode.d5.loss_dice: 0.9923, decode.d6.loss_cls: 0.2398, decode.d6.loss_mask: 0.5920, decode.d6.loss_dice: 0.9886, decode.d7.loss_cls: 0.2552, decode.d7.loss_mask: 0.5873, decode.d7.loss_dice: 0.9817, decode.d8.loss_cls: 0.2503, decode.d8.loss_mask: 0.5792, decode.d8.loss_dice: 0.9840, loss: 19.0097
2023-02-22 17:52:18,707 - mmseg - INFO - Iter [15400/80000]	lr: 1.159e-06, eta: 3 days, 4:37:50, time: 2.745, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2700, decode.loss_mask: 0.5923, decode.loss_dice: 1.0083, decode.d0.loss_cls: 0.8916, decode.d0.loss_mask: 0.6430, decode.d0.loss_dice: 1.0663, decode.d1.loss_cls: 0.2473, decode.d1.loss_mask: 0.6063, decode.d1.loss_dice: 1.0675, decode.d2.loss_cls: 0.2521, decode.d2.loss_mask: 0.5997, decode.d2.loss_dice: 1.0250, decode.d3.loss_cls: 0.2472, decode.d3.loss_mask: 0.5969, decode.d3.loss_dice: 1.0193, decode.d4.loss_cls: 0.2588, decode.d4.loss_mask: 0.5948, decode.d4.loss_dice: 0.9819, decode.d5.loss_cls: 0.2530, decode.d5.loss_mask: 0.5940, decode.d5.loss_dice: 1.0047, decode.d6.loss_cls: 0.2721, decode.d6.loss_mask: 0.5942, decode.d6.loss_dice: 1.0055, decode.d7.loss_cls: 0.2725, decode.d7.loss_mask: 0.5963, decode.d7.loss_dice: 1.0139, decode.d8.loss_cls: 0.2730, decode.d8.loss_mask: 0.5960, decode.d8.loss_dice: 1.0200, loss: 19.4637
2023-02-22 17:54:36,081 - mmseg - INFO - Iter [15450/80000]	lr: 1.159e-06, eta: 3 days, 4:28:59, time: 2.747, data_time: 0.022, memory: 31493, decode.loss_cls: 0.2579, decode.loss_mask: 0.5548, decode.loss_dice: 0.9617, decode.d0.loss_cls: 0.8927, decode.d0.loss_mask: 0.5886, decode.d0.loss_dice: 1.0371, decode.d1.loss_cls: 0.2325, decode.d1.loss_mask: 0.5657, decode.d1.loss_dice: 1.0161, decode.d2.loss_cls: 0.2618, decode.d2.loss_mask: 0.5615, decode.d2.loss_dice: 0.9715, decode.d3.loss_cls: 0.2759, decode.d3.loss_mask: 0.5549, decode.d3.loss_dice: 0.9612, decode.d4.loss_cls: 0.2716, decode.d4.loss_mask: 0.5551, decode.d4.loss_dice: 0.9669, decode.d5.loss_cls: 0.2682, decode.d5.loss_mask: 0.5522, decode.d5.loss_dice: 0.9577, decode.d6.loss_cls: 0.2627, decode.d6.loss_mask: 0.5540, decode.d6.loss_dice: 0.9531, decode.d7.loss_cls: 0.2576, decode.d7.loss_mask: 0.5536, decode.d7.loss_dice: 0.9810, decode.d8.loss_cls: 0.2793, decode.d8.loss_mask: 0.5533, decode.d8.loss_dice: 0.9499, loss: 18.6105
2023-02-22 17:56:52,830 - mmseg - INFO - Iter [15500/80000]	lr: 1.158e-06, eta: 3 days, 4:20:07, time: 2.735, data_time: 0.020, memory: 31493, decode.loss_cls: 0.3215, decode.loss_mask: 0.5577, decode.loss_dice: 0.9330, decode.d0.loss_cls: 0.9588, decode.d0.loss_mask: 0.6268, decode.d0.loss_dice: 1.0216, decode.d1.loss_cls: 0.3431, decode.d1.loss_mask: 0.5556, decode.d1.loss_dice: 0.9915, decode.d2.loss_cls: 0.3061, decode.d2.loss_mask: 0.5579, decode.d2.loss_dice: 0.9699, decode.d3.loss_cls: 0.3001, decode.d3.loss_mask: 0.5643, decode.d3.loss_dice: 0.9538, decode.d4.loss_cls: 0.3013, decode.d4.loss_mask: 0.5616, decode.d4.loss_dice: 0.9585, decode.d5.loss_cls: 0.3177, decode.d5.loss_mask: 0.5529, decode.d5.loss_dice: 0.9611, decode.d6.loss_cls: 0.3051, decode.d6.loss_mask: 0.5682, decode.d6.loss_dice: 0.9603, decode.d7.loss_cls: 0.3009, decode.d7.loss_mask: 0.5663, decode.d7.loss_dice: 0.9662, decode.d8.loss_cls: 0.3114, decode.d8.loss_mask: 0.5633, decode.d8.loss_dice: 0.9490, loss: 19.1053
2023-02-22 17:59:09,552 - mmseg - INFO - Iter [15550/80000]	lr: 1.157e-06, eta: 3 days, 4:11:18, time: 2.734, data_time: 0.020, memory: 31493, decode.loss_cls: 0.2783, decode.loss_mask: 0.5662, decode.loss_dice: 0.9445, decode.d0.loss_cls: 0.8943, decode.d0.loss_mask: 0.6012, decode.d0.loss_dice: 1.0060, decode.d1.loss_cls: 0.3305, decode.d1.loss_mask: 0.5801, decode.d1.loss_dice: 0.9633, decode.d2.loss_cls: 0.3006, decode.d2.loss_mask: 0.5751, decode.d2.loss_dice: 0.9588, decode.d3.loss_cls: 0.3328, decode.d3.loss_mask: 0.5661, decode.d3.loss_dice: 0.9562, decode.d4.loss_cls: 0.3157, decode.d4.loss_mask: 0.5666, decode.d4.loss_dice: 0.9368, decode.d5.loss_cls: 0.3205, decode.d5.loss_mask: 0.5662, decode.d5.loss_dice: 0.9441, decode.d6.loss_cls: 0.2779, decode.d6.loss_mask: 0.5676, decode.d6.loss_dice: 0.9372, decode.d7.loss_cls: 0.2910, decode.d7.loss_mask: 0.5616, decode.d7.loss_dice: 0.9499, decode.d8.loss_cls: 0.2874, decode.d8.loss_mask: 0.5681, decode.d8.loss_dice: 0.9450, loss: 18.8895
2023-02-22 18:01:26,493 - mmseg - INFO - Iter [15600/80000]	lr: 1.156e-06, eta: 3 days, 4:02:32, time: 2.739, data_time: 0.023, memory: 31493, decode.loss_cls: 0.3490, decode.loss_mask: 0.5470, decode.loss_dice: 0.9820, decode.d0.loss_cls: 0.9489, decode.d0.loss_mask: 0.5861, decode.d0.loss_dice: 1.0549, decode.d1.loss_cls: 0.3555, decode.d1.loss_mask: 0.5649, decode.d1.loss_dice: 0.9984, decode.d2.loss_cls: 0.3187, decode.d2.loss_mask: 0.5539, decode.d2.loss_dice: 0.9903, decode.d3.loss_cls: 0.3439, decode.d3.loss_mask: 0.5496, decode.d3.loss_dice: 0.9851, decode.d4.loss_cls: 0.3466, decode.d4.loss_mask: 0.5434, decode.d4.loss_dice: 0.9767, decode.d5.loss_cls: 0.3324, decode.d5.loss_mask: 0.5487, decode.d5.loss_dice: 0.9777, decode.d6.loss_cls: 0.3446, decode.d6.loss_mask: 0.5482, decode.d6.loss_dice: 0.9775, decode.d7.loss_cls: 0.3696, decode.d7.loss_mask: 0.5438, decode.d7.loss_dice: 0.9541, decode.d8.loss_cls: 0.3455, decode.d8.loss_mask: 0.5451, decode.d8.loss_dice: 0.9760, loss: 19.4579
2023-02-22 18:03:43,763 - mmseg - INFO - Iter [15650/80000]	lr: 1.155e-06, eta: 3 days, 3:53:50, time: 2.745, data_time: 0.024, memory: 31493, decode.loss_cls: 0.2979, decode.loss_mask: 0.5243, decode.loss_dice: 0.9205, decode.d0.loss_cls: 0.8880, decode.d0.loss_mask: 0.5527, decode.d0.loss_dice: 0.9985, decode.d1.loss_cls: 0.3404, decode.d1.loss_mask: 0.5466, decode.d1.loss_dice: 0.9580, decode.d2.loss_cls: 0.3471, decode.d2.loss_mask: 0.5279, decode.d2.loss_dice: 0.9232, decode.d3.loss_cls: 0.3191, decode.d3.loss_mask: 0.5261, decode.d3.loss_dice: 0.9241, decode.d4.loss_cls: 0.3064, decode.d4.loss_mask: 0.5245, decode.d4.loss_dice: 0.9074, decode.d5.loss_cls: 0.2774, decode.d5.loss_mask: 0.5314, decode.d5.loss_dice: 0.9388, decode.d6.loss_cls: 0.2977, decode.d6.loss_mask: 0.5354, decode.d6.loss_dice: 0.9228, decode.d7.loss_cls: 0.2880, decode.d7.loss_mask: 0.5284, decode.d7.loss_dice: 0.9186, decode.d8.loss_cls: 0.3041, decode.d8.loss_mask: 0.5263, decode.d8.loss_dice: 0.9065, loss: 18.3083
2023-02-22 18:06:00,648 - mmseg - INFO - Iter [15700/80000]	lr: 1.154e-06, eta: 3 days, 3:45:08, time: 2.738, data_time: 0.023, memory: 31493, decode.loss_cls: 0.2917, decode.loss_mask: 0.5588, decode.loss_dice: 0.8702, decode.d0.loss_cls: 0.8494, decode.d0.loss_mask: 0.5961, decode.d0.loss_dice: 0.9562, decode.d1.loss_cls: 0.2847, decode.d1.loss_mask: 0.5700, decode.d1.loss_dice: 0.9151, decode.d2.loss_cls: 0.2659, decode.d2.loss_mask: 0.5702, decode.d2.loss_dice: 0.9000, decode.d3.loss_cls: 0.2719, decode.d3.loss_mask: 0.5573, decode.d3.loss_dice: 0.8801, decode.d4.loss_cls: 0.2830, decode.d4.loss_mask: 0.5687, decode.d4.loss_dice: 0.8794, decode.d5.loss_cls: 0.2900, decode.d5.loss_mask: 0.5589, decode.d5.loss_dice: 0.8772, decode.d6.loss_cls: 0.2828, decode.d6.loss_mask: 0.5528, decode.d6.loss_dice: 0.8766, decode.d7.loss_cls: 0.2810, decode.d7.loss_mask: 0.5493, decode.d7.loss_dice: 0.8658, decode.d8.loss_cls: 0.2844, decode.d8.loss_mask: 0.5543, decode.d8.loss_dice: 0.8762, loss: 17.9180
2023-02-22 18:08:20,447 - mmseg - INFO - Iter [15750/80000]	lr: 1.153e-06, eta: 3 days, 3:36:42, time: 2.796, data_time: 0.072, memory: 31493, decode.loss_cls: 0.3438, decode.loss_mask: 0.5807, decode.loss_dice: 0.9968, decode.d0.loss_cls: 0.9051, decode.d0.loss_mask: 0.5978, decode.d0.loss_dice: 1.0976, decode.d1.loss_cls: 0.3812, decode.d1.loss_mask: 0.5903, decode.d1.loss_dice: 1.0343, decode.d2.loss_cls: 0.3492, decode.d2.loss_mask: 0.5804, decode.d2.loss_dice: 0.9932, decode.d3.loss_cls: 0.3278, decode.d3.loss_mask: 0.5865, decode.d3.loss_dice: 0.9882, decode.d4.loss_cls: 0.3452, decode.d4.loss_mask: 0.5818, decode.d4.loss_dice: 1.0020, decode.d5.loss_cls: 0.3592, decode.d5.loss_mask: 0.5782, decode.d5.loss_dice: 0.9945, decode.d6.loss_cls: 0.3610, decode.d6.loss_mask: 0.5768, decode.d6.loss_dice: 0.9845, decode.d7.loss_cls: 0.3605, decode.d7.loss_mask: 0.5813, decode.d7.loss_dice: 0.9923, decode.d8.loss_cls: 0.3327, decode.d8.loss_mask: 0.5813, decode.d8.loss_dice: 0.9983, loss: 19.9825
2023-02-22 18:10:37,273 - mmseg - INFO - Iter [15800/80000]	lr: 1.152e-06, eta: 3 days, 3:28:05, time: 2.736, data_time: 0.023, memory: 31493, decode.loss_cls: 0.2183, decode.loss_mask: 0.5192, decode.loss_dice: 0.9467, decode.d0.loss_cls: 0.8679, decode.d0.loss_mask: 0.5443, decode.d0.loss_dice: 1.0222, decode.d1.loss_cls: 0.2517, decode.d1.loss_mask: 0.5203, decode.d1.loss_dice: 0.9696, decode.d2.loss_cls: 0.2185, decode.d2.loss_mask: 0.5192, decode.d2.loss_dice: 0.9762, decode.d3.loss_cls: 0.2115, decode.d3.loss_mask: 0.5270, decode.d3.loss_dice: 0.9701, decode.d4.loss_cls: 0.2160, decode.d4.loss_mask: 0.5233, decode.d4.loss_dice: 0.9584, decode.d5.loss_cls: 0.1993, decode.d5.loss_mask: 0.5219, decode.d5.loss_dice: 0.9516, decode.d6.loss_cls: 0.2136, decode.d6.loss_mask: 0.5186, decode.d6.loss_dice: 0.9402, decode.d7.loss_cls: 0.2102, decode.d7.loss_mask: 0.5184, decode.d7.loss_dice: 0.9592, decode.d8.loss_cls: 0.2068, decode.d8.loss_mask: 0.5202, decode.d8.loss_dice: 0.9618, loss: 17.7022
2023-02-22 18:12:53,691 - mmseg - INFO - Iter [15850/80000]	lr: 1.151e-06, eta: 3 days, 3:19:29, time: 2.728, data_time: 0.022, memory: 31493, decode.loss_cls: 0.2493, decode.loss_mask: 0.6139, decode.loss_dice: 1.0109, decode.d0.loss_cls: 0.8459, decode.d0.loss_mask: 0.6194, decode.d0.loss_dice: 1.0718, decode.d1.loss_cls: 0.2645, decode.d1.loss_mask: 0.6166, decode.d1.loss_dice: 1.0378, decode.d2.loss_cls: 0.2728, decode.d2.loss_mask: 0.6144, decode.d2.loss_dice: 1.0279, decode.d3.loss_cls: 0.2641, decode.d3.loss_mask: 0.6147, decode.d3.loss_dice: 1.0154, decode.d4.loss_cls: 0.2638, decode.d4.loss_mask: 0.6171, decode.d4.loss_dice: 1.0292, decode.d5.loss_cls: 0.2489, decode.d5.loss_mask: 0.6196, decode.d5.loss_dice: 1.0324, decode.d6.loss_cls: 0.2365, decode.d6.loss_mask: 0.6208, decode.d6.loss_dice: 1.0253, decode.d7.loss_cls: 0.2424, decode.d7.loss_mask: 0.6177, decode.d7.loss_dice: 1.0198, decode.d8.loss_cls: 0.2605, decode.d8.loss_mask: 0.6109, decode.d8.loss_dice: 0.9959, loss: 19.5803
2023-02-22 18:15:10,219 - mmseg - INFO - Iter [15900/80000]	lr: 1.150e-06, eta: 3 days, 3:10:56, time: 2.731, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2636, decode.loss_mask: 0.5945, decode.loss_dice: 0.9879, decode.d0.loss_cls: 0.8957, decode.d0.loss_mask: 0.6214, decode.d0.loss_dice: 1.0474, decode.d1.loss_cls: 0.2997, decode.d1.loss_mask: 0.6011, decode.d1.loss_dice: 1.0208, decode.d2.loss_cls: 0.2756, decode.d2.loss_mask: 0.5943, decode.d2.loss_dice: 0.9885, decode.d3.loss_cls: 0.2864, decode.d3.loss_mask: 0.5859, decode.d3.loss_dice: 0.9628, decode.d4.loss_cls: 0.2935, decode.d4.loss_mask: 0.5855, decode.d4.loss_dice: 0.9641, decode.d5.loss_cls: 0.2370, decode.d5.loss_mask: 0.5980, decode.d5.loss_dice: 0.9964, decode.d6.loss_cls: 0.2707, decode.d6.loss_mask: 0.5912, decode.d6.loss_dice: 0.9816, decode.d7.loss_cls: 0.2815, decode.d7.loss_mask: 0.5896, decode.d7.loss_dice: 0.9683, decode.d8.loss_cls: 0.2618, decode.d8.loss_mask: 0.5943, decode.d8.loss_dice: 0.9858, loss: 19.2254
2023-02-22 18:17:26,919 - mmseg - INFO - Iter [15950/80000]	lr: 1.150e-06, eta: 3 days, 3:02:26, time: 2.734, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2739, decode.loss_mask: 0.5989, decode.loss_dice: 1.0410, decode.d0.loss_cls: 0.8543, decode.d0.loss_mask: 0.6420, decode.d0.loss_dice: 1.1113, decode.d1.loss_cls: 0.3010, decode.d1.loss_mask: 0.6087, decode.d1.loss_dice: 1.0764, decode.d2.loss_cls: 0.2828, decode.d2.loss_mask: 0.5968, decode.d2.loss_dice: 1.0515, decode.d3.loss_cls: 0.2634, decode.d3.loss_mask: 0.5908, decode.d3.loss_dice: 1.0556, decode.d4.loss_cls: 0.2683, decode.d4.loss_mask: 0.5914, decode.d4.loss_dice: 1.0332, decode.d5.loss_cls: 0.2729, decode.d5.loss_mask: 0.5972, decode.d5.loss_dice: 1.0574, decode.d6.loss_cls: 0.2800, decode.d6.loss_mask: 0.5983, decode.d6.loss_dice: 1.0508, decode.d7.loss_cls: 0.2964, decode.d7.loss_mask: 0.5942, decode.d7.loss_dice: 1.0459, decode.d8.loss_cls: 0.2733, decode.d8.loss_mask: 0.5892, decode.d8.loss_dice: 1.0415, loss: 19.9382
2023-02-22 18:19:43,709 - mmseg - INFO - Saving checkpoint at 16000 iterations
2023-02-22 18:20:03,054 - mmseg - INFO - Exp name: my_city.py
2023-02-22 18:20:03,054 - mmseg - INFO - Iter [16000/80000]	lr: 1.149e-06, eta: 3 days, 2:55:16, time: 3.123, data_time: 0.021, memory: 31493, decode.loss_cls: 0.3269, decode.loss_mask: 0.5491, decode.loss_dice: 0.9253, decode.d0.loss_cls: 0.8452, decode.d0.loss_mask: 0.5870, decode.d0.loss_dice: 0.9819, decode.d1.loss_cls: 0.3396, decode.d1.loss_mask: 0.5580, decode.d1.loss_dice: 0.9513, decode.d2.loss_cls: 0.3359, decode.d2.loss_mask: 0.5544, decode.d2.loss_dice: 0.9498, decode.d3.loss_cls: 0.3234, decode.d3.loss_mask: 0.5518, decode.d3.loss_dice: 0.9327, decode.d4.loss_cls: 0.3350, decode.d4.loss_mask: 0.5513, decode.d4.loss_dice: 0.9389, decode.d5.loss_cls: 0.3196, decode.d5.loss_mask: 0.5540, decode.d5.loss_dice: 0.9511, decode.d6.loss_cls: 0.3148, decode.d6.loss_mask: 0.5493, decode.d6.loss_dice: 0.9316, decode.d7.loss_cls: 0.3059, decode.d7.loss_mask: 0.5527, decode.d7.loss_dice: 0.9297, decode.d8.loss_cls: 0.3149, decode.d8.loss_mask: 0.5501, decode.d8.loss_dice: 0.9344, loss: 18.7457
2023-02-22 18:22:19,690 - mmseg - INFO - Iter [16050/80000]	lr: 1.148e-06, eta: 3 days, 2:46:50, time: 2.733, data_time: 0.020, memory: 31493, decode.loss_cls: 0.2976, decode.loss_mask: 0.5624, decode.loss_dice: 0.9961, decode.d0.loss_cls: 0.8828, decode.d0.loss_mask: 0.5991, decode.d0.loss_dice: 1.0863, decode.d1.loss_cls: 0.3411, decode.d1.loss_mask: 0.5768, decode.d1.loss_dice: 1.0453, decode.d2.loss_cls: 0.3463, decode.d2.loss_mask: 0.5586, decode.d2.loss_dice: 1.0113, decode.d3.loss_cls: 0.3200, decode.d3.loss_mask: 0.5573, decode.d3.loss_dice: 0.9863, decode.d4.loss_cls: 0.3193, decode.d4.loss_mask: 0.5587, decode.d4.loss_dice: 1.0098, decode.d5.loss_cls: 0.3058, decode.d5.loss_mask: 0.5542, decode.d5.loss_dice: 1.0082, decode.d6.loss_cls: 0.3111, decode.d6.loss_mask: 0.5548, decode.d6.loss_dice: 0.9904, decode.d7.loss_cls: 0.3056, decode.d7.loss_mask: 0.5597, decode.d7.loss_dice: 0.9963, decode.d8.loss_cls: 0.3057, decode.d8.loss_mask: 0.5579, decode.d8.loss_dice: 0.9890, loss: 19.4940
2023-02-22 18:24:36,418 - mmseg - INFO - Iter [16100/80000]	lr: 1.147e-06, eta: 3 days, 2:38:27, time: 2.735, data_time: 0.018, memory: 31493, decode.loss_cls: 0.3172, decode.loss_mask: 0.5465, decode.loss_dice: 0.9417, decode.d0.loss_cls: 0.8834, decode.d0.loss_mask: 0.5852, decode.d0.loss_dice: 1.0448, decode.d1.loss_cls: 0.2951, decode.d1.loss_mask: 0.5537, decode.d1.loss_dice: 1.0040, decode.d2.loss_cls: 0.3180, decode.d2.loss_mask: 0.5459, decode.d2.loss_dice: 0.9545, decode.d3.loss_cls: 0.3213, decode.d3.loss_mask: 0.5437, decode.d3.loss_dice: 0.9381, decode.d4.loss_cls: 0.3080, decode.d4.loss_mask: 0.5387, decode.d4.loss_dice: 0.9574, decode.d5.loss_cls: 0.2980, decode.d5.loss_mask: 0.5448, decode.d5.loss_dice: 0.9583, decode.d6.loss_cls: 0.2945, decode.d6.loss_mask: 0.5473, decode.d6.loss_dice: 0.9671, decode.d7.loss_cls: 0.2933, decode.d7.loss_mask: 0.5474, decode.d7.loss_dice: 0.9653, decode.d8.loss_cls: 0.3074, decode.d8.loss_mask: 0.5462, decode.d8.loss_dice: 0.9562, loss: 18.8232
2023-02-22 18:26:52,909 - mmseg - INFO - Iter [16150/80000]	lr: 1.146e-06, eta: 3 days, 2:30:05, time: 2.730, data_time: 0.018, memory: 31493, decode.loss_cls: 0.3025, decode.loss_mask: 0.5497, decode.loss_dice: 0.9804, decode.d0.loss_cls: 0.9120, decode.d0.loss_mask: 0.5684, decode.d0.loss_dice: 1.0496, decode.d1.loss_cls: 0.3618, decode.d1.loss_mask: 0.5529, decode.d1.loss_dice: 1.0135, decode.d2.loss_cls: 0.3418, decode.d2.loss_mask: 0.5542, decode.d2.loss_dice: 0.9874, decode.d3.loss_cls: 0.3422, decode.d3.loss_mask: 0.5505, decode.d3.loss_dice: 0.9654, decode.d4.loss_cls: 0.3130, decode.d4.loss_mask: 0.5505, decode.d4.loss_dice: 0.9858, decode.d5.loss_cls: 0.3274, decode.d5.loss_mask: 0.5439, decode.d5.loss_dice: 0.9720, decode.d6.loss_cls: 0.3168, decode.d6.loss_mask: 0.5471, decode.d6.loss_dice: 0.9798, decode.d7.loss_cls: 0.3150, decode.d7.loss_mask: 0.5466, decode.d7.loss_dice: 0.9753, decode.d8.loss_cls: 0.3192, decode.d8.loss_mask: 0.5467, decode.d8.loss_dice: 0.9905, loss: 19.2618
2023-02-22 18:29:09,456 - mmseg - INFO - Iter [16200/80000]	lr: 1.145e-06, eta: 3 days, 2:21:46, time: 2.731, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2254, decode.loss_mask: 0.5422, decode.loss_dice: 0.8855, decode.d0.loss_cls: 0.8365, decode.d0.loss_mask: 0.5834, decode.d0.loss_dice: 0.9437, decode.d1.loss_cls: 0.2583, decode.d1.loss_mask: 0.5496, decode.d1.loss_dice: 0.9210, decode.d2.loss_cls: 0.2392, decode.d2.loss_mask: 0.5469, decode.d2.loss_dice: 0.8982, decode.d3.loss_cls: 0.2437, decode.d3.loss_mask: 0.5394, decode.d3.loss_dice: 0.8839, decode.d4.loss_cls: 0.2133, decode.d4.loss_mask: 0.5516, decode.d4.loss_dice: 0.8836, decode.d5.loss_cls: 0.2248, decode.d5.loss_mask: 0.5480, decode.d5.loss_dice: 0.8961, decode.d6.loss_cls: 0.2344, decode.d6.loss_mask: 0.5468, decode.d6.loss_dice: 0.8881, decode.d7.loss_cls: 0.2141, decode.d7.loss_mask: 0.5492, decode.d7.loss_dice: 0.8981, decode.d8.loss_cls: 0.2044, decode.d8.loss_mask: 0.5477, decode.d8.loss_dice: 0.8910, loss: 17.3879
2023-02-22 18:31:26,180 - mmseg - INFO - Iter [16250/80000]	lr: 1.144e-06, eta: 3 days, 2:13:29, time: 2.734, data_time: 0.019, memory: 31493, decode.loss_cls: 0.3583, decode.loss_mask: 0.5947, decode.loss_dice: 0.9931, decode.d0.loss_cls: 0.8904, decode.d0.loss_mask: 0.6399, decode.d0.loss_dice: 1.0883, decode.d1.loss_cls: 0.3270, decode.d1.loss_mask: 0.6189, decode.d1.loss_dice: 1.0658, decode.d2.loss_cls: 0.3418, decode.d2.loss_mask: 0.6076, decode.d2.loss_dice: 1.0317, decode.d3.loss_cls: 0.3154, decode.d3.loss_mask: 0.6058, decode.d3.loss_dice: 1.0185, decode.d4.loss_cls: 0.3445, decode.d4.loss_mask: 0.5932, decode.d4.loss_dice: 1.0106, decode.d5.loss_cls: 0.3394, decode.d5.loss_mask: 0.6035, decode.d5.loss_dice: 1.0224, decode.d6.loss_cls: 0.3646, decode.d6.loss_mask: 0.5892, decode.d6.loss_dice: 0.9967, decode.d7.loss_cls: 0.3282, decode.d7.loss_mask: 0.6011, decode.d7.loss_dice: 1.0069, decode.d8.loss_cls: 0.3443, decode.d8.loss_mask: 0.5994, decode.d8.loss_dice: 1.0060, loss: 20.2471
2023-02-22 18:33:42,959 - mmseg - INFO - Iter [16300/80000]	lr: 1.143e-06, eta: 3 days, 2:05:15, time: 2.736, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2721, decode.loss_mask: 0.5565, decode.loss_dice: 0.9821, decode.d0.loss_cls: 0.7982, decode.d0.loss_mask: 0.5813, decode.d0.loss_dice: 1.0570, decode.d1.loss_cls: 0.2912, decode.d1.loss_mask: 0.5660, decode.d1.loss_dice: 1.0306, decode.d2.loss_cls: 0.2709, decode.d2.loss_mask: 0.5469, decode.d2.loss_dice: 1.0077, decode.d3.loss_cls: 0.2678, decode.d3.loss_mask: 0.5501, decode.d3.loss_dice: 0.9939, decode.d4.loss_cls: 0.2747, decode.d4.loss_mask: 0.5575, decode.d4.loss_dice: 0.9977, decode.d5.loss_cls: 0.3126, decode.d5.loss_mask: 0.5530, decode.d5.loss_dice: 0.9923, decode.d6.loss_cls: 0.2690, decode.d6.loss_mask: 0.5609, decode.d6.loss_dice: 0.9968, decode.d7.loss_cls: 0.2642, decode.d7.loss_mask: 0.5608, decode.d7.loss_dice: 1.0031, decode.d8.loss_cls: 0.2824, decode.d8.loss_mask: 0.5579, decode.d8.loss_dice: 0.9845, loss: 18.9398
2023-02-22 18:36:02,153 - mmseg - INFO - Iter [16350/80000]	lr: 1.142e-06, eta: 3 days, 1:57:13, time: 2.784, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2886, decode.loss_mask: 0.5775, decode.loss_dice: 0.9759, decode.d0.loss_cls: 0.9154, decode.d0.loss_mask: 0.6276, decode.d0.loss_dice: 1.0622, decode.d1.loss_cls: 0.3600, decode.d1.loss_mask: 0.5794, decode.d1.loss_dice: 0.9920, decode.d2.loss_cls: 0.3092, decode.d2.loss_mask: 0.5798, decode.d2.loss_dice: 0.9670, decode.d3.loss_cls: 0.3165, decode.d3.loss_mask: 0.5732, decode.d3.loss_dice: 0.9552, decode.d4.loss_cls: 0.3037, decode.d4.loss_mask: 0.5812, decode.d4.loss_dice: 0.9638, decode.d5.loss_cls: 0.2912, decode.d5.loss_mask: 0.5795, decode.d5.loss_dice: 0.9835, decode.d6.loss_cls: 0.3177, decode.d6.loss_mask: 0.5783, decode.d6.loss_dice: 0.9766, decode.d7.loss_cls: 0.3005, decode.d7.loss_mask: 0.5768, decode.d7.loss_dice: 0.9758, decode.d8.loss_cls: 0.3099, decode.d8.loss_mask: 0.5750, decode.d8.loss_dice: 0.9625, loss: 19.3555
2023-02-22 18:38:20,401 - mmseg - INFO - Iter [16400/80000]	lr: 1.141e-06, eta: 3 days, 1:49:09, time: 2.765, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2914, decode.loss_mask: 0.5589, decode.loss_dice: 0.9606, decode.d0.loss_cls: 0.8919, decode.d0.loss_mask: 0.6324, decode.d0.loss_dice: 1.0539, decode.d1.loss_cls: 0.3057, decode.d1.loss_mask: 0.5886, decode.d1.loss_dice: 0.9975, decode.d2.loss_cls: 0.2909, decode.d2.loss_mask: 0.5713, decode.d2.loss_dice: 0.9692, decode.d3.loss_cls: 0.2934, decode.d3.loss_mask: 0.5667, decode.d3.loss_dice: 0.9735, decode.d4.loss_cls: 0.2846, decode.d4.loss_mask: 0.5647, decode.d4.loss_dice: 0.9522, decode.d5.loss_cls: 0.3056, decode.d5.loss_mask: 0.5565, decode.d5.loss_dice: 0.9593, decode.d6.loss_cls: 0.2884, decode.d6.loss_mask: 0.5640, decode.d6.loss_dice: 0.9639, decode.d7.loss_cls: 0.3059, decode.d7.loss_mask: 0.5578, decode.d7.loss_dice: 0.9499, decode.d8.loss_cls: 0.2864, decode.d8.loss_mask: 0.5586, decode.d8.loss_dice: 0.9469, loss: 18.9903
2023-02-22 18:40:41,045 - mmseg - INFO - Iter [16450/80000]	lr: 1.141e-06, eta: 3 days, 1:41:16, time: 2.813, data_time: 0.068, memory: 31493, decode.loss_cls: 0.2278, decode.loss_mask: 0.5380, decode.loss_dice: 0.9657, decode.d0.loss_cls: 0.8453, decode.d0.loss_mask: 0.5808, decode.d0.loss_dice: 1.0434, decode.d1.loss_cls: 0.2706, decode.d1.loss_mask: 0.5479, decode.d1.loss_dice: 0.9995, decode.d2.loss_cls: 0.2615, decode.d2.loss_mask: 0.5400, decode.d2.loss_dice: 0.9830, decode.d3.loss_cls: 0.2239, decode.d3.loss_mask: 0.5405, decode.d3.loss_dice: 0.9653, decode.d4.loss_cls: 0.2368, decode.d4.loss_mask: 0.5386, decode.d4.loss_dice: 0.9773, decode.d5.loss_cls: 0.2468, decode.d5.loss_mask: 0.5407, decode.d5.loss_dice: 0.9742, decode.d6.loss_cls: 0.2450, decode.d6.loss_mask: 0.5360, decode.d6.loss_dice: 0.9721, decode.d7.loss_cls: 0.2387, decode.d7.loss_mask: 0.5424, decode.d7.loss_dice: 0.9815, decode.d8.loss_cls: 0.2257, decode.d8.loss_mask: 0.5354, decode.d8.loss_dice: 0.9735, loss: 18.2980
2023-02-22 18:42:58,439 - mmseg - INFO - Iter [16500/80000]	lr: 1.140e-06, eta: 3 days, 1:33:13, time: 2.748, data_time: 0.020, memory: 31493, decode.loss_cls: 0.2549, decode.loss_mask: 0.5734, decode.loss_dice: 1.0034, decode.d0.loss_cls: 0.8184, decode.d0.loss_mask: 0.5928, decode.d0.loss_dice: 1.0535, decode.d1.loss_cls: 0.2653, decode.d1.loss_mask: 0.5770, decode.d1.loss_dice: 1.0350, decode.d2.loss_cls: 0.2554, decode.d2.loss_mask: 0.5761, decode.d2.loss_dice: 1.0149, decode.d3.loss_cls: 0.2585, decode.d3.loss_mask: 0.5748, decode.d3.loss_dice: 1.0012, decode.d4.loss_cls: 0.2284, decode.d4.loss_mask: 0.5722, decode.d4.loss_dice: 1.0072, decode.d5.loss_cls: 0.2559, decode.d5.loss_mask: 0.5774, decode.d5.loss_dice: 1.0001, decode.d6.loss_cls: 0.2347, decode.d6.loss_mask: 0.5761, decode.d6.loss_dice: 1.0053, decode.d7.loss_cls: 0.2217, decode.d7.loss_mask: 0.5815, decode.d7.loss_dice: 0.9986, decode.d8.loss_cls: 0.2661, decode.d8.loss_mask: 0.5778, decode.d8.loss_dice: 1.0108, loss: 18.9684
2023-02-22 18:45:15,224 - mmseg - INFO - Iter [16550/80000]	lr: 1.139e-06, eta: 3 days, 1:25:09, time: 2.736, data_time: 0.019, memory: 31493, decode.loss_cls: 0.3121, decode.loss_mask: 0.5087, decode.loss_dice: 0.9130, decode.d0.loss_cls: 0.8567, decode.d0.loss_mask: 0.5465, decode.d0.loss_dice: 0.9947, decode.d1.loss_cls: 0.2976, decode.d1.loss_mask: 0.5280, decode.d1.loss_dice: 0.9620, decode.d2.loss_cls: 0.3186, decode.d2.loss_mask: 0.5110, decode.d2.loss_dice: 0.9316, decode.d3.loss_cls: 0.3188, decode.d3.loss_mask: 0.5047, decode.d3.loss_dice: 0.9238, decode.d4.loss_cls: 0.3101, decode.d4.loss_mask: 0.5084, decode.d4.loss_dice: 0.9314, decode.d5.loss_cls: 0.3221, decode.d5.loss_mask: 0.5152, decode.d5.loss_dice: 0.9183, decode.d6.loss_cls: 0.3303, decode.d6.loss_mask: 0.5077, decode.d6.loss_dice: 0.8944, decode.d7.loss_cls: 0.3104, decode.d7.loss_mask: 0.5164, decode.d7.loss_dice: 0.9085, decode.d8.loss_cls: 0.3109, decode.d8.loss_mask: 0.5115, decode.d8.loss_dice: 0.9219, loss: 18.1451
2023-02-22 18:47:31,854 - mmseg - INFO - Iter [16600/80000]	lr: 1.138e-06, eta: 3 days, 1:17:07, time: 2.733, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2202, decode.loss_mask: 0.5355, decode.loss_dice: 0.9640, decode.d0.loss_cls: 0.7300, decode.d0.loss_mask: 0.5636, decode.d0.loss_dice: 1.0272, decode.d1.loss_cls: 0.2195, decode.d1.loss_mask: 0.5478, decode.d1.loss_dice: 0.9896, decode.d2.loss_cls: 0.2098, decode.d2.loss_mask: 0.5383, decode.d2.loss_dice: 0.9645, decode.d3.loss_cls: 0.2072, decode.d3.loss_mask: 0.5284, decode.d3.loss_dice: 0.9530, decode.d4.loss_cls: 0.2138, decode.d4.loss_mask: 0.5315, decode.d4.loss_dice: 0.9512, decode.d5.loss_cls: 0.2388, decode.d5.loss_mask: 0.5358, decode.d5.loss_dice: 0.9519, decode.d6.loss_cls: 0.2236, decode.d6.loss_mask: 0.5347, decode.d6.loss_dice: 0.9544, decode.d7.loss_cls: 0.2102, decode.d7.loss_mask: 0.5362, decode.d7.loss_dice: 0.9553, decode.d8.loss_cls: 0.2251, decode.d8.loss_mask: 0.5337, decode.d8.loss_dice: 0.9619, loss: 17.7566
2023-02-22 18:49:48,631 - mmseg - INFO - Iter [16650/80000]	lr: 1.137e-06, eta: 3 days, 1:09:08, time: 2.736, data_time: 0.019, memory: 31493, decode.loss_cls: 0.3311, decode.loss_mask: 0.5969, decode.loss_dice: 0.9909, decode.d0.loss_cls: 0.8548, decode.d0.loss_mask: 0.6382, decode.d0.loss_dice: 1.0664, decode.d1.loss_cls: 0.3100, decode.d1.loss_mask: 0.6009, decode.d1.loss_dice: 1.0280, decode.d2.loss_cls: 0.3187, decode.d2.loss_mask: 0.5920, decode.d2.loss_dice: 0.9964, decode.d3.loss_cls: 0.3184, decode.d3.loss_mask: 0.5882, decode.d3.loss_dice: 0.9738, decode.d4.loss_cls: 0.3279, decode.d4.loss_mask: 0.5922, decode.d4.loss_dice: 0.9876, decode.d5.loss_cls: 0.3392, decode.d5.loss_mask: 0.5791, decode.d5.loss_dice: 0.9637, decode.d6.loss_cls: 0.3050, decode.d6.loss_mask: 0.5937, decode.d6.loss_dice: 0.9671, decode.d7.loss_cls: 0.3137, decode.d7.loss_mask: 0.5937, decode.d7.loss_dice: 0.9749, decode.d8.loss_cls: 0.3215, decode.d8.loss_mask: 0.5847, decode.d8.loss_dice: 0.9736, loss: 19.6222
2023-02-22 18:52:05,434 - mmseg - INFO - Iter [16700/80000]	lr: 1.136e-06, eta: 3 days, 1:01:11, time: 2.736, data_time: 0.019, memory: 31493, decode.loss_cls: 0.3042, decode.loss_mask: 0.5459, decode.loss_dice: 1.0034, decode.d0.loss_cls: 0.8217, decode.d0.loss_mask: 0.6058, decode.d0.loss_dice: 1.1239, decode.d1.loss_cls: 0.3011, decode.d1.loss_mask: 0.5586, decode.d1.loss_dice: 1.0413, decode.d2.loss_cls: 0.2994, decode.d2.loss_mask: 0.5538, decode.d2.loss_dice: 1.0238, decode.d3.loss_cls: 0.3121, decode.d3.loss_mask: 0.5557, decode.d3.loss_dice: 1.0134, decode.d4.loss_cls: 0.3082, decode.d4.loss_mask: 0.5529, decode.d4.loss_dice: 1.0185, decode.d5.loss_cls: 0.3083, decode.d5.loss_mask: 0.5599, decode.d5.loss_dice: 1.0110, decode.d6.loss_cls: 0.3000, decode.d6.loss_mask: 0.5497, decode.d6.loss_dice: 0.9884, decode.d7.loss_cls: 0.2917, decode.d7.loss_mask: 0.5456, decode.d7.loss_dice: 0.9969, decode.d8.loss_cls: 0.3034, decode.d8.loss_mask: 0.5427, decode.d8.loss_dice: 1.0197, loss: 19.3611
2023-02-22 18:54:21,911 - mmseg - INFO - Iter [16750/80000]	lr: 1.135e-06, eta: 3 days, 0:53:15, time: 2.729, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2701, decode.loss_mask: 0.5343, decode.loss_dice: 0.9082, decode.d0.loss_cls: 0.8415, decode.d0.loss_mask: 0.5686, decode.d0.loss_dice: 0.9828, decode.d1.loss_cls: 0.3076, decode.d1.loss_mask: 0.5434, decode.d1.loss_dice: 0.9464, decode.d2.loss_cls: 0.2803, decode.d2.loss_mask: 0.5419, decode.d2.loss_dice: 0.9277, decode.d3.loss_cls: 0.2749, decode.d3.loss_mask: 0.5429, decode.d3.loss_dice: 0.9005, decode.d4.loss_cls: 0.2887, decode.d4.loss_mask: 0.5344, decode.d4.loss_dice: 0.9146, decode.d5.loss_cls: 0.2886, decode.d5.loss_mask: 0.5344, decode.d5.loss_dice: 0.9049, decode.d6.loss_cls: 0.2787, decode.d6.loss_mask: 0.5342, decode.d6.loss_dice: 0.8962, decode.d7.loss_cls: 0.2736, decode.d7.loss_mask: 0.5358, decode.d7.loss_dice: 0.9097, decode.d8.loss_cls: 0.2926, decode.d8.loss_mask: 0.5351, decode.d8.loss_dice: 0.9050, loss: 17.9973
2023-02-22 18:56:38,450 - mmseg - INFO - Iter [16800/80000]	lr: 1.134e-06, eta: 3 days, 0:45:21, time: 2.731, data_time: 0.018, memory: 31493, decode.loss_cls: 0.2652, decode.loss_mask: 0.5711, decode.loss_dice: 0.9851, decode.d0.loss_cls: 0.8154, decode.d0.loss_mask: 0.6055, decode.d0.loss_dice: 1.0323, decode.d1.loss_cls: 0.2399, decode.d1.loss_mask: 0.5790, decode.d1.loss_dice: 1.0138, decode.d2.loss_cls: 0.2488, decode.d2.loss_mask: 0.5778, decode.d2.loss_dice: 0.9928, decode.d3.loss_cls: 0.2393, decode.d3.loss_mask: 0.5738, decode.d3.loss_dice: 0.9958, decode.d4.loss_cls: 0.2261, decode.d4.loss_mask: 0.5770, decode.d4.loss_dice: 1.0066, decode.d5.loss_cls: 0.2410, decode.d5.loss_mask: 0.5754, decode.d5.loss_dice: 1.0033, decode.d6.loss_cls: 0.2149, decode.d6.loss_mask: 0.5786, decode.d6.loss_dice: 1.0152, decode.d7.loss_cls: 0.2472, decode.d7.loss_mask: 0.5745, decode.d7.loss_dice: 0.9857, decode.d8.loss_cls: 0.2425, decode.d8.loss_mask: 0.5720, decode.d8.loss_dice: 1.0008, loss: 18.7966
2023-02-22 18:58:54,838 - mmseg - INFO - Iter [16850/80000]	lr: 1.133e-06, eta: 3 days, 0:37:28, time: 2.728, data_time: 0.018, memory: 31493, decode.loss_cls: 0.2975, decode.loss_mask: 0.5715, decode.loss_dice: 0.9117, decode.d0.loss_cls: 0.8623, decode.d0.loss_mask: 0.6033, decode.d0.loss_dice: 0.9585, decode.d1.loss_cls: 0.2985, decode.d1.loss_mask: 0.5681, decode.d1.loss_dice: 0.9405, decode.d2.loss_cls: 0.2909, decode.d2.loss_mask: 0.5662, decode.d2.loss_dice: 0.9329, decode.d3.loss_cls: 0.3137, decode.d3.loss_mask: 0.5653, decode.d3.loss_dice: 0.9115, decode.d4.loss_cls: 0.3130, decode.d4.loss_mask: 0.5649, decode.d4.loss_dice: 0.9251, decode.d5.loss_cls: 0.2971, decode.d5.loss_mask: 0.5722, decode.d5.loss_dice: 0.9217, decode.d6.loss_cls: 0.2974, decode.d6.loss_mask: 0.5667, decode.d6.loss_dice: 0.9236, decode.d7.loss_cls: 0.2988, decode.d7.loss_mask: 0.5732, decode.d7.loss_dice: 0.9221, decode.d8.loss_cls: 0.2926, decode.d8.loss_mask: 0.5679, decode.d8.loss_dice: 0.9230, loss: 18.5518
2023-02-22 19:01:11,851 - mmseg - INFO - Iter [16900/80000]	lr: 1.133e-06, eta: 3 days, 0:29:40, time: 2.740, data_time: 0.019, memory: 31493, decode.loss_cls: 0.3270, decode.loss_mask: 0.5049, decode.loss_dice: 0.9556, decode.d0.loss_cls: 0.8936, decode.d0.loss_mask: 0.5235, decode.d0.loss_dice: 1.0521, decode.d1.loss_cls: 0.3457, decode.d1.loss_mask: 0.5068, decode.d1.loss_dice: 1.0017, decode.d2.loss_cls: 0.3354, decode.d2.loss_mask: 0.5033, decode.d2.loss_dice: 0.9697, decode.d3.loss_cls: 0.3304, decode.d3.loss_mask: 0.4990, decode.d3.loss_dice: 0.9843, decode.d4.loss_cls: 0.3223, decode.d4.loss_mask: 0.5017, decode.d4.loss_dice: 0.9681, decode.d5.loss_cls: 0.3422, decode.d5.loss_mask: 0.5041, decode.d5.loss_dice: 0.9711, decode.d6.loss_cls: 0.3468, decode.d6.loss_mask: 0.5009, decode.d6.loss_dice: 0.9663, decode.d7.loss_cls: 0.3340, decode.d7.loss_mask: 0.5039, decode.d7.loss_dice: 0.9660, decode.d8.loss_cls: 0.3151, decode.d8.loss_mask: 0.5038, decode.d8.loss_dice: 0.9575, loss: 18.7367
2023-02-22 19:03:28,715 - mmseg - INFO - Iter [16950/80000]	lr: 1.132e-06, eta: 3 days, 0:21:53, time: 2.737, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2889, decode.loss_mask: 0.5053, decode.loss_dice: 0.9842, decode.d0.loss_cls: 0.7692, decode.d0.loss_mask: 0.5613, decode.d0.loss_dice: 1.0839, decode.d1.loss_cls: 0.3616, decode.d1.loss_mask: 0.5141, decode.d1.loss_dice: 1.0054, decode.d2.loss_cls: 0.2979, decode.d2.loss_mask: 0.5107, decode.d2.loss_dice: 1.0018, decode.d3.loss_cls: 0.2954, decode.d3.loss_mask: 0.5088, decode.d3.loss_dice: 0.9756, decode.d4.loss_cls: 0.2918, decode.d4.loss_mask: 0.5067, decode.d4.loss_dice: 0.9834, decode.d5.loss_cls: 0.2878, decode.d5.loss_mask: 0.5015, decode.d5.loss_dice: 0.9782, decode.d6.loss_cls: 0.2887, decode.d6.loss_mask: 0.5133, decode.d6.loss_dice: 1.0198, decode.d7.loss_cls: 0.2862, decode.d7.loss_mask: 0.5075, decode.d7.loss_dice: 0.9804, decode.d8.loss_cls: 0.2873, decode.d8.loss_mask: 0.5068, decode.d8.loss_dice: 0.9950, loss: 18.5987
2023-02-22 19:05:45,490 - mmseg - INFO - Saving checkpoint at 17000 iterations
2023-02-22 19:06:07,028 - mmseg - INFO - Exp name: my_city.py
2023-02-22 19:06:07,028 - mmseg - INFO - Iter [17000/80000]	lr: 1.131e-06, eta: 3 days, 0:15:27, time: 3.166, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2875, decode.loss_mask: 0.5836, decode.loss_dice: 0.9863, decode.d0.loss_cls: 0.7925, decode.d0.loss_mask: 0.6355, decode.d0.loss_dice: 1.0603, decode.d1.loss_cls: 0.2991, decode.d1.loss_mask: 0.5945, decode.d1.loss_dice: 0.9978, decode.d2.loss_cls: 0.2841, decode.d2.loss_mask: 0.5866, decode.d2.loss_dice: 0.9830, decode.d3.loss_cls: 0.2714, decode.d3.loss_mask: 0.5898, decode.d3.loss_dice: 0.9909, decode.d4.loss_cls: 0.2807, decode.d4.loss_mask: 0.5863, decode.d4.loss_dice: 0.9838, decode.d5.loss_cls: 0.2875, decode.d5.loss_mask: 0.5812, decode.d5.loss_dice: 0.9643, decode.d6.loss_cls: 0.2921, decode.d6.loss_mask: 0.5768, decode.d6.loss_dice: 0.9649, decode.d7.loss_cls: 0.3128, decode.d7.loss_mask: 0.5823, decode.d7.loss_dice: 0.9717, decode.d8.loss_cls: 0.2977, decode.d8.loss_mask: 0.5839, decode.d8.loss_dice: 0.9654, loss: 19.1742
2023-02-22 19:08:23,754 - mmseg - INFO - Iter [17050/80000]	lr: 1.130e-06, eta: 3 days, 0:07:43, time: 2.735, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2591, decode.loss_mask: 0.5526, decode.loss_dice: 0.9708, decode.d0.loss_cls: 0.8266, decode.d0.loss_mask: 0.5961, decode.d0.loss_dice: 1.0530, decode.d1.loss_cls: 0.2501, decode.d1.loss_mask: 0.5726, decode.d1.loss_dice: 1.0194, decode.d2.loss_cls: 0.2672, decode.d2.loss_mask: 0.5604, decode.d2.loss_dice: 0.9912, decode.d3.loss_cls: 0.2874, decode.d3.loss_mask: 0.5534, decode.d3.loss_dice: 0.9799, decode.d4.loss_cls: 0.2718, decode.d4.loss_mask: 0.5488, decode.d4.loss_dice: 0.9821, decode.d5.loss_cls: 0.2668, decode.d5.loss_mask: 0.5524, decode.d5.loss_dice: 0.9834, decode.d6.loss_cls: 0.2764, decode.d6.loss_mask: 0.5523, decode.d6.loss_dice: 0.9794, decode.d7.loss_cls: 0.2646, decode.d7.loss_mask: 0.5494, decode.d7.loss_dice: 0.9816, decode.d8.loss_cls: 0.2613, decode.d8.loss_mask: 0.5555, decode.d8.loss_dice: 0.9863, loss: 18.7518
2023-02-22 19:10:40,490 - mmseg - INFO - Iter [17100/80000]	lr: 1.129e-06, eta: 3 days, 0:00:01, time: 2.735, data_time: 0.018, memory: 31493, decode.loss_cls: 0.3046, decode.loss_mask: 0.6506, decode.loss_dice: 1.0790, decode.d0.loss_cls: 0.8742, decode.d0.loss_mask: 0.6669, decode.d0.loss_dice: 1.1606, decode.d1.loss_cls: 0.3465, decode.d1.loss_mask: 0.6450, decode.d1.loss_dice: 1.1027, decode.d2.loss_cls: 0.3105, decode.d2.loss_mask: 0.6524, decode.d2.loss_dice: 1.0758, decode.d3.loss_cls: 0.3283, decode.d3.loss_mask: 0.6518, decode.d3.loss_dice: 1.0745, decode.d4.loss_cls: 0.3243, decode.d4.loss_mask: 0.6419, decode.d4.loss_dice: 1.0806, decode.d5.loss_cls: 0.3312, decode.d5.loss_mask: 0.6423, decode.d5.loss_dice: 1.0531, decode.d6.loss_cls: 0.3100, decode.d6.loss_mask: 0.6509, decode.d6.loss_dice: 1.0564, decode.d7.loss_cls: 0.3123, decode.d7.loss_mask: 0.6380, decode.d7.loss_dice: 1.0639, decode.d8.loss_cls: 0.3034, decode.d8.loss_mask: 0.6537, decode.d8.loss_dice: 1.0757, loss: 21.0609
2023-02-22 19:12:59,941 - mmseg - INFO - Iter [17150/80000]	lr: 1.128e-06, eta: 2 days, 23:52:31, time: 2.789, data_time: 0.068, memory: 31493, decode.loss_cls: 0.3269, decode.loss_mask: 0.5664, decode.loss_dice: 0.9241, decode.d0.loss_cls: 0.8626, decode.d0.loss_mask: 0.6208, decode.d0.loss_dice: 0.9995, decode.d1.loss_cls: 0.3748, decode.d1.loss_mask: 0.5631, decode.d1.loss_dice: 0.9529, decode.d2.loss_cls: 0.3760, decode.d2.loss_mask: 0.5547, decode.d2.loss_dice: 0.9245, decode.d3.loss_cls: 0.3555, decode.d3.loss_mask: 0.5563, decode.d3.loss_dice: 0.9138, decode.d4.loss_cls: 0.3730, decode.d4.loss_mask: 0.5647, decode.d4.loss_dice: 0.9104, decode.d5.loss_cls: 0.3506, decode.d5.loss_mask: 0.5642, decode.d5.loss_dice: 0.9129, decode.d6.loss_cls: 0.3138, decode.d6.loss_mask: 0.5659, decode.d6.loss_dice: 0.9191, decode.d7.loss_cls: 0.3291, decode.d7.loss_mask: 0.5648, decode.d7.loss_dice: 0.9278, decode.d8.loss_cls: 0.3315, decode.d8.loss_mask: 0.5639, decode.d8.loss_dice: 0.9141, loss: 18.9774
2023-02-22 19:15:16,757 - mmseg - INFO - Iter [17200/80000]	lr: 1.127e-06, eta: 2 days, 23:44:53, time: 2.736, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2645, decode.loss_mask: 0.5946, decode.loss_dice: 0.9523, decode.d0.loss_cls: 0.8089, decode.d0.loss_mask: 0.6233, decode.d0.loss_dice: 1.0336, decode.d1.loss_cls: 0.2722, decode.d1.loss_mask: 0.5914, decode.d1.loss_dice: 0.9899, decode.d2.loss_cls: 0.2655, decode.d2.loss_mask: 0.5878, decode.d2.loss_dice: 0.9574, decode.d3.loss_cls: 0.2819, decode.d3.loss_mask: 0.5797, decode.d3.loss_dice: 0.9484, decode.d4.loss_cls: 0.2618, decode.d4.loss_mask: 0.5880, decode.d4.loss_dice: 0.9788, decode.d5.loss_cls: 0.2733, decode.d5.loss_mask: 0.5833, decode.d5.loss_dice: 0.9524, decode.d6.loss_cls: 0.2756, decode.d6.loss_mask: 0.5850, decode.d6.loss_dice: 0.9516, decode.d7.loss_cls: 0.2664, decode.d7.loss_mask: 0.5902, decode.d7.loss_dice: 0.9399, decode.d8.loss_cls: 0.2913, decode.d8.loss_mask: 0.5879, decode.d8.loss_dice: 0.9466, loss: 18.8233
2023-02-22 19:17:33,728 - mmseg - INFO - Iter [17250/80000]	lr: 1.126e-06, eta: 2 days, 23:37:18, time: 2.739, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2480, decode.loss_mask: 0.5452, decode.loss_dice: 0.9449, decode.d0.loss_cls: 0.8063, decode.d0.loss_mask: 0.5660, decode.d0.loss_dice: 1.0243, decode.d1.loss_cls: 0.2660, decode.d1.loss_mask: 0.5560, decode.d1.loss_dice: 0.9838, decode.d2.loss_cls: 0.2703, decode.d2.loss_mask: 0.5490, decode.d2.loss_dice: 0.9717, decode.d3.loss_cls: 0.2434, decode.d3.loss_mask: 0.5459, decode.d3.loss_dice: 0.9261, decode.d4.loss_cls: 0.2330, decode.d4.loss_mask: 0.5454, decode.d4.loss_dice: 0.9464, decode.d5.loss_cls: 0.2439, decode.d5.loss_mask: 0.5429, decode.d5.loss_dice: 0.9448, decode.d6.loss_cls: 0.2295, decode.d6.loss_mask: 0.5448, decode.d6.loss_dice: 0.9503, decode.d7.loss_cls: 0.2448, decode.d7.loss_mask: 0.5455, decode.d7.loss_dice: 0.9366, decode.d8.loss_cls: 0.2520, decode.d8.loss_mask: 0.5428, decode.d8.loss_dice: 0.9463, loss: 18.0960
2023-02-22 19:19:50,371 - mmseg - INFO - Iter [17300/80000]	lr: 1.125e-06, eta: 2 days, 23:29:43, time: 2.733, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2523, decode.loss_mask: 0.5596, decode.loss_dice: 0.9189, decode.d0.loss_cls: 0.8375, decode.d0.loss_mask: 0.5910, decode.d0.loss_dice: 0.9880, decode.d1.loss_cls: 0.2870, decode.d1.loss_mask: 0.5545, decode.d1.loss_dice: 0.9319, decode.d2.loss_cls: 0.2856, decode.d2.loss_mask: 0.5567, decode.d2.loss_dice: 0.9338, decode.d3.loss_cls: 0.2636, decode.d3.loss_mask: 0.5562, decode.d3.loss_dice: 0.8964, decode.d4.loss_cls: 0.2692, decode.d4.loss_mask: 0.5594, decode.d4.loss_dice: 0.9173, decode.d5.loss_cls: 0.2862, decode.d5.loss_mask: 0.5457, decode.d5.loss_dice: 0.9078, decode.d6.loss_cls: 0.2639, decode.d6.loss_mask: 0.5603, decode.d6.loss_dice: 0.9111, decode.d7.loss_cls: 0.2552, decode.d7.loss_mask: 0.5654, decode.d7.loss_dice: 0.9127, decode.d8.loss_cls: 0.2544, decode.d8.loss_mask: 0.5611, decode.d8.loss_dice: 0.9139, loss: 18.0966
2023-02-22 19:22:07,101 - mmseg - INFO - Iter [17350/80000]	lr: 1.124e-06, eta: 2 days, 23:22:10, time: 2.735, data_time: 0.020, memory: 31493, decode.loss_cls: 0.2255, decode.loss_mask: 0.5225, decode.loss_dice: 0.9270, decode.d0.loss_cls: 0.8026, decode.d0.loss_mask: 0.5537, decode.d0.loss_dice: 0.9640, decode.d1.loss_cls: 0.2284, decode.d1.loss_mask: 0.5256, decode.d1.loss_dice: 0.9315, decode.d2.loss_cls: 0.2379, decode.d2.loss_mask: 0.5187, decode.d2.loss_dice: 0.9235, decode.d3.loss_cls: 0.2357, decode.d3.loss_mask: 0.5210, decode.d3.loss_dice: 0.9363, decode.d4.loss_cls: 0.2319, decode.d4.loss_mask: 0.5225, decode.d4.loss_dice: 0.9245, decode.d5.loss_cls: 0.2459, decode.d5.loss_mask: 0.5202, decode.d5.loss_dice: 0.9185, decode.d6.loss_cls: 0.2446, decode.d6.loss_mask: 0.5234, decode.d6.loss_dice: 0.9229, decode.d7.loss_cls: 0.2335, decode.d7.loss_mask: 0.5250, decode.d7.loss_dice: 0.9293, decode.d8.loss_cls: 0.2230, decode.d8.loss_mask: 0.5233, decode.d8.loss_dice: 0.9296, loss: 17.4718
2023-02-22 19:24:24,099 - mmseg - INFO - Iter [17400/80000]	lr: 1.124e-06, eta: 2 days, 23:14:40, time: 2.740, data_time: 0.020, memory: 31493, decode.loss_cls: 0.3213, decode.loss_mask: 0.5872, decode.loss_dice: 0.9637, decode.d0.loss_cls: 0.8515, decode.d0.loss_mask: 0.6329, decode.d0.loss_dice: 1.0305, decode.d1.loss_cls: 0.3412, decode.d1.loss_mask: 0.5956, decode.d1.loss_dice: 0.9773, decode.d2.loss_cls: 0.3377, decode.d2.loss_mask: 0.5980, decode.d2.loss_dice: 0.9579, decode.d3.loss_cls: 0.3375, decode.d3.loss_mask: 0.5836, decode.d3.loss_dice: 0.9624, decode.d4.loss_cls: 0.3368, decode.d4.loss_mask: 0.5756, decode.d4.loss_dice: 0.9587, decode.d5.loss_cls: 0.3187, decode.d5.loss_mask: 0.5929, decode.d5.loss_dice: 0.9593, decode.d6.loss_cls: 0.3047, decode.d6.loss_mask: 0.6007, decode.d6.loss_dice: 0.9789, decode.d7.loss_cls: 0.3218, decode.d7.loss_mask: 0.5995, decode.d7.loss_dice: 0.9814, decode.d8.loss_cls: 0.3179, decode.d8.loss_mask: 0.5865, decode.d8.loss_dice: 0.9556, loss: 19.4674
2023-02-22 19:26:40,918 - mmseg - INFO - Iter [17450/80000]	lr: 1.123e-06, eta: 2 days, 23:07:12, time: 2.736, data_time: 0.021, memory: 31493, decode.loss_cls: 0.3147, decode.loss_mask: 0.6152, decode.loss_dice: 1.0418, decode.d0.loss_cls: 0.7913, decode.d0.loss_mask: 0.6683, decode.d0.loss_dice: 1.1208, decode.d1.loss_cls: 0.3557, decode.d1.loss_mask: 0.6309, decode.d1.loss_dice: 1.0651, decode.d2.loss_cls: 0.3376, decode.d2.loss_mask: 0.6176, decode.d2.loss_dice: 1.0605, decode.d3.loss_cls: 0.3345, decode.d3.loss_mask: 0.6187, decode.d3.loss_dice: 1.0614, decode.d4.loss_cls: 0.3030, decode.d4.loss_mask: 0.6253, decode.d4.loss_dice: 1.0535, decode.d5.loss_cls: 0.3194, decode.d5.loss_mask: 0.6278, decode.d5.loss_dice: 1.0531, decode.d6.loss_cls: 0.3294, decode.d6.loss_mask: 0.6210, decode.d6.loss_dice: 1.0462, decode.d7.loss_cls: 0.3154, decode.d7.loss_mask: 0.6176, decode.d7.loss_dice: 1.0549, decode.d8.loss_cls: 0.3205, decode.d8.loss_mask: 0.6219, decode.d8.loss_dice: 1.0490, loss: 20.5921
2023-02-22 19:28:57,975 - mmseg - INFO - Iter [17500/80000]	lr: 1.122e-06, eta: 2 days, 22:59:46, time: 2.741, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2051, decode.loss_mask: 0.5519, decode.loss_dice: 0.9833, decode.d0.loss_cls: 0.7287, decode.d0.loss_mask: 0.5765, decode.d0.loss_dice: 1.0397, decode.d1.loss_cls: 0.2653, decode.d1.loss_mask: 0.5396, decode.d1.loss_dice: 0.9935, decode.d2.loss_cls: 0.2205, decode.d2.loss_mask: 0.5328, decode.d2.loss_dice: 0.9798, decode.d3.loss_cls: 0.2190, decode.d3.loss_mask: 0.5307, decode.d3.loss_dice: 0.9669, decode.d4.loss_cls: 0.2284, decode.d4.loss_mask: 0.5318, decode.d4.loss_dice: 0.9704, decode.d5.loss_cls: 0.2262, decode.d5.loss_mask: 0.5308, decode.d5.loss_dice: 0.9781, decode.d6.loss_cls: 0.2252, decode.d6.loss_mask: 0.5407, decode.d6.loss_dice: 0.9667, decode.d7.loss_cls: 0.2153, decode.d7.loss_mask: 0.5386, decode.d7.loss_dice: 0.9708, decode.d8.loss_cls: 0.1949, decode.d8.loss_mask: 0.5380, decode.d8.loss_dice: 0.9820, loss: 17.9711
2023-02-22 19:31:16,624 - mmseg - INFO - Iter [17550/80000]	lr: 1.121e-06, eta: 2 days, 22:52:27, time: 2.773, data_time: 0.020, memory: 31493, decode.loss_cls: 0.3161, decode.loss_mask: 0.5499, decode.loss_dice: 1.0213, decode.d0.loss_cls: 0.8257, decode.d0.loss_mask: 0.6142, decode.d0.loss_dice: 1.1171, decode.d1.loss_cls: 0.3179, decode.d1.loss_mask: 0.5649, decode.d1.loss_dice: 1.0448, decode.d2.loss_cls: 0.3021, decode.d2.loss_mask: 0.5633, decode.d2.loss_dice: 1.0346, decode.d3.loss_cls: 0.3110, decode.d3.loss_mask: 0.5559, decode.d3.loss_dice: 1.0260, decode.d4.loss_cls: 0.2877, decode.d4.loss_mask: 0.5573, decode.d4.loss_dice: 1.0260, decode.d5.loss_cls: 0.3184, decode.d5.loss_mask: 0.5532, decode.d5.loss_dice: 1.0153, decode.d6.loss_cls: 0.3025, decode.d6.loss_mask: 0.5535, decode.d6.loss_dice: 1.0170, decode.d7.loss_cls: 0.3157, decode.d7.loss_mask: 0.5496, decode.d7.loss_dice: 1.0117, decode.d8.loss_cls: 0.3112, decode.d8.loss_mask: 0.5530, decode.d8.loss_dice: 1.0282, loss: 19.5651
2023-02-22 19:33:39,651 - mmseg - INFO - Iter [17600/80000]	lr: 1.120e-06, eta: 2 days, 22:45:25, time: 2.860, data_time: 0.025, memory: 31493, decode.loss_cls: 0.2793, decode.loss_mask: 0.5407, decode.loss_dice: 0.8919, decode.d0.loss_cls: 0.8432, decode.d0.loss_mask: 0.5835, decode.d0.loss_dice: 0.9749, decode.d1.loss_cls: 0.3215, decode.d1.loss_mask: 0.5496, decode.d1.loss_dice: 0.8983, decode.d2.loss_cls: 0.3284, decode.d2.loss_mask: 0.5470, decode.d2.loss_dice: 0.8851, decode.d3.loss_cls: 0.3250, decode.d3.loss_mask: 0.5463, decode.d3.loss_dice: 0.8805, decode.d4.loss_cls: 0.2978, decode.d4.loss_mask: 0.5444, decode.d4.loss_dice: 0.8685, decode.d5.loss_cls: 0.3009, decode.d5.loss_mask: 0.5426, decode.d5.loss_dice: 0.8728, decode.d6.loss_cls: 0.2903, decode.d6.loss_mask: 0.5373, decode.d6.loss_dice: 0.8770, decode.d7.loss_cls: 0.2634, decode.d7.loss_mask: 0.5427, decode.d7.loss_dice: 0.8813, decode.d8.loss_cls: 0.2687, decode.d8.loss_mask: 0.5458, decode.d8.loss_dice: 0.8830, loss: 17.9118
2023-02-22 19:36:05,564 - mmseg - INFO - Iter [17650/80000]	lr: 1.119e-06, eta: 2 days, 22:38:36, time: 2.918, data_time: 0.024, memory: 31493, decode.loss_cls: 0.2548, decode.loss_mask: 0.5699, decode.loss_dice: 0.9980, decode.d0.loss_cls: 0.7683, decode.d0.loss_mask: 0.5978, decode.d0.loss_dice: 1.0444, decode.d1.loss_cls: 0.2719, decode.d1.loss_mask: 0.5908, decode.d1.loss_dice: 1.0321, decode.d2.loss_cls: 0.2641, decode.d2.loss_mask: 0.5712, decode.d2.loss_dice: 0.9813, decode.d3.loss_cls: 0.2480, decode.d3.loss_mask: 0.5671, decode.d3.loss_dice: 0.9857, decode.d4.loss_cls: 0.2537, decode.d4.loss_mask: 0.5610, decode.d4.loss_dice: 0.9774, decode.d5.loss_cls: 0.2279, decode.d5.loss_mask: 0.5771, decode.d5.loss_dice: 1.0111, decode.d6.loss_cls: 0.2571, decode.d6.loss_mask: 0.5590, decode.d6.loss_dice: 0.9887, decode.d7.loss_cls: 0.2683, decode.d7.loss_mask: 0.5653, decode.d7.loss_dice: 0.9974, decode.d8.loss_cls: 0.2539, decode.d8.loss_mask: 0.5654, decode.d8.loss_dice: 0.9900, loss: 18.7986
2023-02-22 19:38:22,617 - mmseg - INFO - Iter [17700/80000]	lr: 1.118e-06, eta: 2 days, 22:31:16, time: 2.741, data_time: 0.020, memory: 31493, decode.loss_cls: 0.3130, decode.loss_mask: 0.5606, decode.loss_dice: 0.9719, decode.d0.loss_cls: 0.8632, decode.d0.loss_mask: 0.6141, decode.d0.loss_dice: 1.0447, decode.d1.loss_cls: 0.3310, decode.d1.loss_mask: 0.5752, decode.d1.loss_dice: 0.9984, decode.d2.loss_cls: 0.3170, decode.d2.loss_mask: 0.5671, decode.d2.loss_dice: 0.9932, decode.d3.loss_cls: 0.3049, decode.d3.loss_mask: 0.5599, decode.d3.loss_dice: 0.9834, decode.d4.loss_cls: 0.3231, decode.d4.loss_mask: 0.5574, decode.d4.loss_dice: 0.9695, decode.d5.loss_cls: 0.3044, decode.d5.loss_mask: 0.5608, decode.d5.loss_dice: 0.9801, decode.d6.loss_cls: 0.3194, decode.d6.loss_mask: 0.5565, decode.d6.loss_dice: 0.9834, decode.d7.loss_cls: 0.3093, decode.d7.loss_mask: 0.5664, decode.d7.loss_dice: 0.9851, decode.d8.loss_cls: 0.3093, decode.d8.loss_mask: 0.5614, decode.d8.loss_dice: 0.9863, loss: 19.2700
2023-02-22 19:40:39,536 - mmseg - INFO - Iter [17750/80000]	lr: 1.117e-06, eta: 2 days, 22:23:58, time: 2.738, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2917, decode.loss_mask: 0.5620, decode.loss_dice: 0.9787, decode.d0.loss_cls: 0.8505, decode.d0.loss_mask: 0.6001, decode.d0.loss_dice: 1.0576, decode.d1.loss_cls: 0.3314, decode.d1.loss_mask: 0.5849, decode.d1.loss_dice: 1.0131, decode.d2.loss_cls: 0.2803, decode.d2.loss_mask: 0.5773, decode.d2.loss_dice: 1.0116, decode.d3.loss_cls: 0.3230, decode.d3.loss_mask: 0.5655, decode.d3.loss_dice: 0.9678, decode.d4.loss_cls: 0.2922, decode.d4.loss_mask: 0.5622, decode.d4.loss_dice: 0.9775, decode.d5.loss_cls: 0.2968, decode.d5.loss_mask: 0.5641, decode.d5.loss_dice: 0.9844, decode.d6.loss_cls: 0.2879, decode.d6.loss_mask: 0.5584, decode.d6.loss_dice: 0.9679, decode.d7.loss_cls: 0.3224, decode.d7.loss_mask: 0.5565, decode.d7.loss_dice: 0.9518, decode.d8.loss_cls: 0.2911, decode.d8.loss_mask: 0.5606, decode.d8.loss_dice: 0.9725, loss: 19.1419
2023-02-22 19:42:56,233 - mmseg - INFO - Iter [17800/80000]	lr: 1.116e-06, eta: 2 days, 22:16:41, time: 2.734, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2508, decode.loss_mask: 0.5555, decode.loss_dice: 0.9553, decode.d0.loss_cls: 0.8144, decode.d0.loss_mask: 0.6130, decode.d0.loss_dice: 1.0426, decode.d1.loss_cls: 0.3039, decode.d1.loss_mask: 0.5644, decode.d1.loss_dice: 0.9817, decode.d2.loss_cls: 0.2546, decode.d2.loss_mask: 0.5673, decode.d2.loss_dice: 0.9831, decode.d3.loss_cls: 0.2498, decode.d3.loss_mask: 0.5724, decode.d3.loss_dice: 0.9480, decode.d4.loss_cls: 0.2664, decode.d4.loss_mask: 0.5658, decode.d4.loss_dice: 0.9462, decode.d5.loss_cls: 0.2468, decode.d5.loss_mask: 0.5634, decode.d5.loss_dice: 0.9586, decode.d6.loss_cls: 0.2488, decode.d6.loss_mask: 0.5706, decode.d6.loss_dice: 0.9526, decode.d7.loss_cls: 0.2523, decode.d7.loss_mask: 0.5636, decode.d7.loss_dice: 0.9576, decode.d8.loss_cls: 0.2584, decode.d8.loss_mask: 0.5607, decode.d8.loss_dice: 0.9553, loss: 18.5240
2023-02-22 19:45:13,431 - mmseg - INFO - Iter [17850/80000]	lr: 1.115e-06, eta: 2 days, 22:09:27, time: 2.744, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2364, decode.loss_mask: 0.5511, decode.loss_dice: 0.9103, decode.d0.loss_cls: 0.8007, decode.d0.loss_mask: 0.5908, decode.d0.loss_dice: 1.0186, decode.d1.loss_cls: 0.2961, decode.d1.loss_mask: 0.5501, decode.d1.loss_dice: 0.9324, decode.d2.loss_cls: 0.2545, decode.d2.loss_mask: 0.5475, decode.d2.loss_dice: 0.9358, decode.d3.loss_cls: 0.2636, decode.d3.loss_mask: 0.5452, decode.d3.loss_dice: 0.9213, decode.d4.loss_cls: 0.2691, decode.d4.loss_mask: 0.5422, decode.d4.loss_dice: 0.9076, decode.d5.loss_cls: 0.2368, decode.d5.loss_mask: 0.5498, decode.d5.loss_dice: 0.9234, decode.d6.loss_cls: 0.2278, decode.d6.loss_mask: 0.5509, decode.d6.loss_dice: 0.9177, decode.d7.loss_cls: 0.2201, decode.d7.loss_mask: 0.5554, decode.d7.loss_dice: 0.9184, decode.d8.loss_cls: 0.2374, decode.d8.loss_mask: 0.5573, decode.d8.loss_dice: 0.9161, loss: 17.8844
2023-02-22 19:47:32,878 - mmseg - INFO - Iter [17900/80000]	lr: 1.115e-06, eta: 2 days, 22:02:23, time: 2.789, data_time: 0.070, memory: 31493, decode.loss_cls: 0.2360, decode.loss_mask: 0.5712, decode.loss_dice: 0.9499, decode.d0.loss_cls: 0.7488, decode.d0.loss_mask: 0.6238, decode.d0.loss_dice: 1.0092, decode.d1.loss_cls: 0.2375, decode.d1.loss_mask: 0.5779, decode.d1.loss_dice: 0.9453, decode.d2.loss_cls: 0.2294, decode.d2.loss_mask: 0.5779, decode.d2.loss_dice: 0.9445, decode.d3.loss_cls: 0.2299, decode.d3.loss_mask: 0.5698, decode.d3.loss_dice: 0.9602, decode.d4.loss_cls: 0.2535, decode.d4.loss_mask: 0.5751, decode.d4.loss_dice: 0.9498, decode.d5.loss_cls: 0.2248, decode.d5.loss_mask: 0.5721, decode.d5.loss_dice: 0.9517, decode.d6.loss_cls: 0.2502, decode.d6.loss_mask: 0.5693, decode.d6.loss_dice: 0.9415, decode.d7.loss_cls: 0.2445, decode.d7.loss_mask: 0.5721, decode.d7.loss_dice: 0.9334, decode.d8.loss_cls: 0.2343, decode.d8.loss_mask: 0.5713, decode.d8.loss_dice: 0.9280, loss: 18.1830
2023-02-22 19:49:50,219 - mmseg - INFO - Iter [17950/80000]	lr: 1.114e-06, eta: 2 days, 21:55:13, time: 2.747, data_time: 0.022, memory: 31493, decode.loss_cls: 0.2393, decode.loss_mask: 0.5586, decode.loss_dice: 0.9473, decode.d0.loss_cls: 0.7787, decode.d0.loss_mask: 0.5965, decode.d0.loss_dice: 1.0162, decode.d1.loss_cls: 0.2619, decode.d1.loss_mask: 0.5696, decode.d1.loss_dice: 0.9821, decode.d2.loss_cls: 0.2429, decode.d2.loss_mask: 0.5687, decode.d2.loss_dice: 0.9529, decode.d3.loss_cls: 0.2114, decode.d3.loss_mask: 0.5761, decode.d3.loss_dice: 0.9579, decode.d4.loss_cls: 0.2575, decode.d4.loss_mask: 0.5639, decode.d4.loss_dice: 0.9494, decode.d5.loss_cls: 0.2625, decode.d5.loss_mask: 0.5595, decode.d5.loss_dice: 0.9486, decode.d6.loss_cls: 0.2765, decode.d6.loss_mask: 0.5583, decode.d6.loss_dice: 0.9343, decode.d7.loss_cls: 0.2563, decode.d7.loss_mask: 0.5593, decode.d7.loss_dice: 0.9395, decode.d8.loss_cls: 0.2616, decode.d8.loss_mask: 0.5595, decode.d8.loss_dice: 0.9389, loss: 18.2858
2023-02-22 19:52:06,947 - mmseg - INFO - Saving checkpoint at 18000 iterations
2023-02-22 19:52:31,618 - mmseg - INFO - Exp name: my_city.py
2023-02-22 19:52:31,618 - mmseg - INFO - Iter [18000/80000]	lr: 1.113e-06, eta: 2 days, 21:49:27, time: 3.228, data_time: 0.020, memory: 31493, decode.loss_cls: 0.2506, decode.loss_mask: 0.5296, decode.loss_dice: 0.9015, decode.d0.loss_cls: 0.7582, decode.d0.loss_mask: 0.5546, decode.d0.loss_dice: 0.9629, decode.d1.loss_cls: 0.2896, decode.d1.loss_mask: 0.5352, decode.d1.loss_dice: 0.9152, decode.d2.loss_cls: 0.2580, decode.d2.loss_mask: 0.5284, decode.d2.loss_dice: 0.8977, decode.d3.loss_cls: 0.2519, decode.d3.loss_mask: 0.5337, decode.d3.loss_dice: 0.9000, decode.d4.loss_cls: 0.2558, decode.d4.loss_mask: 0.5321, decode.d4.loss_dice: 0.9170, decode.d5.loss_cls: 0.2506, decode.d5.loss_mask: 0.5289, decode.d5.loss_dice: 0.9018, decode.d6.loss_cls: 0.2315, decode.d6.loss_mask: 0.5286, decode.d6.loss_dice: 0.9033, decode.d7.loss_cls: 0.2465, decode.d7.loss_mask: 0.5282, decode.d7.loss_dice: 0.9069, decode.d8.loss_cls: 0.2367, decode.d8.loss_mask: 0.5297, decode.d8.loss_dice: 0.9035, loss: 17.4682
2023-02-22 19:54:48,143 - mmseg - INFO - Iter [18050/80000]	lr: 1.112e-06, eta: 2 days, 21:42:17, time: 2.731, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2252, decode.loss_mask: 0.5187, decode.loss_dice: 0.8917, decode.d0.loss_cls: 0.7533, decode.d0.loss_mask: 0.5492, decode.d0.loss_dice: 0.9483, decode.d1.loss_cls: 0.2457, decode.d1.loss_mask: 0.5204, decode.d1.loss_dice: 0.8903, decode.d2.loss_cls: 0.2035, decode.d2.loss_mask: 0.5296, decode.d2.loss_dice: 0.8948, decode.d3.loss_cls: 0.2212, decode.d3.loss_mask: 0.5295, decode.d3.loss_dice: 0.9013, decode.d4.loss_cls: 0.2116, decode.d4.loss_mask: 0.5246, decode.d4.loss_dice: 0.9067, decode.d5.loss_cls: 0.2145, decode.d5.loss_mask: 0.5327, decode.d5.loss_dice: 0.8914, decode.d6.loss_cls: 0.2090, decode.d6.loss_mask: 0.5310, decode.d6.loss_dice: 0.9005, decode.d7.loss_cls: 0.2031, decode.d7.loss_mask: 0.5218, decode.d7.loss_dice: 0.8958, decode.d8.loss_cls: 0.2184, decode.d8.loss_mask: 0.5260, decode.d8.loss_dice: 0.8956, loss: 17.0056
2023-02-22 19:57:04,594 - mmseg - INFO - Iter [18100/80000]	lr: 1.111e-06, eta: 2 days, 21:35:09, time: 2.729, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2339, decode.loss_mask: 0.5173, decode.loss_dice: 0.9124, decode.d0.loss_cls: 0.7424, decode.d0.loss_mask: 0.5457, decode.d0.loss_dice: 0.9747, decode.d1.loss_cls: 0.2762, decode.d1.loss_mask: 0.5254, decode.d1.loss_dice: 0.9182, decode.d2.loss_cls: 0.2516, decode.d2.loss_mask: 0.5173, decode.d2.loss_dice: 0.9012, decode.d3.loss_cls: 0.2257, decode.d3.loss_mask: 0.5133, decode.d3.loss_dice: 0.9144, decode.d4.loss_cls: 0.2274, decode.d4.loss_mask: 0.5135, decode.d4.loss_dice: 0.9204, decode.d5.loss_cls: 0.2208, decode.d5.loss_mask: 0.5125, decode.d5.loss_dice: 0.9050, decode.d6.loss_cls: 0.2213, decode.d6.loss_mask: 0.5213, decode.d6.loss_dice: 0.9001, decode.d7.loss_cls: 0.2031, decode.d7.loss_mask: 0.5201, decode.d7.loss_dice: 0.9105, decode.d8.loss_cls: 0.2144, decode.d8.loss_mask: 0.5201, decode.d8.loss_dice: 0.9156, loss: 17.1958
2023-02-22 19:59:21,390 - mmseg - INFO - Iter [18150/80000]	lr: 1.110e-06, eta: 2 days, 21:28:03, time: 2.736, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2357, decode.loss_mask: 0.5432, decode.loss_dice: 0.9353, decode.d0.loss_cls: 0.7870, decode.d0.loss_mask: 0.5780, decode.d0.loss_dice: 1.0022, decode.d1.loss_cls: 0.2364, decode.d1.loss_mask: 0.5544, decode.d1.loss_dice: 0.9612, decode.d2.loss_cls: 0.2448, decode.d2.loss_mask: 0.5519, decode.d2.loss_dice: 0.9452, decode.d3.loss_cls: 0.2650, decode.d3.loss_mask: 0.5440, decode.d3.loss_dice: 0.9470, decode.d4.loss_cls: 0.2564, decode.d4.loss_mask: 0.5462, decode.d4.loss_dice: 0.9393, decode.d5.loss_cls: 0.2412, decode.d5.loss_mask: 0.5436, decode.d5.loss_dice: 0.9425, decode.d6.loss_cls: 0.2338, decode.d6.loss_mask: 0.5447, decode.d6.loss_dice: 0.9466, decode.d7.loss_cls: 0.2341, decode.d7.loss_mask: 0.5478, decode.d7.loss_dice: 0.9541, decode.d8.loss_cls: 0.2457, decode.d8.loss_mask: 0.5451, decode.d8.loss_dice: 0.9526, loss: 18.0050
2023-02-22 20:01:37,935 - mmseg - INFO - Iter [18200/80000]	lr: 1.109e-06, eta: 2 days, 21:20:58, time: 2.731, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2786, decode.loss_mask: 0.5924, decode.loss_dice: 1.0186, decode.d0.loss_cls: 0.7516, decode.d0.loss_mask: 0.6233, decode.d0.loss_dice: 1.0697, decode.d1.loss_cls: 0.3048, decode.d1.loss_mask: 0.5858, decode.d1.loss_dice: 1.0390, decode.d2.loss_cls: 0.2778, decode.d2.loss_mask: 0.5890, decode.d2.loss_dice: 1.0332, decode.d3.loss_cls: 0.2629, decode.d3.loss_mask: 0.5949, decode.d3.loss_dice: 1.0281, decode.d4.loss_cls: 0.2739, decode.d4.loss_mask: 0.5767, decode.d4.loss_dice: 1.0330, decode.d5.loss_cls: 0.2774, decode.d5.loss_mask: 0.5749, decode.d5.loss_dice: 1.0278, decode.d6.loss_cls: 0.2924, decode.d6.loss_mask: 0.5785, decode.d6.loss_dice: 1.0253, decode.d7.loss_cls: 0.2736, decode.d7.loss_mask: 0.5818, decode.d7.loss_dice: 1.0201, decode.d8.loss_cls: 0.2566, decode.d8.loss_mask: 0.5956, decode.d8.loss_dice: 1.0212, loss: 19.4588
2023-02-22 20:03:54,536 - mmseg - INFO - Iter [18250/80000]	lr: 1.108e-06, eta: 2 days, 21:13:55, time: 2.732, data_time: 0.018, memory: 31493, decode.loss_cls: 0.2804, decode.loss_mask: 0.5442, decode.loss_dice: 0.9022, decode.d0.loss_cls: 0.8111, decode.d0.loss_mask: 0.5694, decode.d0.loss_dice: 0.9812, decode.d1.loss_cls: 0.2925, decode.d1.loss_mask: 0.5536, decode.d1.loss_dice: 0.9323, decode.d2.loss_cls: 0.2669, decode.d2.loss_mask: 0.5422, decode.d2.loss_dice: 0.9342, decode.d3.loss_cls: 0.2723, decode.d3.loss_mask: 0.5432, decode.d3.loss_dice: 0.9038, decode.d4.loss_cls: 0.2561, decode.d4.loss_mask: 0.5415, decode.d4.loss_dice: 0.9142, decode.d5.loss_cls: 0.2561, decode.d5.loss_mask: 0.5461, decode.d5.loss_dice: 0.9222, decode.d6.loss_cls: 0.2623, decode.d6.loss_mask: 0.5507, decode.d6.loss_dice: 0.9128, decode.d7.loss_cls: 0.2707, decode.d7.loss_mask: 0.5509, decode.d7.loss_dice: 0.9035, decode.d8.loss_cls: 0.2709, decode.d8.loss_mask: 0.5449, decode.d8.loss_dice: 0.9067, loss: 17.9390
2023-02-22 20:06:11,418 - mmseg - INFO - Iter [18300/80000]	lr: 1.107e-06, eta: 2 days, 21:06:54, time: 2.738, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2458, decode.loss_mask: 0.5430, decode.loss_dice: 0.9044, decode.d0.loss_cls: 0.7416, decode.d0.loss_mask: 0.5946, decode.d0.loss_dice: 0.9841, decode.d1.loss_cls: 0.2786, decode.d1.loss_mask: 0.5510, decode.d1.loss_dice: 0.9379, decode.d2.loss_cls: 0.2621, decode.d2.loss_mask: 0.5400, decode.d2.loss_dice: 0.9243, decode.d3.loss_cls: 0.2390, decode.d3.loss_mask: 0.5416, decode.d3.loss_dice: 0.9009, decode.d4.loss_cls: 0.2510, decode.d4.loss_mask: 0.5505, decode.d4.loss_dice: 0.9059, decode.d5.loss_cls: 0.2508, decode.d5.loss_mask: 0.5474, decode.d5.loss_dice: 0.9172, decode.d6.loss_cls: 0.2416, decode.d6.loss_mask: 0.5505, decode.d6.loss_dice: 0.9011, decode.d7.loss_cls: 0.2632, decode.d7.loss_mask: 0.5380, decode.d7.loss_dice: 0.9007, decode.d8.loss_cls: 0.2519, decode.d8.loss_mask: 0.5435, decode.d8.loss_dice: 0.9181, loss: 17.7201
2023-02-22 20:08:28,470 - mmseg - INFO - Iter [18350/80000]	lr: 1.106e-06, eta: 2 days, 20:59:55, time: 2.741, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2303, decode.loss_mask: 0.4993, decode.loss_dice: 0.8721, decode.d0.loss_cls: 0.7549, decode.d0.loss_mask: 0.5540, decode.d0.loss_dice: 0.9393, decode.d1.loss_cls: 0.2856, decode.d1.loss_mask: 0.5171, decode.d1.loss_dice: 0.8909, decode.d2.loss_cls: 0.2478, decode.d2.loss_mask: 0.5090, decode.d2.loss_dice: 0.8588, decode.d3.loss_cls: 0.2363, decode.d3.loss_mask: 0.5054, decode.d3.loss_dice: 0.8876, decode.d4.loss_cls: 0.2577, decode.d4.loss_mask: 0.5018, decode.d4.loss_dice: 0.8765, decode.d5.loss_cls: 0.2511, decode.d5.loss_mask: 0.4986, decode.d5.loss_dice: 0.8819, decode.d6.loss_cls: 0.2454, decode.d6.loss_mask: 0.5042, decode.d6.loss_dice: 0.8561, decode.d7.loss_cls: 0.2396, decode.d7.loss_mask: 0.5097, decode.d7.loss_dice: 0.8847, decode.d8.loss_cls: 0.2256, decode.d8.loss_mask: 0.5052, decode.d8.loss_dice: 0.8598, loss: 16.8858
2023-02-22 20:10:45,588 - mmseg - INFO - Iter [18400/80000]	lr: 1.106e-06, eta: 2 days, 20:52:59, time: 2.742, data_time: 0.020, memory: 31493, decode.loss_cls: 0.2842, decode.loss_mask: 0.5553, decode.loss_dice: 0.9581, decode.d0.loss_cls: 0.8539, decode.d0.loss_mask: 0.6035, decode.d0.loss_dice: 1.0160, decode.d1.loss_cls: 0.3294, decode.d1.loss_mask: 0.5621, decode.d1.loss_dice: 0.9560, decode.d2.loss_cls: 0.2939, decode.d2.loss_mask: 0.5575, decode.d2.loss_dice: 0.9326, decode.d3.loss_cls: 0.2870, decode.d3.loss_mask: 0.5538, decode.d3.loss_dice: 0.9490, decode.d4.loss_cls: 0.3017, decode.d4.loss_mask: 0.5554, decode.d4.loss_dice: 0.9451, decode.d5.loss_cls: 0.2905, decode.d5.loss_mask: 0.5594, decode.d5.loss_dice: 0.9446, decode.d6.loss_cls: 0.3096, decode.d6.loss_mask: 0.5534, decode.d6.loss_dice: 0.9391, decode.d7.loss_cls: 0.2938, decode.d7.loss_mask: 0.5601, decode.d7.loss_dice: 0.9520, decode.d8.loss_cls: 0.2842, decode.d8.loss_mask: 0.5624, decode.d8.loss_dice: 0.9561, loss: 18.6996
2023-02-22 20:13:14,641 - mmseg - INFO - Iter [18450/80000]	lr: 1.105e-06, eta: 2 days, 20:46:43, time: 2.981, data_time: 0.027, memory: 31493, decode.loss_cls: 0.2763, decode.loss_mask: 0.5363, decode.loss_dice: 0.9552, decode.d0.loss_cls: 0.8458, decode.d0.loss_mask: 0.5849, decode.d0.loss_dice: 1.0072, decode.d1.loss_cls: 0.3117, decode.d1.loss_mask: 0.5424, decode.d1.loss_dice: 0.9779, decode.d2.loss_cls: 0.2726, decode.d2.loss_mask: 0.5389, decode.d2.loss_dice: 0.9608, decode.d3.loss_cls: 0.2688, decode.d3.loss_mask: 0.5333, decode.d3.loss_dice: 0.9501, decode.d4.loss_cls: 0.2917, decode.d4.loss_mask: 0.5375, decode.d4.loss_dice: 0.9282, decode.d5.loss_cls: 0.2868, decode.d5.loss_mask: 0.5369, decode.d5.loss_dice: 0.9358, decode.d6.loss_cls: 0.2940, decode.d6.loss_mask: 0.5389, decode.d6.loss_dice: 0.9299, decode.d7.loss_cls: 0.2971, decode.d7.loss_mask: 0.5414, decode.d7.loss_dice: 0.9390, decode.d8.loss_cls: 0.2944, decode.d8.loss_mask: 0.5398, decode.d8.loss_dice: 0.9538, loss: 18.4075
2023-02-22 20:15:31,369 - mmseg - INFO - Iter [18500/80000]	lr: 1.104e-06, eta: 2 days, 20:39:48, time: 2.735, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2951, decode.loss_mask: 0.5596, decode.loss_dice: 0.9335, decode.d0.loss_cls: 0.7712, decode.d0.loss_mask: 0.6042, decode.d0.loss_dice: 0.9984, decode.d1.loss_cls: 0.3283, decode.d1.loss_mask: 0.5680, decode.d1.loss_dice: 0.9689, decode.d2.loss_cls: 0.3001, decode.d2.loss_mask: 0.5640, decode.d2.loss_dice: 0.9233, decode.d3.loss_cls: 0.2822, decode.d3.loss_mask: 0.5592, decode.d3.loss_dice: 0.9260, decode.d4.loss_cls: 0.2999, decode.d4.loss_mask: 0.5657, decode.d4.loss_dice: 0.9069, decode.d5.loss_cls: 0.3071, decode.d5.loss_mask: 0.5627, decode.d5.loss_dice: 0.9193, decode.d6.loss_cls: 0.2887, decode.d6.loss_mask: 0.5625, decode.d6.loss_dice: 0.9107, decode.d7.loss_cls: 0.3023, decode.d7.loss_mask: 0.5584, decode.d7.loss_dice: 0.9256, decode.d8.loss_cls: 0.2957, decode.d8.loss_mask: 0.5576, decode.d8.loss_dice: 0.9212, loss: 18.4665
2023-02-22 20:17:48,216 - mmseg - INFO - Iter [18550/80000]	lr: 1.103e-06, eta: 2 days, 20:32:54, time: 2.737, data_time: 0.021, memory: 31493, decode.loss_cls: 0.3094, decode.loss_mask: 0.5130, decode.loss_dice: 0.9066, decode.d0.loss_cls: 0.8059, decode.d0.loss_mask: 0.5461, decode.d0.loss_dice: 0.9892, decode.d1.loss_cls: 0.3413, decode.d1.loss_mask: 0.5179, decode.d1.loss_dice: 0.9276, decode.d2.loss_cls: 0.3117, decode.d2.loss_mask: 0.5097, decode.d2.loss_dice: 0.9026, decode.d3.loss_cls: 0.3309, decode.d3.loss_mask: 0.5132, decode.d3.loss_dice: 0.9110, decode.d4.loss_cls: 0.3124, decode.d4.loss_mask: 0.5132, decode.d4.loss_dice: 0.9073, decode.d5.loss_cls: 0.3323, decode.d5.loss_mask: 0.5086, decode.d5.loss_dice: 0.9005, decode.d6.loss_cls: 0.3403, decode.d6.loss_mask: 0.5086, decode.d6.loss_dice: 0.8864, decode.d7.loss_cls: 0.3283, decode.d7.loss_mask: 0.5183, decode.d7.loss_dice: 0.8889, decode.d8.loss_cls: 0.3236, decode.d8.loss_mask: 0.5142, decode.d8.loss_dice: 0.8902, loss: 18.0093
2023-02-22 20:20:08,056 - mmseg - INFO - Iter [18600/80000]	lr: 1.102e-06, eta: 2 days, 20:26:12, time: 2.797, data_time: 0.070, memory: 31493, decode.loss_cls: 0.2739, decode.loss_mask: 0.5488, decode.loss_dice: 1.0097, decode.d0.loss_cls: 0.8304, decode.d0.loss_mask: 0.5722, decode.d0.loss_dice: 1.0798, decode.d1.loss_cls: 0.3068, decode.d1.loss_mask: 0.5648, decode.d1.loss_dice: 1.0270, decode.d2.loss_cls: 0.2998, decode.d2.loss_mask: 0.5594, decode.d2.loss_dice: 1.0172, decode.d3.loss_cls: 0.2672, decode.d3.loss_mask: 0.5567, decode.d3.loss_dice: 1.0100, decode.d4.loss_cls: 0.2643, decode.d4.loss_mask: 0.5529, decode.d4.loss_dice: 1.0171, decode.d5.loss_cls: 0.2643, decode.d5.loss_mask: 0.5536, decode.d5.loss_dice: 1.0116, decode.d6.loss_cls: 0.2884, decode.d6.loss_mask: 0.5596, decode.d6.loss_dice: 1.0022, decode.d7.loss_cls: 0.2804, decode.d7.loss_mask: 0.5583, decode.d7.loss_dice: 1.0087, decode.d8.loss_cls: 0.2862, decode.d8.loss_mask: 0.5510, decode.d8.loss_dice: 1.0000, loss: 19.1225
2023-02-22 20:22:33,579 - mmseg - INFO - Iter [18650/80000]	lr: 1.101e-06, eta: 2 days, 20:19:50, time: 2.910, data_time: 0.022, memory: 31493, decode.loss_cls: 0.2917, decode.loss_mask: 0.5521, decode.loss_dice: 0.9247, decode.d0.loss_cls: 0.8130, decode.d0.loss_mask: 0.6195, decode.d0.loss_dice: 1.0239, decode.d1.loss_cls: 0.3102, decode.d1.loss_mask: 0.5645, decode.d1.loss_dice: 0.9245, decode.d2.loss_cls: 0.2841, decode.d2.loss_mask: 0.5529, decode.d2.loss_dice: 0.9305, decode.d3.loss_cls: 0.2906, decode.d3.loss_mask: 0.5524, decode.d3.loss_dice: 0.9391, decode.d4.loss_cls: 0.2967, decode.d4.loss_mask: 0.5486, decode.d4.loss_dice: 0.9275, decode.d5.loss_cls: 0.3004, decode.d5.loss_mask: 0.5448, decode.d5.loss_dice: 0.9098, decode.d6.loss_cls: 0.3045, decode.d6.loss_mask: 0.5459, decode.d6.loss_dice: 0.9080, decode.d7.loss_cls: 0.2866, decode.d7.loss_mask: 0.5485, decode.d7.loss_dice: 0.9268, decode.d8.loss_cls: 0.2832, decode.d8.loss_mask: 0.5538, decode.d8.loss_dice: 0.9219, loss: 18.3805
2023-02-22 20:24:50,857 - mmseg - INFO - Iter [18700/80000]	lr: 1.100e-06, eta: 2 days, 20:13:03, time: 2.746, data_time: 0.022, memory: 31493, decode.loss_cls: 0.2775, decode.loss_mask: 0.5149, decode.loss_dice: 0.9306, decode.d0.loss_cls: 0.7040, decode.d0.loss_mask: 0.5315, decode.d0.loss_dice: 1.0330, decode.d1.loss_cls: 0.2712, decode.d1.loss_mask: 0.5166, decode.d1.loss_dice: 0.9703, decode.d2.loss_cls: 0.2725, decode.d2.loss_mask: 0.5123, decode.d2.loss_dice: 0.9500, decode.d3.loss_cls: 0.2545, decode.d3.loss_mask: 0.5085, decode.d3.loss_dice: 0.9359, decode.d4.loss_cls: 0.2565, decode.d4.loss_mask: 0.5028, decode.d4.loss_dice: 0.9420, decode.d5.loss_cls: 0.2775, decode.d5.loss_mask: 0.5076, decode.d5.loss_dice: 0.9342, decode.d6.loss_cls: 0.2662, decode.d6.loss_mask: 0.5073, decode.d6.loss_dice: 0.9192, decode.d7.loss_cls: 0.2472, decode.d7.loss_mask: 0.5111, decode.d7.loss_dice: 0.9486, decode.d8.loss_cls: 0.2629, decode.d8.loss_mask: 0.5109, decode.d8.loss_dice: 0.9253, loss: 17.7025
2023-02-22 20:27:07,775 - mmseg - INFO - Iter [18750/80000]	lr: 1.099e-06, eta: 2 days, 20:06:15, time: 2.738, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2171, decode.loss_mask: 0.5766, decode.loss_dice: 0.9319, decode.d0.loss_cls: 0.7166, decode.d0.loss_mask: 0.6193, decode.d0.loss_dice: 0.9920, decode.d1.loss_cls: 0.2511, decode.d1.loss_mask: 0.5874, decode.d1.loss_dice: 0.9428, decode.d2.loss_cls: 0.2237, decode.d2.loss_mask: 0.5721, decode.d2.loss_dice: 0.9340, decode.d3.loss_cls: 0.2309, decode.d3.loss_mask: 0.5724, decode.d3.loss_dice: 0.9360, decode.d4.loss_cls: 0.2141, decode.d4.loss_mask: 0.5760, decode.d4.loss_dice: 0.9364, decode.d5.loss_cls: 0.2275, decode.d5.loss_mask: 0.5771, decode.d5.loss_dice: 0.9373, decode.d6.loss_cls: 0.2178, decode.d6.loss_mask: 0.5723, decode.d6.loss_dice: 0.9271, decode.d7.loss_cls: 0.2187, decode.d7.loss_mask: 0.5732, decode.d7.loss_dice: 0.9266, decode.d8.loss_cls: 0.2116, decode.d8.loss_mask: 0.5800, decode.d8.loss_dice: 0.9317, loss: 17.9311
2023-02-22 20:29:34,832 - mmseg - INFO - Iter [18800/80000]	lr: 1.098e-06, eta: 2 days, 20:00:02, time: 2.941, data_time: 0.060, memory: 31493, decode.loss_cls: 0.3049, decode.loss_mask: 0.5913, decode.loss_dice: 0.9483, decode.d0.loss_cls: 0.7552, decode.d0.loss_mask: 0.6289, decode.d0.loss_dice: 1.0102, decode.d1.loss_cls: 0.2879, decode.d1.loss_mask: 0.6069, decode.d1.loss_dice: 0.9631, decode.d2.loss_cls: 0.2825, decode.d2.loss_mask: 0.6042, decode.d2.loss_dice: 0.9399, decode.d3.loss_cls: 0.2894, decode.d3.loss_mask: 0.5930, decode.d3.loss_dice: 0.9513, decode.d4.loss_cls: 0.2958, decode.d4.loss_mask: 0.5912, decode.d4.loss_dice: 0.9409, decode.d5.loss_cls: 0.3214, decode.d5.loss_mask: 0.5846, decode.d5.loss_dice: 0.9373, decode.d6.loss_cls: 0.3193, decode.d6.loss_mask: 0.5914, decode.d6.loss_dice: 0.9571, decode.d7.loss_cls: 0.3125, decode.d7.loss_mask: 0.5912, decode.d7.loss_dice: 0.9311, decode.d8.loss_cls: 0.3042, decode.d8.loss_mask: 0.5930, decode.d8.loss_dice: 0.9414, loss: 18.9696
2023-02-22 20:32:00,929 - mmseg - INFO - Iter [18850/80000]	lr: 1.098e-06, eta: 2 days, 19:53:48, time: 2.922, data_time: 0.029, memory: 31493, decode.loss_cls: 0.2539, decode.loss_mask: 0.5330, decode.loss_dice: 0.9916, decode.d0.loss_cls: 0.7560, decode.d0.loss_mask: 0.5518, decode.d0.loss_dice: 1.0629, decode.d1.loss_cls: 0.2432, decode.d1.loss_mask: 0.5432, decode.d1.loss_dice: 1.0177, decode.d2.loss_cls: 0.2474, decode.d2.loss_mask: 0.5329, decode.d2.loss_dice: 0.9865, decode.d3.loss_cls: 0.2568, decode.d3.loss_mask: 0.5306, decode.d3.loss_dice: 0.9946, decode.d4.loss_cls: 0.2583, decode.d4.loss_mask: 0.5308, decode.d4.loss_dice: 0.9889, decode.d5.loss_cls: 0.2596, decode.d5.loss_mask: 0.5322, decode.d5.loss_dice: 0.9930, decode.d6.loss_cls: 0.2597, decode.d6.loss_mask: 0.5308, decode.d6.loss_dice: 0.9876, decode.d7.loss_cls: 0.2628, decode.d7.loss_mask: 0.5322, decode.d7.loss_dice: 0.9927, decode.d8.loss_cls: 0.2760, decode.d8.loss_mask: 0.5287, decode.d8.loss_dice: 0.9824, loss: 18.4178
2023-02-22 20:34:18,077 - mmseg - INFO - Iter [18900/80000]	lr: 1.097e-06, eta: 2 days, 19:47:05, time: 2.743, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2631, decode.loss_mask: 0.5603, decode.loss_dice: 0.9881, decode.d0.loss_cls: 0.7907, decode.d0.loss_mask: 0.6007, decode.d0.loss_dice: 1.0488, decode.d1.loss_cls: 0.2973, decode.d1.loss_mask: 0.5857, decode.d1.loss_dice: 1.0179, decode.d2.loss_cls: 0.2536, decode.d2.loss_mask: 0.5772, decode.d2.loss_dice: 1.0169, decode.d3.loss_cls: 0.2673, decode.d3.loss_mask: 0.5617, decode.d3.loss_dice: 0.9965, decode.d4.loss_cls: 0.2712, decode.d4.loss_mask: 0.5588, decode.d4.loss_dice: 0.9939, decode.d5.loss_cls: 0.2768, decode.d5.loss_mask: 0.5659, decode.d5.loss_dice: 0.9868, decode.d6.loss_cls: 0.2520, decode.d6.loss_mask: 0.5634, decode.d6.loss_dice: 0.9924, decode.d7.loss_cls: 0.2721, decode.d7.loss_mask: 0.5600, decode.d7.loss_dice: 0.9895, decode.d8.loss_cls: 0.2593, decode.d8.loss_mask: 0.5603, decode.d8.loss_dice: 0.9883, loss: 18.9164
2023-02-22 20:36:34,996 - mmseg - INFO - Iter [18950/80000]	lr: 1.096e-06, eta: 2 days, 19:40:23, time: 2.738, data_time: 0.020, memory: 31493, decode.loss_cls: 0.2653, decode.loss_mask: 0.4916, decode.loss_dice: 0.9034, decode.d0.loss_cls: 0.7429, decode.d0.loss_mask: 0.5330, decode.d0.loss_dice: 0.9787, decode.d1.loss_cls: 0.2563, decode.d1.loss_mask: 0.5096, decode.d1.loss_dice: 0.9405, decode.d2.loss_cls: 0.2967, decode.d2.loss_mask: 0.4996, decode.d2.loss_dice: 0.8994, decode.d3.loss_cls: 0.2602, decode.d3.loss_mask: 0.5077, decode.d3.loss_dice: 0.9053, decode.d4.loss_cls: 0.2470, decode.d4.loss_mask: 0.5070, decode.d4.loss_dice: 0.9122, decode.d5.loss_cls: 0.2512, decode.d5.loss_mask: 0.5046, decode.d5.loss_dice: 0.9105, decode.d6.loss_cls: 0.2702, decode.d6.loss_mask: 0.4931, decode.d6.loss_dice: 0.9059, decode.d7.loss_cls: 0.2731, decode.d7.loss_mask: 0.4972, decode.d7.loss_dice: 0.9111, decode.d8.loss_cls: 0.2696, decode.d8.loss_mask: 0.4966, decode.d8.loss_dice: 0.9077, loss: 17.3474
2023-02-22 20:38:52,118 - mmseg - INFO - Saving checkpoint at 19000 iterations
2023-02-22 20:39:13,787 - mmseg - INFO - Exp name: my_city.py
2023-02-22 20:39:13,788 - mmseg - INFO - Iter [19000/80000]	lr: 1.095e-06, eta: 2 days, 19:34:53, time: 3.176, data_time: 0.020, memory: 31493, decode.loss_cls: 0.2321, decode.loss_mask: 0.5515, decode.loss_dice: 0.9386, decode.d0.loss_cls: 0.7255, decode.d0.loss_mask: 0.5775, decode.d0.loss_dice: 1.0103, decode.d1.loss_cls: 0.2390, decode.d1.loss_mask: 0.5709, decode.d1.loss_dice: 0.9894, decode.d2.loss_cls: 0.2190, decode.d2.loss_mask: 0.5704, decode.d2.loss_dice: 0.9653, decode.d3.loss_cls: 0.2272, decode.d3.loss_mask: 0.5635, decode.d3.loss_dice: 0.9547, decode.d4.loss_cls: 0.2098, decode.d4.loss_mask: 0.5566, decode.d4.loss_dice: 0.9571, decode.d5.loss_cls: 0.2185, decode.d5.loss_mask: 0.5536, decode.d5.loss_dice: 0.9576, decode.d6.loss_cls: 0.2241, decode.d6.loss_mask: 0.5538, decode.d6.loss_dice: 0.9458, decode.d7.loss_cls: 0.2194, decode.d7.loss_mask: 0.5501, decode.d7.loss_dice: 0.9480, decode.d8.loss_cls: 0.2111, decode.d8.loss_mask: 0.5541, decode.d8.loss_dice: 0.9453, loss: 17.9396
2023-02-22 20:41:30,893 - mmseg - INFO - Iter [19050/80000]	lr: 1.094e-06, eta: 2 days, 19:28:14, time: 2.742, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2276, decode.loss_mask: 0.5343, decode.loss_dice: 0.8600, decode.d0.loss_cls: 0.7775, decode.d0.loss_mask: 0.5705, decode.d0.loss_dice: 0.9481, decode.d1.loss_cls: 0.2669, decode.d1.loss_mask: 0.5442, decode.d1.loss_dice: 0.9093, decode.d2.loss_cls: 0.2627, decode.d2.loss_mask: 0.5383, decode.d2.loss_dice: 0.8765, decode.d3.loss_cls: 0.2711, decode.d3.loss_mask: 0.5309, decode.d3.loss_dice: 0.8629, decode.d4.loss_cls: 0.2595, decode.d4.loss_mask: 0.5295, decode.d4.loss_dice: 0.8598, decode.d5.loss_cls: 0.2377, decode.d5.loss_mask: 0.5346, decode.d5.loss_dice: 0.8736, decode.d6.loss_cls: 0.2488, decode.d6.loss_mask: 0.5347, decode.d6.loss_dice: 0.8667, decode.d7.loss_cls: 0.2334, decode.d7.loss_mask: 0.5356, decode.d7.loss_dice: 0.8638, decode.d8.loss_cls: 0.2515, decode.d8.loss_mask: 0.5324, decode.d8.loss_dice: 0.8611, loss: 17.2035
2023-02-22 20:43:47,655 - mmseg - INFO - Iter [19100/80000]	lr: 1.093e-06, eta: 2 days, 19:21:35, time: 2.735, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2702, decode.loss_mask: 0.5494, decode.loss_dice: 0.8675, decode.d0.loss_cls: 0.7751, decode.d0.loss_mask: 0.5984, decode.d0.loss_dice: 0.9586, decode.d1.loss_cls: 0.2608, decode.d1.loss_mask: 0.5638, decode.d1.loss_dice: 0.9273, decode.d2.loss_cls: 0.2429, decode.d2.loss_mask: 0.5561, decode.d2.loss_dice: 0.9080, decode.d3.loss_cls: 0.2768, decode.d3.loss_mask: 0.5470, decode.d3.loss_dice: 0.9012, decode.d4.loss_cls: 0.2736, decode.d4.loss_mask: 0.5460, decode.d4.loss_dice: 0.8916, decode.d5.loss_cls: 0.2878, decode.d5.loss_mask: 0.5434, decode.d5.loss_dice: 0.8814, decode.d6.loss_cls: 0.2724, decode.d6.loss_mask: 0.5523, decode.d6.loss_dice: 0.8804, decode.d7.loss_cls: 0.2746, decode.d7.loss_mask: 0.5478, decode.d7.loss_dice: 0.8806, decode.d8.loss_cls: 0.2583, decode.d8.loss_mask: 0.5483, decode.d8.loss_dice: 0.8729, loss: 17.7147
2023-02-22 20:46:04,905 - mmseg - INFO - Iter [19150/80000]	lr: 1.092e-06, eta: 2 days, 19:15:00, time: 2.745, data_time: 0.022, memory: 31493, decode.loss_cls: 0.3021, decode.loss_mask: 0.5392, decode.loss_dice: 0.9324, decode.d0.loss_cls: 0.7620, decode.d0.loss_mask: 0.5855, decode.d0.loss_dice: 1.0300, decode.d1.loss_cls: 0.3164, decode.d1.loss_mask: 0.5462, decode.d1.loss_dice: 0.9571, decode.d2.loss_cls: 0.2990, decode.d2.loss_mask: 0.5366, decode.d2.loss_dice: 0.9572, decode.d3.loss_cls: 0.3097, decode.d3.loss_mask: 0.5300, decode.d3.loss_dice: 0.9495, decode.d4.loss_cls: 0.3137, decode.d4.loss_mask: 0.5342, decode.d4.loss_dice: 0.9381, decode.d5.loss_cls: 0.3143, decode.d5.loss_mask: 0.5338, decode.d5.loss_dice: 0.9307, decode.d6.loss_cls: 0.2978, decode.d6.loss_mask: 0.5375, decode.d6.loss_dice: 0.9370, decode.d7.loss_cls: 0.2977, decode.d7.loss_mask: 0.5395, decode.d7.loss_dice: 0.9406, decode.d8.loss_cls: 0.2958, decode.d8.loss_mask: 0.5386, decode.d8.loss_dice: 0.9353, loss: 18.4375
2023-02-22 20:48:23,026 - mmseg - INFO - Iter [19200/80000]	lr: 1.091e-06, eta: 2 days, 19:08:28, time: 2.762, data_time: 0.022, memory: 31493, decode.loss_cls: 0.2255, decode.loss_mask: 0.5140, decode.loss_dice: 0.9214, decode.d0.loss_cls: 0.6659, decode.d0.loss_mask: 0.5507, decode.d0.loss_dice: 0.9816, decode.d1.loss_cls: 0.2426, decode.d1.loss_mask: 0.5200, decode.d1.loss_dice: 0.9411, decode.d2.loss_cls: 0.1909, decode.d2.loss_mask: 0.5193, decode.d2.loss_dice: 0.9418, decode.d3.loss_cls: 0.2201, decode.d3.loss_mask: 0.5157, decode.d3.loss_dice: 0.9394, decode.d4.loss_cls: 0.2151, decode.d4.loss_mask: 0.5135, decode.d4.loss_dice: 0.9310, decode.d5.loss_cls: 0.2049, decode.d5.loss_mask: 0.5130, decode.d5.loss_dice: 0.9459, decode.d6.loss_cls: 0.2291, decode.d6.loss_mask: 0.5095, decode.d6.loss_dice: 0.9217, decode.d7.loss_cls: 0.2284, decode.d7.loss_mask: 0.5137, decode.d7.loss_dice: 0.9161, decode.d8.loss_cls: 0.2247, decode.d8.loss_mask: 0.5145, decode.d8.loss_dice: 0.9306, loss: 17.2017
2023-02-22 20:50:40,621 - mmseg - INFO - Iter [19250/80000]	lr: 1.090e-06, eta: 2 days, 19:01:56, time: 2.752, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2306, decode.loss_mask: 0.5714, decode.loss_dice: 0.8668, decode.d0.loss_cls: 0.7783, decode.d0.loss_mask: 0.6030, decode.d0.loss_dice: 0.9130, decode.d1.loss_cls: 0.2489, decode.d1.loss_mask: 0.5745, decode.d1.loss_dice: 0.9025, decode.d2.loss_cls: 0.2120, decode.d2.loss_mask: 0.5785, decode.d2.loss_dice: 0.8916, decode.d3.loss_cls: 0.2374, decode.d3.loss_mask: 0.5696, decode.d3.loss_dice: 0.8745, decode.d4.loss_cls: 0.2380, decode.d4.loss_mask: 0.5749, decode.d4.loss_dice: 0.8729, decode.d5.loss_cls: 0.2194, decode.d5.loss_mask: 0.5788, decode.d5.loss_dice: 0.8679, decode.d6.loss_cls: 0.2168, decode.d6.loss_mask: 0.5769, decode.d6.loss_dice: 0.8664, decode.d7.loss_cls: 0.2319, decode.d7.loss_mask: 0.5680, decode.d7.loss_dice: 0.8637, decode.d8.loss_cls: 0.2228, decode.d8.loss_mask: 0.5684, decode.d8.loss_dice: 0.8809, loss: 17.4004
2023-02-22 20:53:00,592 - mmseg - INFO - Iter [19300/80000]	lr: 1.089e-06, eta: 2 days, 18:55:33, time: 2.799, data_time: 0.068, memory: 31493, decode.loss_cls: 0.2595, decode.loss_mask: 0.5419, decode.loss_dice: 0.9440, decode.d0.loss_cls: 0.7789, decode.d0.loss_mask: 0.5817, decode.d0.loss_dice: 0.9892, decode.d1.loss_cls: 0.2984, decode.d1.loss_mask: 0.5492, decode.d1.loss_dice: 0.9581, decode.d2.loss_cls: 0.3139, decode.d2.loss_mask: 0.5361, decode.d2.loss_dice: 0.9442, decode.d3.loss_cls: 0.2798, decode.d3.loss_mask: 0.5310, decode.d3.loss_dice: 0.9302, decode.d4.loss_cls: 0.2709, decode.d4.loss_mask: 0.5346, decode.d4.loss_dice: 0.9268, decode.d5.loss_cls: 0.2785, decode.d5.loss_mask: 0.5424, decode.d5.loss_dice: 0.9278, decode.d6.loss_cls: 0.2866, decode.d6.loss_mask: 0.5340, decode.d6.loss_dice: 0.9342, decode.d7.loss_cls: 0.2504, decode.d7.loss_mask: 0.5392, decode.d7.loss_dice: 0.9353, decode.d8.loss_cls: 0.2544, decode.d8.loss_mask: 0.5471, decode.d8.loss_dice: 0.9499, loss: 18.1483
2023-02-22 20:55:17,749 - mmseg - INFO - Iter [19350/80000]	lr: 1.089e-06, eta: 2 days, 18:49:03, time: 2.743, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2511, decode.loss_mask: 0.5516, decode.loss_dice: 0.9329, decode.d0.loss_cls: 0.7673, decode.d0.loss_mask: 0.6005, decode.d0.loss_dice: 1.0282, decode.d1.loss_cls: 0.2707, decode.d1.loss_mask: 0.5636, decode.d1.loss_dice: 0.9734, decode.d2.loss_cls: 0.2616, decode.d2.loss_mask: 0.5496, decode.d2.loss_dice: 0.9433, decode.d3.loss_cls: 0.2744, decode.d3.loss_mask: 0.5480, decode.d3.loss_dice: 0.9386, decode.d4.loss_cls: 0.2600, decode.d4.loss_mask: 0.5513, decode.d4.loss_dice: 0.9495, decode.d5.loss_cls: 0.2702, decode.d5.loss_mask: 0.5466, decode.d5.loss_dice: 0.9460, decode.d6.loss_cls: 0.2659, decode.d6.loss_mask: 0.5476, decode.d6.loss_dice: 0.9387, decode.d7.loss_cls: 0.2648, decode.d7.loss_mask: 0.5512, decode.d7.loss_dice: 0.9374, decode.d8.loss_cls: 0.2464, decode.d8.loss_mask: 0.5510, decode.d8.loss_dice: 0.9404, loss: 18.2220
2023-02-22 20:57:35,115 - mmseg - INFO - Iter [19400/80000]	lr: 1.088e-06, eta: 2 days, 18:42:34, time: 2.747, data_time: 0.020, memory: 31493, decode.loss_cls: 0.3348, decode.loss_mask: 0.6028, decode.loss_dice: 0.9413, decode.d0.loss_cls: 0.7233, decode.d0.loss_mask: 0.6447, decode.d0.loss_dice: 1.0471, decode.d1.loss_cls: 0.3460, decode.d1.loss_mask: 0.6222, decode.d1.loss_dice: 0.9825, decode.d2.loss_cls: 0.3156, decode.d2.loss_mask: 0.6119, decode.d2.loss_dice: 0.9728, decode.d3.loss_cls: 0.3127, decode.d3.loss_mask: 0.6159, decode.d3.loss_dice: 0.9482, decode.d4.loss_cls: 0.3068, decode.d4.loss_mask: 0.6135, decode.d4.loss_dice: 0.9448, decode.d5.loss_cls: 0.3117, decode.d5.loss_mask: 0.6111, decode.d5.loss_dice: 0.9493, decode.d6.loss_cls: 0.3153, decode.d6.loss_mask: 0.6140, decode.d6.loss_dice: 0.9392, decode.d7.loss_cls: 0.3357, decode.d7.loss_mask: 0.6065, decode.d7.loss_dice: 0.9382, decode.d8.loss_cls: 0.3246, decode.d8.loss_mask: 0.6135, decode.d8.loss_dice: 0.9396, loss: 19.3857
2023-02-22 20:59:52,312 - mmseg - INFO - Iter [19450/80000]	lr: 1.087e-06, eta: 2 days, 18:36:06, time: 2.744, data_time: 0.019, memory: 31493, decode.loss_cls: 0.2536, decode.loss_mask: 0.5240, decode.loss_dice: 0.9252, decode.d0.loss_cls: 0.7429, decode.d0.loss_mask: 0.5600, decode.d0.loss_dice: 1.0012, decode.d1.loss_cls: 0.2700, decode.d1.loss_mask: 0.5405, decode.d1.loss_dice: 0.9591, decode.d2.loss_cls: 0.2685, decode.d2.loss_mask: 0.5299, decode.d2.loss_dice: 0.9341, decode.d3.loss_cls: 0.2610, decode.d3.loss_mask: 0.5305, decode.d3.loss_dice: 0.9341, decode.d4.loss_cls: 0.2538, decode.d4.loss_mask: 0.5279, decode.d4.loss_dice: 0.9326, decode.d5.loss_cls: 0.2794, decode.d5.loss_mask: 0.5257, decode.d5.loss_dice: 0.9284, decode.d6.loss_cls: 0.2692, decode.d6.loss_mask: 0.5223, decode.d6.loss_dice: 0.9191, decode.d7.loss_cls: 0.2758, decode.d7.loss_mask: 0.5242, decode.d7.loss_dice: 0.9216, decode.d8.loss_cls: 0.2916, decode.d8.loss_mask: 0.5209, decode.d8.loss_dice: 0.9127, loss: 17.8400
2023-02-22 21:02:09,539 - mmseg - INFO - Iter [19500/80000]	lr: 1.086e-06, eta: 2 days, 18:29:40, time: 2.745, data_time: 0.020, memory: 31493, decode.loss_cls: 0.2613, decode.loss_mask: 0.5628, decode.loss_dice: 0.9624, decode.d0.loss_cls: 0.7160, decode.d0.loss_mask: 0.5851, decode.d0.loss_dice: 1.0002, decode.d1.loss_cls: 0.2424, decode.d1.loss_mask: 0.5647, decode.d1.loss_dice: 0.9818, decode.d2.loss_cls: 0.2373, decode.d2.loss_mask: 0.5619, decode.d2.loss_dice: 0.9739, decode.d3.loss_cls: 0.2417, decode.d3.loss_mask: 0.5592, decode.d3.loss_dice: 0.9542, decode.d4.loss_cls: 0.2519, decode.d4.loss_mask: 0.5531, decode.d4.loss_dice: 0.9484, decode.d5.loss_cls: 0.2551, decode.d5.loss_mask: 0.5559, decode.d5.loss_dice: 0.9710, decode.d6.loss_cls: 0.2513, decode.d6.loss_mask: 0.5588, decode.d6.loss_dice: 0.9531, decode.d7.loss_cls: 0.2481, decode.d7.loss_mask: 0.5561, decode.d7.loss_dice: 0.9545, decode.d8.loss_cls: 0.2608, decode.d8.loss_mask: 0.5576, decode.d8.loss_dice: 0.9488, loss: 18.2294
2023-02-22 21:04:27,190 - mmseg - INFO - Iter [19550/80000]	lr: 1.085e-06, eta: 2 days, 18:23:16, time: 2.753, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2502, decode.loss_mask: 0.5720, decode.loss_dice: 0.9726, decode.d0.loss_cls: 0.7777, decode.d0.loss_mask: 0.6247, decode.d0.loss_dice: 1.0620, decode.d1.loss_cls: 0.2528, decode.d1.loss_mask: 0.5711, decode.d1.loss_dice: 1.0048, decode.d2.loss_cls: 0.2502, decode.d2.loss_mask: 0.5642, decode.d2.loss_dice: 0.9764, decode.d3.loss_cls: 0.2365, decode.d3.loss_mask: 0.5651, decode.d3.loss_dice: 0.9768, decode.d4.loss_cls: 0.2491, decode.d4.loss_mask: 0.5654, decode.d4.loss_dice: 0.9646, decode.d5.loss_cls: 0.2550, decode.d5.loss_mask: 0.5639, decode.d5.loss_dice: 0.9707, decode.d6.loss_cls: 0.2236, decode.d6.loss_mask: 0.5694, decode.d6.loss_dice: 0.9720, decode.d7.loss_cls: 0.2324, decode.d7.loss_mask: 0.5663, decode.d7.loss_dice: 0.9678, decode.d8.loss_cls: 0.2438, decode.d8.loss_mask: 0.5658, decode.d8.loss_dice: 0.9689, loss: 18.5355
2023-02-22 21:06:45,302 - mmseg - INFO - Iter [19600/80000]	lr: 1.084e-06, eta: 2 days, 18:16:55, time: 2.762, data_time: 0.022, memory: 31493, decode.loss_cls: 0.2428, decode.loss_mask: 0.5106, decode.loss_dice: 0.9355, decode.d0.loss_cls: 0.7622, decode.d0.loss_mask: 0.5393, decode.d0.loss_dice: 1.0043, decode.d1.loss_cls: 0.2731, decode.d1.loss_mask: 0.5154, decode.d1.loss_dice: 0.9819, decode.d2.loss_cls: 0.2696, decode.d2.loss_mask: 0.5118, decode.d2.loss_dice: 0.9531, decode.d3.loss_cls: 0.2693, decode.d3.loss_mask: 0.5090, decode.d3.loss_dice: 0.9288, decode.d4.loss_cls: 0.2537, decode.d4.loss_mask: 0.5122, decode.d4.loss_dice: 0.9466, decode.d5.loss_cls: 0.2503, decode.d5.loss_mask: 0.5105, decode.d5.loss_dice: 0.9403, decode.d6.loss_cls: 0.2633, decode.d6.loss_mask: 0.5094, decode.d6.loss_dice: 0.9241, decode.d7.loss_cls: 0.2629, decode.d7.loss_mask: 0.5107, decode.d7.loss_dice: 0.9333, decode.d8.loss_cls: 0.2430, decode.d8.loss_mask: 0.5091, decode.d8.loss_dice: 0.9366, loss: 17.7128
2023-02-22 21:09:10,516 - mmseg - INFO - Iter [19650/80000]	lr: 1.083e-06, eta: 2 days, 18:10:56, time: 2.904, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2608, decode.loss_mask: 0.5437, decode.loss_dice: 0.9305, decode.d0.loss_cls: 0.7265, decode.d0.loss_mask: 0.5820, decode.d0.loss_dice: 0.9971, decode.d1.loss_cls: 0.2517, decode.d1.loss_mask: 0.5597, decode.d1.loss_dice: 0.9498, decode.d2.loss_cls: 0.2331, decode.d2.loss_mask: 0.5605, decode.d2.loss_dice: 0.9433, decode.d3.loss_cls: 0.2631, decode.d3.loss_mask: 0.5522, decode.d3.loss_dice: 0.9387, decode.d4.loss_cls: 0.2447, decode.d4.loss_mask: 0.5563, decode.d4.loss_dice: 0.9371, decode.d5.loss_cls: 0.2347, decode.d5.loss_mask: 0.5507, decode.d5.loss_dice: 0.9425, decode.d6.loss_cls: 0.2241, decode.d6.loss_mask: 0.5611, decode.d6.loss_dice: 0.9423, decode.d7.loss_cls: 0.2397, decode.d7.loss_mask: 0.5529, decode.d7.loss_dice: 0.9391, decode.d8.loss_cls: 0.2476, decode.d8.loss_mask: 0.5459, decode.d8.loss_dice: 0.9332, loss: 17.9449
2023-02-22 21:11:37,332 - mmseg - INFO - Iter [19700/80000]	lr: 1.082e-06, eta: 2 days, 18:05:04, time: 2.936, data_time: 0.024, memory: 31493, decode.loss_cls: 0.2073, decode.loss_mask: 0.5576, decode.loss_dice: 0.9307, decode.d0.loss_cls: 0.7003, decode.d0.loss_mask: 0.5910, decode.d0.loss_dice: 1.0082, decode.d1.loss_cls: 0.2226, decode.d1.loss_mask: 0.5665, decode.d1.loss_dice: 0.9607, decode.d2.loss_cls: 0.2065, decode.d2.loss_mask: 0.5631, decode.d2.loss_dice: 0.9405, decode.d3.loss_cls: 0.1931, decode.d3.loss_mask: 0.5610, decode.d3.loss_dice: 0.9401, decode.d4.loss_cls: 0.2030, decode.d4.loss_mask: 0.5594, decode.d4.loss_dice: 0.9300, decode.d5.loss_cls: 0.1938, decode.d5.loss_mask: 0.5588, decode.d5.loss_dice: 0.9398, decode.d6.loss_cls: 0.1940, decode.d6.loss_mask: 0.5595, decode.d6.loss_dice: 0.9357, decode.d7.loss_cls: 0.1866, decode.d7.loss_mask: 0.5608, decode.d7.loss_dice: 0.9387, decode.d8.loss_cls: 0.2025, decode.d8.loss_mask: 0.5587, decode.d8.loss_dice: 0.9431, loss: 17.6136
2023-02-22 21:13:54,807 - mmseg - INFO - Iter [19750/80000]	lr: 1.081e-06, eta: 2 days, 17:58:44, time: 2.750, data_time: 0.021, memory: 31493, decode.loss_cls: 0.3681, decode.loss_mask: 0.5402, decode.loss_dice: 0.9418, decode.d0.loss_cls: 0.7791, decode.d0.loss_mask: 0.6027, decode.d0.loss_dice: 1.0299, decode.d1.loss_cls: 0.3401, decode.d1.loss_mask: 0.5728, decode.d1.loss_dice: 0.9903, decode.d2.loss_cls: 0.3145, decode.d2.loss_mask: 0.5645, decode.d2.loss_dice: 0.9691, decode.d3.loss_cls: 0.3340, decode.d3.loss_mask: 0.5554, decode.d3.loss_dice: 0.9577, decode.d4.loss_cls: 0.3217, decode.d4.loss_mask: 0.5551, decode.d4.loss_dice: 0.9693, decode.d5.loss_cls: 0.3609, decode.d5.loss_mask: 0.5525, decode.d5.loss_dice: 0.9507, decode.d6.loss_cls: 0.3427, decode.d6.loss_mask: 0.5470, decode.d6.loss_dice: 0.9350, decode.d7.loss_cls: 0.3458, decode.d7.loss_mask: 0.5578, decode.d7.loss_dice: 0.9607, decode.d8.loss_cls: 0.3327, decode.d8.loss_mask: 0.5552, decode.d8.loss_dice: 0.9683, loss: 19.1158
2023-02-22 21:16:12,203 - mmseg - INFO - Iter [19800/80000]	lr: 1.080e-06, eta: 2 days, 17:52:26, time: 2.748, data_time: 0.020, memory: 31493, decode.loss_cls: 0.2858, decode.loss_mask: 0.5660, decode.loss_dice: 0.9848, decode.d0.loss_cls: 0.7475, decode.d0.loss_mask: 0.5842, decode.d0.loss_dice: 1.0431, decode.d1.loss_cls: 0.3244, decode.d1.loss_mask: 0.5699, decode.d1.loss_dice: 1.0111, decode.d2.loss_cls: 0.3080, decode.d2.loss_mask: 0.5702, decode.d2.loss_dice: 1.0080, decode.d3.loss_cls: 0.2874, decode.d3.loss_mask: 0.5637, decode.d3.loss_dice: 0.9856, decode.d4.loss_cls: 0.2906, decode.d4.loss_mask: 0.5667, decode.d4.loss_dice: 0.9851, decode.d5.loss_cls: 0.3072, decode.d5.loss_mask: 0.5647, decode.d5.loss_dice: 0.9834, decode.d6.loss_cls: 0.3148, decode.d6.loss_mask: 0.5677, decode.d6.loss_dice: 0.9845, decode.d7.loss_cls: 0.2977, decode.d7.loss_mask: 0.5766, decode.d7.loss_dice: 0.9822, decode.d8.loss_cls: 0.2978, decode.d8.loss_mask: 0.5676, decode.d8.loss_dice: 0.9902, loss: 19.1162
2023-02-22 21:18:38,617 - mmseg - INFO - Iter [19850/80000]	lr: 1.080e-06, eta: 2 days, 17:46:36, time: 2.928, data_time: 0.023, memory: 31493, decode.loss_cls: 0.2710, decode.loss_mask: 0.5373, decode.loss_dice: 0.9260, decode.d0.loss_cls: 0.7806, decode.d0.loss_mask: 0.5790, decode.d0.loss_dice: 1.0093, decode.d1.loss_cls: 0.2969, decode.d1.loss_mask: 0.5407, decode.d1.loss_dice: 0.9370, decode.d2.loss_cls: 0.3079, decode.d2.loss_mask: 0.5317, decode.d2.loss_dice: 0.9065, decode.d3.loss_cls: 0.2719, decode.d3.loss_mask: 0.5318, decode.d3.loss_dice: 0.8980, decode.d4.loss_cls: 0.2625, decode.d4.loss_mask: 0.5302, decode.d4.loss_dice: 0.9059, decode.d5.loss_cls: 0.2526, decode.d5.loss_mask: 0.5337, decode.d5.loss_dice: 0.9105, decode.d6.loss_cls: 0.2701, decode.d6.loss_mask: 0.5337, decode.d6.loss_dice: 0.8940, decode.d7.loss_cls: 0.2587, decode.d7.loss_mask: 0.5325, decode.d7.loss_dice: 0.9112, decode.d8.loss_cls: 0.2544, decode.d8.loss_mask: 0.5315, decode.d8.loss_dice: 0.9070, loss: 17.8141
2023-02-22 21:20:56,026 - mmseg - INFO - Iter [19900/80000]	lr: 1.079e-06, eta: 2 days, 17:40:19, time: 2.748, data_time: 0.021, memory: 31493, decode.loss_cls: 0.2291, decode.loss_mask: 0.4732, decode.loss_dice: 0.9200, decode.d0.loss_cls: 0.6735, decode.d0.loss_mask: 0.4943, decode.d0.loss_dice: 1.0130, decode.d1.loss_cls: 0.2224, decode.d1.loss_mask: 0.4770, decode.d1.loss_dice: 0.9532, decode.d2.loss_cls: 0.2361, decode.d2.loss_mask: 0.4771, decode.d2.loss_dice: 0.9334, decode.d3.loss_cls: 0.2133, decode.d3.loss_mask: 0.4743, decode.d3.loss_dice: 0.9285, decode.d4.loss_cls: 0.2210, decode.d4.loss_mask: 0.4769, decode.d4.loss_dice: 0.9200, decode.d5.loss_cls: 0.2348, decode.d5.loss_mask: 0.4739, decode.d5.loss_dice: 0.9221, decode.d6.loss_cls: 0.2233, decode.d6.loss_mask: 0.4760, decode.d6.loss_dice: 0.9230, decode.d7.loss_cls: 0.2290, decode.d7.loss_mask: 0.4728, decode.d7.loss_dice: 0.9233, decode.d8.loss_cls: 0.2192, decode.d8.loss_mask: 0.4748, decode.d8.loss_dice: 0.9377, loss: 16.8461
2023-02-22 21:23:26,422 - mmseg - INFO - Iter [19950/80000]	lr: 1.078e-06, eta: 2 days, 17:34:43, time: 3.008, data_time: 0.025, memory: 31493, decode.loss_cls: 0.2660, decode.loss_mask: 0.5590, decode.loss_dice: 0.9970, decode.d0.loss_cls: 0.7322, decode.d0.loss_mask: 0.5832, decode.d0.loss_dice: 1.0310, decode.d1.loss_cls: 0.3211, decode.d1.loss_mask: 0.5606, decode.d1.loss_dice: 0.9906, decode.d2.loss_cls: 0.2739, decode.d2.loss_mask: 0.5566, decode.d2.loss_dice: 0.9705, decode.d3.loss_cls: 0.2823, decode.d3.loss_mask: 0.5569, decode.d3.loss_dice: 0.9751, decode.d4.loss_cls: 0.2743, decode.d4.loss_mask: 0.5509, decode.d4.loss_dice: 0.9702, decode.d5.loss_cls: 0.3021, decode.d5.loss_mask: 0.5486, decode.d5.loss_dice: 0.9708, decode.d6.loss_cls: 0.2863, decode.d6.loss_mask: 0.5467, decode.d6.loss_dice: 0.9765, decode.d7.loss_cls: 0.3228, decode.d7.loss_mask: 0.5445, decode.d7.loss_dice: 0.9663, decode.d8.loss_cls: 0.2963, decode.d8.loss_mask: 0.5555, decode.d8.loss_dice: 0.9763, loss: 18.7441
2023-02-22 21:25:52,537 - mmseg - INFO - Saving checkpoint at 20000 iterations
2023-02-22 21:26:16,041 - mmseg - INFO - Exp name: my_city.py
2023-02-22 21:26:16,042 - mmseg - INFO - Iter [20000/80000]	lr: 1.077e-06, eta: 2 days, 17:30:06, time: 3.392, data_time: 0.069, memory: 31493, decode.loss_cls: 0.2617, decode.loss_mask: 0.5631, decode.loss_dice: 0.9439, decode.d0.loss_cls: 0.7418, decode.d0.loss_mask: 0.6140, decode.d0.loss_dice: 1.0057, decode.d1.loss_cls: 0.2660, decode.d1.loss_mask: 0.5778, decode.d1.loss_dice: 0.9538, decode.d2.loss_cls: 0.2603, decode.d2.loss_mask: 0.5817, decode.d2.loss_dice: 0.9344, decode.d3.loss_cls: 0.2664, decode.d3.loss_mask: 0.5756, decode.d3.loss_dice: 0.9467, decode.d4.loss_cls: 0.2489, decode.d4.loss_mask: 0.5683, decode.d4.loss_dice: 0.9372, decode.d5.loss_cls: 0.2691, decode.d5.loss_mask: 0.5704, decode.d5.loss_dice: 0.9420, decode.d6.loss_cls: 0.2582, decode.d6.loss_mask: 0.5664, decode.d6.loss_dice: 0.9352, decode.d7.loss_cls: 0.2700, decode.d7.loss_mask: 0.5611, decode.d7.loss_dice: 0.9400, decode.d8.loss_cls: 0.2494, decode.d8.loss_mask: 0.5719, decode.d8.loss_dice: 0.9573, loss: 18.3381
[                                                  ] 0/357, elapsed: 0s, ETA:[                                ] 1/357, 0.0 task/s, elapsed: 65s, ETA: 23025s[                               ] 2/357, 0.0 task/s, elapsed: 127s, ETA: 22577s[                               ] 3/357, 0.0 task/s, elapsed: 190s, ETA: 22403s[                               ] 4/357, 0.0 task/s, elapsed: 252s, ETA: 22278s[                               ] 5/357, 0.0 task/s, elapsed: 315s, ETA: 22170s[                               ] 6/357, 0.0 task/s, elapsed: 381s, ETA: 22316s[                               ] 7/357, 0.0 task/s, elapsed: 448s, ETA: 22402s[                               ] 8/357, 0.0 task/s, elapsed: 515s, ETA: 22459s[                               ] 9/357, 0.0 task/s, elapsed: 577s, ETA: 22326s[                              ] 10/357, 0.0 task/s, elapsed: 640s, ETA: 22204s[                              ] 11/357, 0.0 task/s, elapsed: 709s, ETA: 22286s[>                             ] 12/357, 0.0 task/s, elapsed: 771s, ETA: 22172s[>                             ] 13/357, 0.0 task/s, elapsed: 834s, ETA: 22066s[>                             ] 14/357, 0.0 task/s, elapsed: 900s, ETA: 22061s[>                             ] 15/357, 0.0 task/s, elapsed: 967s, ETA: 22055s[>                            ] 16/357, 0.0 task/s, elapsed: 1036s, ETA: 22082s[>                            ] 17/357, 0.0 task/s, elapsed: 1105s, ETA: 22098s[>                            ] 18/357, 0.0 task/s, elapsed: 1168s, ETA: 21991s[>                            ] 19/357, 0.0 task/s, elapsed: 1230s, ETA: 21885s[>                            ] 20/357, 0.0 task/s, elapsed: 1297s, ETA: 21854s[>                            ] 21/357, 0.0 task/s, elapsed: 1364s, ETA: 21823s[>                            ] 22/357, 0.0 task/s, elapsed: 1427s, ETA: 21722s[>                            ] 23/357, 0.0 task/s, elapsed: 1489s, ETA: 21625s[>                            ] 24/357, 0.0 task/s, elapsed: 1552s, ETA: 21533s[>>                           ] 25/357, 0.0 task/s, elapsed: 1614s, ETA: 21440s[>>                           ] 26/357, 0.0 task/s, elapsed: 1677s, ETA: 21349s[>>                           ] 27/357, 0.0 task/s, elapsed: 1739s, ETA: 21260s[>>                           ] 28/357, 0.0 task/s, elapsed: 1802s, ETA: 21175s[>>                           ] 29/357, 0.0 task/s, elapsed: 1865s, ETA: 21093s[>>                           ] 30/357, 0.0 task/s, elapsed: 1928s, ETA: 21011s[>>                           ] 31/357, 0.0 task/s, elapsed: 1994s, ETA: 20971s[>>                           ] 32/357, 0.0 task/s, elapsed: 2061s, ETA: 20928s[>>                           ] 33/357, 0.0 task/s, elapsed: 2127s, ETA: 20885s[>>                           ] 34/357, 0.0 task/s, elapsed: 2196s, ETA: 20857s[>>                           ] 35/357, 0.0 task/s, elapsed: 2264s, ETA: 20829s[>>                           ] 36/357, 0.0 task/s, elapsed: 2333s, ETA: 20798s[>>>                          ] 37/357, 0.0 task/s, elapsed: 2401s, ETA: 20764s[>>>                          ] 38/357, 0.0 task/s, elapsed: 2467s, ETA: 20713s[>>>                          ] 39/357, 0.0 task/s, elapsed: 2534s, ETA: 20660s[>>>                          ] 40/357, 0.0 task/s, elapsed: 2596s, ETA: 20577s[>>>                          ] 41/357, 0.0 task/s, elapsed: 2659s, ETA: 20495s[>>>                          ] 42/357, 0.0 task/s, elapsed: 2726s, ETA: 20443s[>>>                          ] 43/357, 0.0 task/s, elapsed: 2788s, ETA: 20359s[>>>                          ] 44/357, 0.0 task/s, elapsed: 2851s, ETA: 20279s[>>>                          ] 45/357, 0.0 task/s, elapsed: 2927s, ETA: 20292s[>>>                          ] 46/357, 0.0 task/s, elapsed: 2994s, ETA: 20239s[>>>                          ] 47/357, 0.0 task/s, elapsed: 3060s, ETA: 20186s[>>>                          ] 48/357, 0.0 task/s, elapsed: 3123s, ETA: 20104s[>>>                          ] 49/357, 0.0 task/s, elapsed: 3193s, ETA: 20067s[>>>>                         ] 50/357, 0.0 task/s, elapsed: 3259s, ETA: 20011s[>>>>                         ] 51/357, 0.0 task/s, elapsed: 3325s, ETA: 19950s[>>>>                         ] 52/357, 0.0 task/s, elapsed: 3397s, ETA: 19925s[>>>>                         ] 53/357, 0.0 task/s, elapsed: 3467s, ETA: 19885s[>>>>                         ] 54/357, 0.0 task/s, elapsed: 3533s, ETA: 19825s[>>>>                         ] 55/357, 0.0 task/s, elapsed: 3600s, ETA: 19765s[>>>>                         ] 56/357, 0.0 task/s, elapsed: 3666s, ETA: 19706s[>>>>                         ] 57/357, 0.0 task/s, elapsed: 3729s, ETA: 19627s[>>>>                         ] 58/357, 0.0 task/s, elapsed: 3792s, ETA: 19546s[>>>>                         ] 59/357, 0.0 task/s, elapsed: 3854s, ETA: 19466s[>>>>                         ] 60/357, 0.0 task/s, elapsed: 3916s, ETA: 19386s[>>>>                         ] 61/357, 0.0 task/s, elapsed: 3979s, ETA: 19308s